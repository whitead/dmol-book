
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub ðŸ“–" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>3. Regression &amp; Model Assessment &#8212; deep learning for molecules &amp; materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cb1cce99" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css?v=ffeaf963" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/regression';</script>
    <script src="../_static/custom.js?v=3f5092eb"></script>
    <link rel="canonical" href="https://dmol.pub/ml/regression.html" />
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Classification" href="classification.html" />
    <link rel="prev" title="2. Introduction to Machine Learning" href="introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="deep learning for molecules & materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="deep learning for molecules & materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">A. Math Review</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/tensors-and-shapes.html">1. Tensors and Shapes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">B. Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">2. Introduction to Machine Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Regression &amp; Model Assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">4. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="kernel.html">5. Kernel Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C. Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dl/introduction.html">6. Deep Learning Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/layers.html">7. Standard Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/gnn.html">8. Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/data.html">9. Input Data &amp; Equivariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/Equivariant.html">10. Equivariant Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/xai.html">11. Explaining Predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/attention.html">12. Attention Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/NLP.html">13. Deep Learning on Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/VAE.html">14. Variational Autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/flows.html">15. Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/molnets.html">16. Modern Molecular NNs</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">D. Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/QM9.html">17. Predicting DFT Energies with GNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied/MolGenerator.html">18. Generative RNN in Browser</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">E. Contributed Chapters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/e3nn_traj.html">19. Equivariant Neural Network for Predicting Trajectories</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/pretraining.html">20. Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">F. Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../style.html">21. Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">22. Changelog</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/ml/regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/whitead/dmol-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fml/regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression & Model Assessment</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">3.1. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">3.2. Overfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-with-synthetic-data">3.2.1. Overfitting with Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-conclusion">3.2.2. Overfitting Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-effect-of-feature-number">3.3. Exploring Effect of Feature Number</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-decomposition">3.4. Bias Variance Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">3.5. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization">3.5.1. L2 Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization">3.5.2. L1 Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strategies-to-assess-models">3.6. Strategies to Assess Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">3.6.1. k-Fold Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cv">3.6.2. Leave-one-out CV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-other-measures">3.7. Computing Other Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap-resampling">3.7.1. Bootstrap Resampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacknife">3.7.2. Jacknife+</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-distribution">3.8. Training Data Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-class-out-cross-validation">3.8.1. Leave One Class Out Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaffold-splits">3.8.2. Scaffold splits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">3.9. Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.10. Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.10.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.10.2. Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-assessment">3.10.3. Model Assessment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">3.11. Cited References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression-model-assessment">
<h1><span class="section-number">3. </span>Regression &amp; Model Assessment<a class="headerlink" href="#regression-model-assessment" title="Link to this heading">#</a></h1>
<p>Regression is supervised learning with continuous (or sometimes discrete) labels. You are given labeled data consisting of features and labels <span class="math notranslate nohighlight">\(\{\vec{x}_i, y_i\}\)</span>. The goal is to find a function that describes their relationship, <span class="math notranslate nohighlight">\(\hat{f}(\vec{x}) = \hat{y}\)</span>. A more formal discussion of the concepts discussed here can be found in Chapter 3 of Bishopâ€™s Pattern Recognition and Machine Learning<span id="id1">[<a class="reference internal" href="#id20" title="Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.">Bis06</a>]</span>.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This lecture introduces some probability theory, especially expectations. You can get a refresher of <a class="reference external" href="https://whitead.github.io/numerical_stats/">probability of random variables</a> and/or <a class="reference external" href="https://whitead.github.io/numerical_stats/unit_4/lectures/lecture_2.pdf">expectations</a>. Recall an expectation is <span class="math notranslate nohighlight">\(E[x] = \sum P(x)x\)</span> and variance is <span class="math notranslate nohighlight">\(E[\left(x - E[x]\right)^2]\)</span>. We also use and discuss <a class="reference external" href="https://nbviewer.jupyter.org/github/whitead/numerical_stats/blob/master/unit_12/lectures/lecture_1.ipynb#Extending-Least-Squares-to-Multiple-Dimensions-in-Domain---OLS-ND">linear regression techniques</a>. After completing this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Perform multi-dimensional regression with a loss function</p></li>
<li><p>Understand how to and why we batch</p></li>
<li><p>Understand splitting of data</p></li>
<li><p>Reason about model bias and model variance</p></li>
<li><p>Assess model fit and generalization error</p></li>
</ul>
</div>
<section id="running-this-notebook">
<h2><span class="section-number">3.1. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Link to this heading">#</a></h2>
<p>Click the Â <i aria-label="Launch interactive content" class="fas fa-rocket"></i>Â  above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
<p>As usual, the code below sets-up our imports.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.example_libraries</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">rdkit</span><span class="o">,</span><span class="w"> </span><span class="nn">rdkit.Chem</span><span class="o">,</span><span class="w"> </span><span class="nn">rdkit.Chem.Draw</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rdkit.Chem.Scaffolds</span><span class="w"> </span><span class="kn">import</span> <span class="n">MurckoScaffold</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># soldata = pd.read_csv(&#39;https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&amp;gbrecs=true&#39;)</span>
<span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv&quot;</span>
<span class="p">)</span>
<span class="n">features_start_at</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">soldata</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;MolWt&quot;</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">features_start_at</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="overfitting">
<h2><span class="section-number">3.2. </span>Overfitting<a class="headerlink" href="#overfitting" title="Link to this heading">#</a></h2>
<p>Weâ€™ll be working again with the AqSolDB<span id="id2">[<a class="reference internal" href="#id28" title="Murat Cihan Sorkun, Abhishek Khetan, and SÃ¼leyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. Sci. Data, 6(1):143, 2019. doi:10.1038/s41597-019-0151-1.">SKE19</a>]</span> dataset. It has about 10,000 unique compounds with measured solubility in water (label) and 17 molecular descriptors (features). We need to create a better assessment of our supervised ML models. The goal of our ML model is to predict solubility of new unseen molecules. Therefore, to assess we should test on unseen molecules. We will split our data into two subsets: <strong>training data</strong> and <strong>testing data</strong>. Typically this is done with an 80%/20%, so that you train on 80% of your data and test on the remaining 20%. In our case, weâ€™ll just do 50%/50% because we have plenty of data and thus do not need to take 80% for training. Weâ€™ll be using a subset, 50 molecules chosen randomly, rather than the whole dataset. So weâ€™ll have 25 training molecules and 25 testing molecules.</p>
<p>Letâ€™s begin by seeing what effect the split of train/test has on our linear model introduced in the previous chapter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get 50 points and split into train/test</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:</span><span class="mi">25</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="mi">25</span><span class="p">:]</span>

<span class="c1"># standardize the features using only train</span>
<span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">/=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">/=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># convert from pandas dataframe to numpy arrays</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>We will again use a linear model,  <span class="math notranslate nohighlight">\( \hat{y} = \vec{w}\vec{x} + b \)</span>. One change weâ€™ll make is using the <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.html#jax.jit">&#64;jit</a> decorator from <code class="docutils literal notranslate"><span class="pre">jax</span></code>. This decorator will tell <code class="docutils literal notranslate"><span class="pre">jax</span></code> to inspect our function, simplify it, and compile it to run quickly on a GPU (if available) or CPU. The rest of our work is the same as the previous chapter. We begin with defining our loss, which is mean squared error (MSE) again.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>A decorator is a Python-specific syntax that modifies how a function behaves. It is
indicated with the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> symbol. Examples include caching results, compiling the function, running
it in parallel, and timing its execution.</p>
</aside>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define our loss function</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="n">loss_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Array([-2.8831835 ,  1.3794075 , -3.0262635 , -3.1956701 , -4.007525  ,
        -0.83476734, -3.0994835 , -4.0455275 , -3.675129  ,  2.5466921 ,
        -3.11543   , -4.171584  , -1.5834932 , -3.3554041 , -3.1797354 ,
         0.86207145, -2.0010393 ], dtype=float32),
 Array(5.772167, dtype=float32, weak_type=True))
</pre></div>
</div>
</div>
</div>
<p>Now we will train our model, again using gradient descent. This time we will not batch, since our training data only has 25 points. Can you see what the learning rate is? Why is it so different from the last chapter when we used the whole dataset?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_progress</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_loss_progress</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">loss_progress</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">test_loss_progress</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_progress</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_loss_progress</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Testing Loss&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ab3b3595c9c52f1774136336f0fceafe6629d06c36c1af8589471b8a3d16e770.png" src="../_images/ab3b3595c9c52f1774136336f0fceafe6629d06c36c1af8589471b8a3d16e770.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;correlation = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">yhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;loss = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/4745153d6ba9f7dc978298d7201386d166322a3cb4119b1315f837d13589f68b.png" src="../_images/4745153d6ba9f7dc978298d7201386d166322a3cb4119b1315f837d13589f68b.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">test_x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;correlation = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span><span class="w"> </span><span class="n">yhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;loss = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Testing Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/88703617cacbc286e720e211e11786ad423ee8670c934bcb4614d2d123f46d4b.png" src="../_images/88703617cacbc286e720e211e11786ad423ee8670c934bcb4614d2d123f46d4b.png" />
</div>
</div>
<p>Weâ€™ve plotted above the loss on our training data and testing data. The loss on training goes down after each step, as we would expect for gradient descent. However, the testing loss goes down and then starts to go back up. This is called <strong>overfitting</strong>. This is one of the key challenges in ML and weâ€™ll often be discussing it.</p>
<p>Overfitting is a result of training for too many steps or with too many parameters, resulting in our model learning the <strong>noise</strong> in the training data. The noise is specific for the training data and when computing loss on the test data there is poor performance.</p>
<p>To understand this, letâ€™s first define noise. Assume that there is a â€œperfectâ€ function <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> that can compute labels from features. Our model is an estimate <span class="math notranslate nohighlight">\(\hat{f}(\vec{x})\)</span> of that function. Even <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> will not reproduce the data exactly because our features do not capture everything that goes into solubility and/or there is error in the solubility measurements themselves. Mathematically,</p>
<div class="amsmath math notranslate nohighlight" id="equation-83949e79-2d17-4d85-a898-f42431b701a4">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-83949e79-2d17-4d85-a898-f42431b701a4" title="Permalink to this equation">#</a></span>\[\begin{equation}
    y = f(\vec{x}) + \epsilon
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a random number with mean 0 and unknown standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. <span class="math notranslate nohighlight">\(\epsilon\)</span> is the noise. When fitting our function, <span class="math notranslate nohighlight">\(\hat{f}(\vec{x})\)</span>, the noise is fixed because our labels <span class="math notranslate nohighlight">\(y\)</span> are fixed. That means we can accidentally learn to approximate the sum of <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> and the noise <span class="math notranslate nohighlight">\({\epsilon_i}\)</span> instead of only capturing <span class="math notranslate nohighlight">\(f(\vec{x})\)</span>. The noise is random and uncorrelated with solubility. When we move to our testing dataset, this noise changes because we have new data and our modelâ€™s effort to reproduce noise is useless because the new data has new noise. This leads to worse performance.</p>
<p>Overfitting arises when three things happen: you have noise, you have extra features or some part of your features are not correlated with the labels, and your training has converged (your model fit is at the global minimum). This last one is what we saw above. Our model wasnâ€™t overfit after about 100 steps (the training and testing loss were both decreasing), but then they starting going in opposite directions. Letâ€™s see how these things interplay to lead to overfitting in an example where we can exactly control the features and noise.</p>
<section id="overfitting-with-synthetic-data">
<h3><span class="section-number">3.2.1. </span>Overfitting with Synthetic Data<a class="headerlink" href="#overfitting-with-synthetic-data" title="Link to this heading">#</a></h3>
<p>Weâ€™ll explore overfitting in a synthetic example. Our real function weâ€™re trying to learn will be:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bb1d46ba-e6c4-4b88-8bc5-4cc042f57537">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-bb1d46ba-e6c4-4b88-8bc5-4cc042f57537" title="Permalink to this equation">#</a></span>\[\begin{equation}
 f(x) = x^3 - x^2 + x - 1
\end{equation}\]</div>
<p>which we can rewrite as a linear model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dbdea525-a775-421c-b8fa-03f27dd4f713">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-dbdea525-a775-421c-b8fa-03f27dd4f713" title="Permalink to this equation">#</a></span>\[\begin{equation}
  f(\vec{x}) = \vec{w}\cdot\vec{x} = [1, -1, 1, -1]\cdot[x^3, x^2, x, 1]
\end{equation}\]</div>
<p>where our features are <span class="math notranslate nohighlight">\([x^3, x^2, x, 1]\)</span>. To do our split, weâ€™ll take the positive points as training data and the negative as testing data. To avoid the issue of convergence, we will use least squares to fit these models instead of gradient descent.</p>
<p>Letâ€™s establish a benchmark. How well can a model do without noise? Weâ€™ll use 10 training data points and 10 testing data points. Weâ€™ll put our testing data in the center of the polynomial.</p>
<p>Expand the Python cells below to see how this is implemented.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate data from polynomial</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">syn_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="c1"># create feature matrix</span>
<span class="n">syn_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">syn_x</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span> <span class="n">syn_x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">syn_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">syn_x</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
<span class="n">syn_labels</span> <span class="o">=</span> <span class="n">syn_x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="n">syn_x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">syn_x</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split data into train/test</span>
<span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">4</span><span class="p">))</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">test_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">syn_labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">syn_labels</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>

<span class="c1"># fit using numpy least squares method.</span>
<span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># plotting code</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">train_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Testing labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn_features</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fit Model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">syn_labels</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ground Truth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Training Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">train_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Testing Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">,</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;No Noise, Perfect Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/290d271e298fb4471e99f0a11ca6e722e96ea5becab5347d5ec5d64d60df7d0b.png" src="../_images/290d271e298fb4471e99f0a11ca6e722e96ea5becab5347d5ec5d64d60df7d0b.png" />
</div>
</div>
<p>There is no overfitting and the regression is quite accurate without noise. Now weâ€™ll add noise to both the training labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_y</span> <span class="o">=</span> <span class="n">train_y</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">train_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">train_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Testing labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn_features</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fit Model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">syn_labels</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ground Truth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Training Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">train_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Testing Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">,</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noise, Perfect Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/ee8e9029cf2331323f45d055dcce72a1820677ec4fcf3fd0fcd3ca296d38572b.png" src="../_images/ee8e9029cf2331323f45d055dcce72a1820677ec4fcf3fd0fcd3ca296d38572b.png" />
</div>
</div>
<p>Adding noise reduces the accuracy on the training data. The testing labels have no noise and the model is not overfit, so the accuracy is good for the testing loss.</p>
<p>Now weâ€™ll try adding redundant features. Our new features will be <span class="math notranslate nohighlight">\([x^6, x^5, x^4, x^3, x^2, x, 1]\)</span>. Still less than our data point number but not all features are necessary to fit the labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">syn_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">syn_x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">syn_labels</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>

<span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">train_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Testing labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn_features</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fit Model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">syn_labels</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ground Truth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Training Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">train_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Testing Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">,</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noise, Extra Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/11d683346200411be68d1993183673ed2f628d85641b608a3315f9661621a630.png" src="../_images/11d683346200411be68d1993183673ed2f628d85641b608a3315f9661621a630.png" />
</div>
</div>
<p>This is an overfit model. The training loss went down (note the noise was the same in the previous two examples), but at the expense of a large increase in testing loss. This wasnâ€™t possible in the previous example because over-fitting to noise wasnâ€™t feasible when each feature was necessary to capture the correlation with the labels.</p>
<p>Letâ€™s see an example where the feature number is the same but they arenâ€™t perfectly correlated with labels, meaning we cannot match the labels even if there was no noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">syn_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
    <span class="p">[</span><span class="n">syn_x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">syn_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">syn_x</span><span class="o">**</span><span class="mi">2</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">syn_x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">syn_x</span><span class="p">)]</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">syn_features</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>

<span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">train_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Testing labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn_features</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fit Model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">syn_x</span><span class="p">,</span> <span class="n">syn_labels</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ground Truth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Training Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">train_x</span><span class="p">,</span><span class="w"> </span><span class="n">train_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Testing Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">test_x</span><span class="p">,</span><span class="w"> </span><span class="n">test_y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Noise, Imperfectly Correlated Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/7ef2d264533a5401e184f447373a7cc5d585baffbbe3d4345bdfc7dff671b3db.png" src="../_images/7ef2d264533a5401e184f447373a7cc5d585baffbbe3d4345bdfc7dff671b3db.png" />
</div>
</div>
<p>Itâ€™s arguable if this is overfitting. Yes, the testing loss is high but it could be argued itâ€™s more to do with the poor feature choice. In any case, even though our parameter number is less than the clear cut case above, there is still left over variance in our features which can be devoted to fitting noise.</p>
<p>Would there overfitting with imperfectly correlated features if we had no noise? Justify your answer</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Probably not - although features might diverge or become zero where the test data is located, we cannot be overfitting to noise if there is no noise.</p>
</div>
</section>
<section id="overfitting-conclusion">
<h3><span class="section-number">3.2.2. </span>Overfitting Conclusion<a class="headerlink" href="#overfitting-conclusion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Overfitting is inevitable in real data because we cannot avoid noise and rarely have the perfect features.</p></li>
<li><p>Overfitting can be assessed by splitting our data into a train and test split, which mimics how we would use the model (i.e., on unseen data).</p></li>
<li><p>Overfitting is especially affected by having too many features or features that donâ€™t correlate well with the labels.</p></li>
<li><p>We can identify overfitting from a loss curve which shows the testing loss rising while training loss is decreasing.</p></li>
</ul>
</section>
</section>
<section id="exploring-effect-of-feature-number">
<h2><span class="section-number">3.3. </span>Exploring Effect of Feature Number<a class="headerlink" href="#exploring-effect-of-feature-number" title="Link to this heading">#</a></h2>
<p>Weâ€™ve seen that overfitting is sensitive to the number and choice of features. Feature selection is a critical decision in supervised learning. Weâ€™ll return to the solubility dataset to discuss this. It has 17 molecular descriptors, but these are just a small fraction of the possible molecular descriptors that can be used. For example, there is a software called <a class="reference external" href="https://chm.kode-solutions.net/products_dragon.php">Dragon</a> that can compute over 5,000 descriptors. You can also create linear combinations of descriptors and pass them through functions. Then there is the possibility of experimental data, data from molecular simulations, and from quantum calculations. There is essentially an unlimited number of possible molecular descriptors. Weâ€™ll start this chapter by exploring what effect of number of features (dimension of features) has on the data.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Descriptor</strong> is chemistry and materials specific word for feature. It pre-dates the word features and comes from the field of â€œqauntitative-structure activity relationshipâ€ (QSAR), which has a long history in drug design and molecular design.</p>
</aside>
<p>We are now working with a real dataset, which means there is randomness from which features we choose, which training data we choose, and randomness in the labels themselves. In the results below, they are averaged over possible features and possible training data splits to deal with this. Thus the code is complex. You can see it on <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/ml/regression.ipynb">the Github repository</a>, but Iâ€™ve omitted it here for simplicity.</p>
<figure class="align-default" id="small-feature-number">
<img alt="../_images/b87a19e28316efc6d2f4ceb2c4c915843e58c7c70acd94769bf7a3e42140e53c.png" src="../_images/b87a19e28316efc6d2f4ceb2c4c915843e58c7c70acd94769bf7a3e42140e53c.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Effect of feature number on 25 training data points averaged over 10 data samples/feature choices combinations. Notice there is not a significant change when the number of features crosses the number of data points.</span><a class="headerlink" href="#small-feature-number" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#small-feature-number"><span class="std std-numref">Fig. 3.1</span></a> shows the effect of choosing different features on both the loss on training data and the loss on test data. There are three regimes in this plot. At 1-3 features, we are <strong>underfit</strong> meaning both the training and testing losses could be improved with more features or more training. In this case, it is because there are too few features. Until about 10 features, we see that adding new features slightly improves training data but doesnâ€™t help test data meaning weâ€™re probably slightly overfitting. Then at 10, there is a large increase as we move to the overfit regime. Finally at about 30 features, our model is no longer converging and training loss rises because it is too difficult to train the increasingly complex model. â€œDifficultâ€ here is a relative term; you can easily train for more time on this simple model but this is meant as an example.</p>
<figure class="align-default" id="large-feature-number">
<img alt="../_images/ce2b2aac8f8c0dc00960e0c994d67331652bbcbbbc5cd2ff7d02c28460f34abd.png" src="../_images/ce2b2aac8f8c0dc00960e0c994d67331652bbcbbbc5cd2ff7d02c28460f34abd.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Effect of feature number on 250 training data points averaged over 10 data samples/feature choices combinations.</span><a class="headerlink" href="#large-feature-number" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#large-feature-number"><span class="std std-numref">Fig. 3.2</span></a> shows the same analysis but for 250 train and 250 test data. The accuracy on test data is better (about 1.9 vs 2.5). There is not much overfitting visible here. The model is clearly underfit until about 10 features and then each additional feature has little effect. Past 20 features, we again see an underfit because the model is not trained well. This could fixed by adding more training steps.</p>
<hr class="docutils" />
<p>Increasing feature numbers is useful up to a certain point. Although some methods are unstable when the number of features is exactly the same as the number of data points, there is reason overfitting begins at or near feature numbers equal to the number of data points. Overfitting can disappear at large feature numbers because of model size and complexity. Here there is also a risk of underfitting.</p>
<p>The risk of overfitting is lower as your dataset size increases. The reason for this is that the noise becomes smaller than the effect of labels on training as you increase data points. Recall from the Central Limit Theorem that reducing noise by a factor of 10 requires 100 times more data, so this is not as efficient as choosing better features. Thinking about these trade-offs, to double your feature number you should quadruple the number of data points to reduce the risk of overfitting. Thus there is a strong relationship between how complex your model can be, the achievable accuracy, the data required, and the noise in labels.</p>
</section>
<section id="bias-variance-decomposition">
<h2><span class="section-number">3.4. </span>Bias Variance Decomposition<a class="headerlink" href="#bias-variance-decomposition" title="Link to this heading">#</a></h2>
<p>We will now try to be more systematic about this difference in model performance between training and testing data. Consider an unseen label <span class="math notranslate nohighlight">\(y\)</span> and our model <span class="math notranslate nohighlight">\(\hat{f}(\vec{x})\)</span>. Our error on the unseen label is:</p>
<div class="math notranslate nohighlight" id="equation-exp-error">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-exp-error" title="Link to this equation">#</a></span>\[    E\left[\left(y - \hat{f}(\vec{x})\right)^2\right]\]</div>
<p>What is the expectation over? For now, letâ€™s just assume the only source of randomness is in the noise from the label (recall <span class="math notranslate nohighlight">\(y = f(\vec{x}) + \epsilon\)</span>). Then our expression becomes:</p>
<div class="amsmath math notranslate nohighlight" id="equation-80cee322-c25f-4cc8-8dfe-4af0091556c6">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-80cee322-c25f-4cc8-8dfe-4af0091556c6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    E\left[\left(y - \hat{f}(\vec{x})\right)^2\right] =  E\left[y^2\right] + E\left[\hat{f}(\vec{x})^2\right] - 2 E\left[y\hat{f}(\vec{x})\right]
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-d303c4da-b16f-4fc2-bad1-ea970f5ebc61">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-d303c4da-b16f-4fc2-bad1-ea970f5ebc61" title="Permalink to this equation">#</a></span>\[\begin{equation}
    E\left[\left(y - \hat{f}(\vec{x})\right)^2\right] =  E\left[\left(f(\vec{x}) - \epsilon\right)^2\right] + \hat{f}(\vec{x})^2 - 2 E\left[\left(f(\vec{x}) - \epsilon\right)\right]\hat{f}(\vec{x})
\end{equation}\]</div>
<p>I have dropped the expectations over deterministic expression <span class="math notranslate nohighlight">\(\hat{f}\)</span>. You can continue this, again dropping any <span class="math notranslate nohighlight">\(E[f(\vec{x})]\)</span> terms and using the definition of <span class="math notranslate nohighlight">\(\epsilon\)</span>, a zero mean normal distribution with standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. You will arrive at:</p>
<div class="math notranslate nohighlight" id="equation-exp-error-nod">
<span class="eqno">(3.7)<a class="headerlink" href="#equation-exp-error-nod" title="Link to this equation">#</a></span>\[    E\left[\left(y - \hat{f}(\vec{x})\right)^2\right] = \left(f(\vec{x}) - \hat{f}(\vec{x})\right)^2 + \sigma^2\]</div>
<p>This expression means the best we can do on an unseen label is the noise of the label. This is very reasonable, and probably matches your intuition. The best you can do is match exactly the noise in the label when you have a perfect agreement between <span class="math notranslate nohighlight">\(f(\vec{x})\)</span>  and <span class="math notranslate nohighlight">\(\hat{f}(\vec{x})\)</span></p>
<p><em>However, this analysis did not account for the fact our choice of training data is random</em>. Things become more complex when we consider that our choice of training data is random. Return to Equation <a class="reference internal" href="#equation-exp-error">(3.4)</a> and now replace <span class="math notranslate nohighlight">\(\hat{f}\left(\vec{x}\right)\)</span> with <span class="math notranslate nohighlight">\(\hat{f}\left(\vec{x}; \mathbf{D}\right)\)</span> where <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is a random variable indicating the random data sample. You can find a complete derivation on <a class="reference external" href="https://en.wikipedia.org/wiki/Bias-variance_tradeoff">Wikipedia</a>. The key change is that  <span class="math notranslate nohighlight">\(\left(f(\vec{x}) - \hat{f}\left(\vec{x}; \mathbf{D}\right)\right)^2\)</span> is now a random variable. Equation <a class="reference internal" href="#equation-exp-error-nod">(3.7)</a> becomes:</p>
<div class="math notranslate nohighlight" id="equation-bv">
<span class="eqno">(3.8)<a class="headerlink" href="#equation-bv" title="Link to this equation">#</a></span>\[    E\left[\left(y - \hat{f}(\vec{x})\right)^2\right] = E\left[f(\vec{x}) - \hat{f}\left(\vec{x}; \mathbf{D}\right)\right]^2 + 
    E\left[\left(E\left[\hat{f}\left(\vec{x}; \mathbf{D}\right)\right] - \hat{f}\left(\vec{x}; \mathbf{D}\right)\right)^2\right] + \sigma^2\]</div>
<p>This expression is the most important equation for understanding ML and deep learning training. The first term in this expression is called <strong>bias</strong> and captures how far away our model is from the correct function <span class="math notranslate nohighlight">\(f(\vec{x})\)</span>. This is the expected (average) loss we get given a random dataset evaluated on a new unseen data point. You may think this the most important quantity â€“ expected difference between the true function and our model on a new data point. However, bias does not determine the expected error on an unseen data point alone, there other terms.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>In Equation<a class="reference internal" href="#equation-bv">(3.8)</a> <span class="math notranslate nohighlight">\(\vec{x}\)</span> is a fixed quantity, unlike what you may be used to in probability. The actual random variables are <span class="math notranslate nohighlight">\(\epsilon\)</span> (noise in label) and <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> (our chosen training data).</p>
</aside>
<p>The second term is surprising. It is called the <strong>variance</strong> and captures how much change at the unseen data point <span class="math notranslate nohighlight">\((\vec{x},y)\)</span> there is due to changes in the random variable <span class="math notranslate nohighlight">\(\mathbf{D}\)</span>. What is surprising is that the expected loss depends on the variance of the learned model. Think carefully about this. A model which is highly sensitive to which training data is chosen has a high expected error on test data. Furthermore, remember that this term <strong>variance</strong> is different than variance in a feature. It captures how the model value changes at a particular <span class="math notranslate nohighlight">\(\vec{x}\)</span> as a function of changing the training data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are three sources of randomness in the expectation: the choice of test data, the label noise, and the choice of training data. However, once you pick the training data, the test data is fixed so we do not indicate or worry about this. A quantity like <span class="math notranslate nohighlight">\(E[\hat{f}(\vec{x})]\)</span> means splitting your data every possible way, fitting the models, then computing the value <span class="math notranslate nohighlight">\(\hat{f}(\vec{x})\)</span> on the unseen test <span class="math notranslate nohighlight">\(\vec{x}\)</span>. Then you take the average over the unseen test values. You can also skip the last step and leave <span class="math notranslate nohighlight">\(E[\hat{f}(\vec{x})]\)</span> as a function of <span class="math notranslate nohighlight">\(\vec{x}\)</span>, which is what is plotted in <a class="reference internal" href="#low-var"><span class="std std-numref">Fig. 3.3</span></a>  and <a class="reference internal" href="#high-var"><span class="std std-numref">Fig. 3.4</span></a>.</p>
</div>
<figure class="align-default" id="low-var">
<img alt="../_images/10364c445759955bfd2d7d2d0a0f17f80e7d58c72868e9555c277c4a9fde9a3b.png" src="../_images/10364c445759955bfd2d7d2d0a0f17f80e7d58c72868e9555c277c4a9fde9a3b.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.3 </span><span class="caption-text">A single feature fit to the polynomial model example above. The left panel shows a single train/test split and the resulting model fit. The right panel shows the result of many fits. The model variance is the variance across each of those model fits and the bias is the agreement of the average model. It can be seen that this model has low variance but poor average agreement (high bias).</span><a class="headerlink" href="#low-var" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>These three terms: noise, bias, and variance set the minimum value for test error. Noise is set by your data and not controllable. However, bias and variance are controllable. What does a high bias, low variance model look like? A 1D linear model is a good example. See <a class="reference internal" href="#low-var"><span class="std std-numref">Fig. 3.3</span></a>. It has one parameter so a sample of data points gives a consistent estimate. However, a 1D model cannot capture the true <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> so it has a large average error (bias) at a given point. What does a low bias, high variance model look like? An overfit model like the one shown in <a class="reference internal" href="#high-var"><span class="std std-numref">Fig. 3.4</span></a>. It has extreme outliers on test data, but on average it actually has a low bias.</p>
<figure class="align-default" id="high-var">
<img alt="../_images/19cc78b01f5f290589dda66ea9368f08bb97739bae911935a22816e56650ec94.png" src="../_images/19cc78b01f5f290589dda66ea9368f08bb97739bae911935a22816e56650ec94.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.4 </span><span class="caption-text">A 7 feature fit to the polynomial model example above. The left panel shows a single train/test split and the resulting model fit. The right panel shows the result of many fits. The model variance is the variance across each of those model fits and the bias is the agreement of the average model. It can be seen that this model has high variance but good average agreement (low bias).</span><a class="headerlink" href="#high-var" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>The Tradeoff</strong></p>
<figure class="align-default" id="bv">
<img alt="../_images/d29ad17a9ce3afc30068bc8101da15d5a7e0ac99a75126e3b6c18759ab4a5498.png" src="../_images/d29ad17a9ce3afc30068bc8101da15d5a7e0ac99a75126e3b6c18759ab4a5498.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.5 </span><span class="caption-text">The bias, variance, and fit on test values for the polynomial example averaged across 2,500 train/test splits. As the number of features increases, variance increases and bias decreases. There is a minimum at 4 features. The plot stops at 5 because the variance becomes very large beyond 5.</span><a class="headerlink" href="#bv" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The way to change bias and variance is through <strong>model complexity</strong>, which is feature number in our linear models. Increasing model complexity reduces bias and increases variance. There is an optimum for our polynomial example, shown in <a class="reference internal" href="#bv"><span class="std std-numref">Fig. 3.5</span></a>. Indeed this is true of most ML models, although it can be difficult to cleanly increase model complexity and keep training converged. However, this is <a class="reference external" href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update">not typically true in deep learning with neural networks</a><span id="id3">[<a class="reference internal" href="#id48" title="Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks. arXiv preprint arXiv:1810.08591, 2018.">NMB+18</a>]</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The biasâ€“variance tradeoff for model complexity is based on experience. The decomposition above does not prove a tradeoff, just that you can split these two terms. Intentionally underfitting, adding noise, and exchanging one feature for another are all ways to affect bias and variance without adjusting complexity. Also, sometimes you can just improve both with better models.</p>
</div>
<p>The biasâ€“variance tradeoff is powerful for explaining the intuition weâ€™ve learned from examples above. Large datasets reduce model variance, explaining why it is possible to increase model complexity to improve model accuracy only with larger datasets. Overfitting reduces bias at the cost of high variance. Not training long enough increases bias, but reduces variance as well since you can only move so far from your starting parameters.</p>
</section>
<section id="regularization">
<h2><span class="section-number">3.5. </span>Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h2>
<p>Adding features is a challenging way to exchange model bias and variance because it comes in discrete steps and some features are just better than others. A different way is to use a complex model (all features) but reduce variance through <strong>regularization</strong>. Regularization is the addition of an extra term to your loss function that captures some unwanted property about your model that you want to minimize.</p>
<section id="l2-regularization">
<h3><span class="section-number">3.5.1. </span>L2 Regularization<a class="headerlink" href="#l2-regularization" title="Link to this heading">#</a></h3>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You can add the bias <span class="math notranslate nohighlight">\(b\)</span> to the regularization term, but this should
only be done if you have some prior belief that the bias should be 0 â€“ like if it represents some physical quantity that should be minimized. Otherwise minimizing <span class="math notranslate nohighlight">\(b\)</span> has no effect on overfitting and so is not part of regularization.</p>
</aside>
<p>Our first example is the magnitude of fit coefficients. The magnitude of the coefficients is <span class="math notranslate nohighlight">\(\sum_k w_k^2\)</span> where <span class="math notranslate nohighlight">\(w_k\)</span> the index of a single coefficient. We add this to our loss function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2396e2c8-65a4-4849-b54c-7cfe931e2687">
<span class="eqno">(3.9)<a class="headerlink" href="#equation-2396e2c8-65a4-4849-b54c-7cfe931e2687" title="Permalink to this equation">#</a></span>\[\begin{equation}
    L = \frac{1}{N}\sum_i^N \left[y_i - \hat{f}(\vec{x}_i, \vec{w}, b)\right]^2 + \lambda \sum_k w_k^2
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is our strength of regularization. By changing <span class="math notranslate nohighlight">\(\lambda\)</span>, we control how large the magnitude of our parameters are and that directly reduces the variance. You can see the result in <a class="reference internal" href="#l2"><span class="std std-numref">Fig. 3.6</span></a> for our polynomial example. Increasing the strength of regularization decreases variance at the cost of increasing model bias. Remember in deep learning there isnâ€™t as much of a tradeoff and often you just get a reduction in variance with no degradation in bias. Adding L2 regularization with a linear model has a specific name: <strong>Ridge Regression</strong>.</p>
<figure class="align-default" id="l2">
<img alt="../_images/ddfae54a2c0dcb4d5fedb1869f2b72fdce03239399bb5d6445c7e1532359c767.png" src="../_images/ddfae54a2c0dcb4d5fedb1869f2b72fdce03239399bb5d6445c7e1532359c767.png" />
<figcaption>
<p><span class="caption-number">Fig. 3.6 </span><span class="caption-text">A 7 feature fit to the polynomial model example above with increasing strength of regularization. The vertical bars indicate standard deviation of model at each point.</span><a class="headerlink" href="#l2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Why does this work? Look at the gradient of a particular weight of our new loss function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ccc2ce64-d271-4bcb-bd54-9a802212a5c5">
<span class="eqno">(3.10)<a class="headerlink" href="#equation-ccc2ce64-d271-4bcb-bd54-9a802212a5c5" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{\partial L}{\partial w_4} = \frac{2}{N}\sum_i^N \left[y_i - \hat{f}(\vec{x}_i, \vec{w}, b)\right]\frac{\partial \hat{f}(\vec{x}_i, \vec{w}, b)}{\partial w_4} + 2\lambda w_4
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_4\)</span> is one of our weights. The first term on the right-hand side accounts for how <span class="math notranslate nohighlight">\(w_4\)</span> affects our accuracy, like usual. The second term is from the regularization. You can see that the gradient is just the value of weight times a constant. Letâ€™s contract the first term into a variable called <span class="math notranslate nohighlight">\(g_{w_4}\)</span> and look at how this new gradient affects our updates to <span class="math notranslate nohighlight">\(w_4\)</span>. Our gradient descent update of <span class="math notranslate nohighlight">\(w_4\)</span> becomes:</p>
<div class="amsmath math notranslate nohighlight" id="equation-501567ac-1405-464e-83a9-6e9a284dd59e">
<span class="eqno">(3.11)<a class="headerlink" href="#equation-501567ac-1405-464e-83a9-6e9a284dd59e" title="Permalink to this equation">#</a></span>\[\begin{equation}
    w_4' = w_4 -\eta g_{w_4} - 2\eta\lambda w_4
\end{equation}\]</div>
<p>So our regularization pushes <span class="math notranslate nohighlight">\(w_4'\)</span> to always have a lower magnitude. If <span class="math notranslate nohighlight">\(w_4' = 2.5\)</span>, the update will include a term of <span class="math notranslate nohighlight">\(-2\eta \lambda 2.5\)</span>, pushing our weight value closer to zero. This means our weights always are pushed towards zero. Of course the term coming from model error (<span class="math notranslate nohighlight">\(g_{w_4}\)</span>) also has an effect so that we end up at a balance of lower magnitude weights and model error. We control that balance through the <span class="math notranslate nohighlight">\(\lambda\)</span> term.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The terms L1 and L2 come from the definition of norms. They indicate the coefficient
used in the norm: <span class="math notranslate nohighlight">\((\sum_i x_i^p)^{1/p}\)</span>, where <span class="math notranslate nohighlight">\(p =1\)</span> for L1 and <span class="math notranslate nohighlight">\(p = 2\)</span> for L2. Others exist, like <span class="math notranslate nohighlight">\(p = 0\)</span> which counts dimension and <span class="math notranslate nohighlight">\(p = \infty\)</span> which takes the maximum element. The â€œLâ€ comes from the word Lebesgue integral, via a confusing path.</p>
</aside>
</section>
<section id="l1-regularization">
<h3><span class="section-number">3.5.2. </span>L1 Regularization<a class="headerlink" href="#l1-regularization" title="Link to this heading">#</a></h3>
<p>L1 regularization changes our loss to be the following:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b11e277c-e232-45a8-8e68-61e9b83289dc">
<span class="eqno">(3.12)<a class="headerlink" href="#equation-b11e277c-e232-45a8-8e68-61e9b83289dc" title="Permalink to this equation">#</a></span>\[\begin{equation}
    L = \frac{1}{N}\sum_i^N \left[y_i - \hat{f}(\vec{x}_i, \vec{w}, b)\right]^2 + \lambda \sum_k \left|w_k\right|
\end{equation}\]</div>
<p>It may appear at first that this is identical to L2. In fact, the L1 regularization has a powerful benefit: it induces sparsity. L2 just causes regression coefficients to be on average lower, but L1 forces some coefficients to be 0. This gives us a kind of â€œautomaticâ€ feature selection. This is called <strong>Lasso Regression</strong> when you combine L1 regularization with linear regression.</p>
<p>As far as choosing which regularization to use, Iâ€™ll <a class="reference external" href="https://stats.stackexchange.com/a/184022">quote Frank Harrell</a>, a biostatistics professor at Vanderbilt:</p>
<blockquote>
<div><p>Generally speaking if you want optimum prediction use L2. If you want parsimony at some sacrifice of predictive discrimination use L1. But note that the parsimony can be illusory, e.g., repeating the lasso process using the bootstrap [introduced below] will often reveal significant instability in the list of features â€œselectedâ€ especially when predictors are correlated with each other.</p>
</div></blockquote>
</section>
</section>
<section id="strategies-to-assess-models">
<h2><span class="section-number">3.6. </span>Strategies to Assess Models<a class="headerlink" href="#strategies-to-assess-models" title="Link to this heading">#</a></h2>
<p>We will now discuss more ways to assess model performance. These are more robust approaches to assess loss on testing data.</p>
<section id="k-fold-cross-validation">
<h3><span class="section-number">3.6.1. </span>k-Fold Cross-Validation<a class="headerlink" href="#k-fold-cross-validation" title="Link to this heading">#</a></h3>
<p>The biasâ€“variance decomposition shows that our testing error is sensitive to what training data has been chosen. The expected mean test error <span class="math notranslate nohighlight">\(E\left[\left(y - \hat{f}(\vec{x})\right)^2\right]\)</span> depends on the label noise <strong>and</strong> the way we split our data into training and testing data. Thus far, weâ€™ve only gotten a single sample from this expectation by splitting. One way to better estimate the value on unseen data is to repeat the process of splitting data into training and testing multiple times. This is called <strong>k-fold</strong> cross-validation, where <span class="math notranslate nohighlight">\(k\)</span> is the number of times you repeat the process. k-fold cross-validation is useful because certain high-variance model choices can give different testing errors depending on the train/test split. k-fold also provides multiple samples so that you can estimate the <strong>uncertainty</strong> in testing error. As all things to do with model variance, the smaller the dataset the more important this is. Typically with very large datasets k-fold cross-validation is not done because label noise dominates and testing a model k times can be time-consuming.</p>
<p>k-fold cross-validation has a specific process for splitting testing and training data. What we did previously was split into a 50/50 split of training and testing. In k-fold, we split our data into k segments. Then we train on k-1 segments and test on the last segment. You can do this k-ways. For example, with K = 3 you would split your data into A, B, C. The first train/test split would be A, B for training and C for testing. Then B, C for training and A for testing. The last would be A, C for training and B for testing. Following this procedure means that your percentage split will be 90/10 for <span class="math notranslate nohighlight">\(k = 10\)</span> and 50/50 for <span class="math notranslate nohighlight">\(k = 2\)</span>. This has a disadvantage that the number of estimates for testing error depends on size of train/test split. For example, you cannot get 10 estimates for an 80/20 split. An 80/20 split means exactly 5-fold cross-validation. Weâ€™ll see other methods that relax this later on. The 80/20 is a typical rule that balances having enough data for good training and enough to robustly assess how well your model performs.</p>
<p>Letâ€™s now use k-fold cross-validation in two examples: our full dataset and a smaller 25 data point sample. Rather than using gradient descent here, weâ€™ll just use the pseudo-inverse to keep our code simple. The pseudo-inverse is the least-squares solution.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)</span>
<span class="c1"># make indices for the k segments</span>
<span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">))</span>
<span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="c1"># slice out segments</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">:</span> <span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">soldata</span><span class="p">[:</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="p">:]])</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># compute coefficients</span>
    <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># compute intercept (b)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="c1"># compute test erropr</span>
    <span class="n">error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Split Number&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">-fold cross-validation of soldata&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/0c676f9189207d449ef457c4c4a64d18aeca437001864c6fdc211920c59b9208.png" src="../_images/0c676f9189207d449ef457c4c4a64d18aeca437001864c6fdc211920c59b9208.png" />
</div>
</div>
<p>The final answer in this case is the average of these values: <span class="pasted-text">2.97</span><span class="math notranslate nohighlight">\(\pm\)</span><span class="pasted-text">2.10</span>. The advantage of the k-fold is that we can report standard deviation like this.</p>
<p>Now what effect does k have on the test error? Letâ€™s see how our choice of k matters</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">error_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">k_error</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="c1"># slice out segments</span>
        <span class="n">test</span> <span class="o">=</span> <span class="n">soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">:</span> <span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">soldata</span><span class="p">[:</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="p">:]])</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="c1"># compute coefficients</span>
        <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># compute intercept (b)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
        <span class="c1"># compute test error</span>
        <span class="n">k_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_error</span><span class="p">))</span>
    <span class="n">error_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">k_error</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="n">error</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">error_std</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;k-fold cross-validation of soldata&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/8513a72cdb033c4ce59f9496bbf97289cb262b747714f7be73978211cff3ad80.png" src="../_images/8513a72cdb033c4ce59f9496bbf97289cb262b747714f7be73978211cff3ad80.png" />
</div>
</div>
<p>As you can see, there is not much sensitivity to k. This is good, because k is mostly arbitrary. Larger k means more samples, but each test data is smaller so that these two effects should balance out.</p>
<p>Large datasets are not that sensitive because the training and testing splits are large. Let us examine what happens with <span class="math notranslate nohighlight">\(N = 25\)</span>, a realistic case in chemistry data. Weâ€™ll just pick 25 data points at the beginning and not change that choice, mocking what would happen in a real example.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_soldata</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">small_soldata</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">error_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">k_error</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="c1"># slice out segments</span>
        <span class="n">test</span> <span class="o">=</span> <span class="n">small_soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">:</span> <span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">small_soldata</span><span class="p">[:</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">small_soldata</span><span class="p">[</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="p">:]])</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="c1"># compute coefficients</span>
        <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># compute intercept (b)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
        <span class="c1"># compute test erropr</span>
        <span class="n">k_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_error</span><span class="p">))</span>
    <span class="n">error_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">k_error</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="n">error</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">error_std</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;k-fold cross-validation of soldata subsample&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/06a587455ec6b8acfdf368949bd36f50f3dcf7f93b794c486e3e74cdfd8377ab.png" src="../_images/06a587455ec6b8acfdf368949bd36f50f3dcf7f93b794c486e3e74cdfd8377ab.png" />
</div>
</div>
<p>Our results are a little sensitive to the choice of <span class="math notranslate nohighlight">\(k\)</span>. Now why might test error decrease? Remember that a larger <span class="math notranslate nohighlight">\(k\)</span> means <em>more</em> data points for training. This did not matter above when we had 10,000 data points. Now it is very importatnt, since we only have 25 data points. Thus larger k means more training data.</p>
</section>
<section id="leave-one-out-cv">
<h3><span class="section-number">3.6.2. </span>Leave-one-out CV<a class="headerlink" href="#leave-one-out-cv" title="Link to this heading">#</a></h3>
<p>Larger k means more training data, so what is the largest it can be? Remember that k is the number segments in your data. So <span class="math notranslate nohighlight">\(k = N\)</span> is the max, where each data point is a segement. This is called <strong>leave-one-out cross-validation</strong> (LOOCV). It creates <span class="math notranslate nohighlight">\(N\)</span> different models, one for each data point left out, and so is only used for small datasets. Thus the advantage of LOOCV is it maximizes training data, but maximizes the number of times the model needs to be trained.</p>
</section>
</section>
<section id="computing-other-measures">
<h2><span class="section-number">3.7. </span>Computing Other Measures<a class="headerlink" href="#computing-other-measures" title="Link to this heading">#</a></h2>
<p>Using LOOCV and k-fold cross-validation, weâ€™re able to predict test error. This â€œtest errorâ€ is specifically an expected error on an unseen data point. Now how do we actually treat a new data point? What will we report as the certainty in a new point? The test error? Weâ€™ll call this point the <strong>prediction point</strong> and weâ€™ll try to estimate the quantiles of this point. Quantiles are the building blocks for confidence intervals. Recall that confidence intervals allow us to report our model prediction as <span class="math notranslate nohighlight">\(4.3 \pm 0.2\)</span>, for example.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Classically bootstrap resampling and <strong>jacknife</strong>, its predecessor, are used for estimating variance in model parameters (i.e., model variance). However, they are more commonly used in ML for predicting confidence intervals and/or test error for new points (also called generalization error).</p>
</aside>
<section id="bootstrap-resampling">
<h3><span class="section-number">3.7.1. </span>Bootstrap Resampling<a class="headerlink" href="#bootstrap-resampling" title="Link to this heading">#</a></h3>
<p>To estimate quantiles, we need to have a series of observations of predictions from the prediction point <span class="math notranslate nohighlight">\(\hat{f}(\vec{x}')\)</span>, where <span class="math notranslate nohighlight">\(\vec{x}'\)</span> is the prediction point. For example, we could do 5-fold cross-validation and have 5 estimates of <span class="math notranslate nohighlight">\(\hat{f}_k(\vec{x}')\)</span> and could estimate the quantiles using a t-statistic. Instead, weâ€™ll use a method called <strong>bootstrap resampling</strong> which removes the restriction that we can only use <span class="math notranslate nohighlight">\(1 - 1 / k\)</span> of the training data. Bootstrap resampling is a general process for estimating uncertainty for empirical statistics without assuming a probability distribution (i.e., non-parametric). In bootstrap resampling, we create as many as desired new training datasets that are the same size as the original by sampling <strong>with replacement</strong> from the original dataset. That means our new dataset has fewer members than the original and makes up the difference with duplicates. Letâ€™s see an example. If your training dataset originally has data A, B, C, D, E, our bootstrap resampled training data is:</p>
<ol class="arabic simple">
<li><p>A, B, B, D, E</p></li>
<li><p>B, C, C, C, E</p></li>
<li><p>A, B, D, E, E</p></li>
<li><p>A, B, C, D, E</p></li>
<li><p>A, A, C, C, D</p></li>
</ol>
<p>and so forth. The â€œwith replacementâ€ means that we allow repeats. This gives some variation to our training data. It also means we can generate <span class="math notranslate nohighlight">\(2^N\)</span> new datasets, which is practically as many as we want. Letâ€™s see now how we could use this to quantile the estimate for a prediction on a test point. Weâ€™ll set <span class="math notranslate nohighlight">\(N = 1000\)</span> and do bootstrap resampling for 100 iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create training data and 1 test point</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="c1"># this line gets the data for our example</span>
<span class="c1"># it is not the bootstrap resampling</span>
<span class="n">tmp</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">small_soldata</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span>
<span class="n">predict_point</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
    <span class="c1"># choose with replacement indices to make new dataset</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">small_soldata</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># compute coefficients</span>
    <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># compute intercept (b)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="c1"># compute test prediction</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predict_point</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="c1"># compute quantiles (lower, median, upper)</span>
<span class="n">qint</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="c1"># compute avg distance from median to report +/-</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s1">&#39;prediction = </span><span class="si">{</span><span class="n">qint</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="p">(</span><span class="n">qint</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">qint</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, label = </span><span class="si">{</span><span class="n">predict_point</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>prediction = -4.08 +/- 0.41, label = -3.45
</pre></div>
</div>
</div>
</div>
<p>The resulting prediction has confidence intervals, thanks to the bootstrap resampling. This approach has a few disadvantages though. The first is that we need to produce and keep 100 models, one for each bootstrap resample. Of course you could choose fewer, but you need to have enough for good statistics.</p>
<p>Another issue is that this process does not give a reportable test error. We could further split our data again and do k-fold cross-validation on this approach to get test error. However, this is a bit overly complex and then weâ€™ll be at a similar problem that weâ€™ll have k sets of 100 models and itâ€™s not obvious how to combine them. These prediction intervals also under-estimate the model bias, because it has no estimate of the test error. It only accounts for variation due to training data. Using the language above, it only accounts for model variance but not model bias.</p>
<p>Bootstrap resampling is still an excellent technique that is used often to estimate uncertainties. However, it is not a great choice for estimating model error on unseen datapoints.</p>
</section>
<section id="jacknife">
<h3><span class="section-number">3.7.2. </span>Jacknife+<a class="headerlink" href="#jacknife" title="Link to this heading">#</a></h3>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>There is a method called Jacknife, which does not compute multiple predictions. It computes the residuals as mentioned above, but it trains one final model on all data. Since it requires you to compute all <span class="math notranslate nohighlight">\(N\)</span> models to get the residuals, it is preferred to just use Jacknife+ which is more robust.</p>
</aside>
<p>An alternative approach that accounts for model variance like the bootstrap method and model bias like the k-fold cross-validation method is called <strong>Jacknife+</strong> <span id="id4">[<a class="reference internal" href="#id18" title="Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inference with the jackknife+. arXiv preprint arXiv:1905.02928, 2019.">BCRT19</a>]</span>.  Jacknife+ carries strong guarantees about accuracy of the confidence intervals generated, regardless of the underlying data or model. The change now is that we use LOOCV to create an ensemble of models (although you can subsample down if you do not want N of them) and also compute the modelsâ€™ test error on the withheld test data. The final quantile estimates incorporate the variance from the variety of models (model variance) and also each modelsâ€™ individual test error (model bias).
Specifically, we compute:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4f1f2df5-3c27-4755-a5cc-1bdbbe90f305">
<span class="eqno">(3.13)<a class="headerlink" href="#equation-4f1f2df5-3c27-4755-a5cc-1bdbbe90f305" title="Permalink to this equation">#</a></span>\[\begin{equation}
R_i = \left|y_i - \hat{f}(\vec{x}_i;\,\mathbf{X} \setminus \vec{x}_i )\right|
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X} \setminus \vec{x}_i\)</span> is the dataset to train the <span class="math notranslate nohighlight">\(i\)</span>th model and is the dataset excluding point <span class="math notranslate nohighlight">\((\vec{x}_i, y_i)\)</span>, <span class="math notranslate nohighlight">\(\hat{f}(\vec{x}_i;\,\mathbf{X} \setminus \vec{x}_i ) \)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th model evaluated on point <span class="math notranslate nohighlight">\(\vec{x}_i\)</span>, and <span class="math notranslate nohighlight">\(R_i\)</span> is the residual of model <span class="math notranslate nohighlight">\(i\)</span> computed by taking the difference between the label and prediction on point <span class="math notranslate nohighlight">\(i\)</span>. <span class="math notranslate nohighlight">\(R_i\)</span> encodes how good the <span class="math notranslate nohighlight">\(i\)</span>th model is. We then combine it with the predictions on our new test point <span class="math notranslate nohighlight">\((\vec{x}', y')\)</span> to make our set for quantiling</p>
<div class="amsmath math notranslate nohighlight" id="equation-ffd8ecd4-77db-4bbd-af4d-6e61be67bc3c">
<span class="eqno">(3.14)<a class="headerlink" href="#equation-ffd8ecd4-77db-4bbd-af4d-6e61be67bc3c" title="Permalink to this equation">#</a></span>\[\begin{equation}
q_1 = \left\{ \hat{f}(\vec{x}';\,\mathbf{X} \setminus \vec{x}_i ) - R_i\right\}
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-e52ca243-f6e6-4646-87db-01cfd76a33ae">
<span class="eqno">(3.15)<a class="headerlink" href="#equation-e52ca243-f6e6-4646-87db-01cfd76a33ae" title="Permalink to this equation">#</a></span>\[\begin{equation}
q_2 = \left\{\hat{f}(\vec{x}';\,\mathbf{X} \setminus \vec{x}_i ) + R_i\right\}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(q\)</span> means quantile. The first quantile <span class="math notranslate nohighlight">\(q_1\)</span>, with <span class="math notranslate nohighlight">\( - R_i\)</span>, is how low below the estimate from the <span class="math notranslate nohighlight">\(i\)</span>th model we could expect to see our prediction based on how the <span class="math notranslate nohighlight">\(i\)</span>th model did on its test point. The second set, with <span class="math notranslate nohighlight">\( + R_i\)</span>, is how high above the estimate from the <span class="math notranslate nohighlight">\(i\)</span>th model we could expect to see our prediction based on how the <span class="math notranslate nohighlight">\(i\)</span>th model did on its test point. To compute our final value, we take the median of <span class="math notranslate nohighlight">\(\hat{f}(\vec{x}';\,\mathbf{X} \setminus \vec{x}_i )\)</span> and report the lower end of the interval as the 5% quantile of <span class="math notranslate nohighlight">\(q_1\)</span> and the top as the 95% quantile of <span class="math notranslate nohighlight">\(q_2\)</span>. You can see that this method combines the ensemble of prediction models given by bootstrap resampling with the error estimates from LOOCV. Letâ€™s see an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">residuals</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="c1"># make train excluding test point</span>
    <span class="c1"># we just make a set and remove one element from it</span>
    <span class="c1"># and then convert back to list</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">small_soldata</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># compute coefficients</span>
    <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># compute intercept (b)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="c1"># compute test prediction</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predict_point</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="c1"># now compute residual on withtheld point</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">small_soldata</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">residuals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">small_soldata</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]))</span>
<span class="c1"># create our set of prediction - R_i and prediction + R_i</span>
<span class="n">q1</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="o">-</span> <span class="n">ri</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)]</span>
<span class="n">q2</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="o">+</span> <span class="n">ri</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)]</span>
<span class="c1"># compute quantiles (lower, median, upper)</span>
<span class="n">qlow</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">qhigh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q2</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># compute avg distance from medianto report +/-</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s1">&#39;prediction = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> +/- </span><span class="si">{</span><span class="p">(</span><span class="n">qlow</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">qhigh</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, label = </span><span class="si">{</span><span class="n">predict_point</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average test error = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>prediction = -4.08 +/- -3.28, label = -3.45
Average test error = 1.05
</pre></div>
</div>
</div>
</div>
<p>The uncertainty is much higher, which is likely closer to reality. You can see that the residuals add about 1 solubility (model variance) unit and the variability in the data (label noise) adds about 2 solubility units. Jacknife+ should be the preferred method when you have small datasets (1-1000) and can train models quickly enough to be able to compute 1000 of them. You can also replace the exhaustive LOOCV with a random process, where you only do a few iterations (like 25) of LOOCV to avoid computing so many models.</p>
</section>
</section>
<section id="training-data-distribution">
<h2><span class="section-number">3.8. </span>Training Data Distribution<a class="headerlink" href="#training-data-distribution" title="Link to this heading">#</a></h2>
<p>We have come a long ways now. Weâ€™re able to compute test error, identify overfitting, understand model bias and variance, and predict uncertainty on unseen data points. One of the implied assumptions so far is that our splitting of data into training and testing data mimics what it will be like to predict on an unseen data point. More specifically, we assume that testing data comes from the same probability distribution as our training data. This is true when weâ€™re doing the splitting, but is often violated when we actually get new data to make predictions with.</p>
<p>There are specific categories for how we have left the training distribution. <strong>Covariate shift</strong> is when the distribution of features changes. Covariate is another word for features. An example might be that the molecular weights of your molecules are larger in your testing data. The relationship between features and labels, <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> remains the same, but the distribution of features is different. <strong>Label shift</strong> means that the distribution of labels has changed. Perhaps our training data was all very soluble molecules but at test time, weâ€™re examining mostly insoluble molecules. Again, our fundamental relationship  <span class="math notranslate nohighlight">\(f(\vec{x})\)</span> that we try to estimate with our model still holds.</p>
<aside class="margin sidebar">
<p class="sidebar-title">Applicability Domain</p>
<p>Applicability domain is a term from cheminformatics describing
avoiding covariate shift by trying to stay within the training data distribution.</p>
</aside>
<p>There are two common reasons unseen data can be out of the training data distribution. The first is that you are extrapolating to new regions of chemical space. For example, you have training data of drug activities. You make a model that can predict activity. What do you do with the model? You obviously find the highest activity drug molecule. However, this molecule is likely to be unusual and not in your training data. If it was in your training data you would probably already be done â€“ namely, you already synthesized and found a molecule with very high activity. Thus you will be pushing your model to regions outside of your training data. Another reason you can be out of training data is that the way you generated training data is different than how the model is used. For example, perhaps you trained on molecules that do not contain fluorine. Then you try your model on molecules that contain fluorine. Your features will be different than what you observed in training. The result of leaving your training data distribution is that your test error increases and the estimates you provide will be too low.</p>
<section id="leave-one-class-out-cross-validation">
<h3><span class="section-number">3.8.1. </span>Leave One Class Out Cross-Validation<a class="headerlink" href="#leave-one-class-out-cross-validation" title="Link to this heading">#</a></h3>
<p>Thus understanding and assessing training data distribution is an important task. In general, standard models that minimize a loss are poor at predicting extreme values. We will approach this challenge later with specific methods like black-box function optimization. For now, be wary of using your models as tools to find extreme values. The second challenge, that youâ€™re leaving your training data due to how points are generated, can be assessed by computing a more realistic estimate of model error. Namely, your training data is typically gathered (generated) according to a different process than when your model is deployed at test time. This is generalization error, sometimes called <strong>covariate shift</strong>, and we sometimes wish to approximate its effect by simulating different training and testing distributions. This leads us to <strong>leave one class out cross-validation</strong> (LOCOCV).</p>
<p>In LOCOCV, we must first assign a class to each training data point. This is domain specific. It could be based on the molecule. You could use a clustering method. In our case, our solubility data actually is a combination of five other datasets so our data is already pre-classified based on who measured the solubility. We will now perform a kind of k-fold cross-validation, leaving one class out at a time and assessing model error. Weâ€™ll compare this to k-fold cross-validation without classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s see what the groups (classes) are</span>
<span class="n">unique_classes</span> <span class="o">=</span> <span class="n">soldata</span><span class="p">[</span><span class="s2">&quot;Group&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">unique_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;StringArray&gt;
[&#39;G1&#39;, &#39;G3&#39;, &#39;G5&#39;, &#39;G4&#39;, &#39;G2&#39;]
Length: 5, dtype: str
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Leave one class out CV</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">error_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unique_classes</span><span class="p">:</span>
    <span class="c1"># slice out segments</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">soldata</span><span class="p">[</span><span class="s2">&quot;Group&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">]</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">soldata</span><span class="p">[</span><span class="s2">&quot;Group&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">c</span><span class="p">]</span>
    <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># compute coefficients</span>
    <span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># compute intercept (b)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="c1"># compute test erropr</span>
    <span class="n">k_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">k_error</span><span class="p">))</span>
    <span class="n">error_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">k_error</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;test error = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>test error = 50.33
</pre></div>
</div>
</div>
</div>
<p>We computed above what the 5-fold cross-validation is for this data, <span class="pasted-text">3.16</span>. You can see the LOCOCV test error (<span class="pasted-text">50.33</span>) is similar, which means our different data sources agree well. So perhaps on new unseen data we can expect similar (not so great) accuracy. There may be other ways to group this data into classes, like based on molecular weight or which atoms are contained in the molecule. It depends on what you believe to be important. Breaking it down into the constituent datasets, like we did above, is a reasonable approach because it captures how different research groups would measure solubility. It is not always obvious or possible to use LOCOCV, but it should be something you consider to assess out of training data distribution. You can read more about the issue of leaving training data distribution for materials in this recent article <span id="id5">[<a class="reference internal" href="#id30" title="Christopher Sutton, Mario Boley, Luca M Ghiringhelli, Matthias Rupp, Jilles Vreeken, and Matthias Scheffler. Identifying domains of applicability of machine learning models for materials science. Nature Communications, 11(1):1â€“9, 2020.">SBG+20</a>]</span>. You can read more about model selection in general in this recent tutorial article <span id="id6">[<a class="reference internal" href="#id37" title="Sebastian Raschka. Model evaluation, model selection, and algorithm selection in machine learning. arXiv preprint arXiv:1811.12808, 2018.">Ras18</a>]</span>.</p>
</section>
<section id="scaffold-splits">
<h3><span class="section-number">3.8.2. </span>Scaffold splits<a class="headerlink" href="#scaffold-splits" title="Link to this heading">#</a></h3>
<p>For molecular problems, a random train/test split can still leak structural similarity between train and test molecules. A <strong>scaffold split</strong> groups molecules by their core scaffold, often the Bemis-Murcko scaffold <span id="id7">[<a class="reference internal" href="#id32" title="Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of Medicinal Chemistry, 39(15):2887â€“2893, 1996.">BM96</a>]</span> and puts whole scaffold groups into either train or test. This usually gives a harder but more realistic estimate of performance on novel chemistry. Scaffold splits were popularized as a standard benchmark by the MoleculeNet suite<span id="id8">[<a class="reference internal" href="#id31" title="Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513â€“530, 2018.">WRF+18</a>]</span>.</p>
<p>The standard approach places the <strong>rarest</strong> scaffolds into the test set, since those represent the most novel chemistry. The demo below builds Murcko scaffolds from SMILES, accumulates the least-common scaffolds into the test set until we reach about 20% of molecules, and compares that error with a random split of the same size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">smiles_col</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
    <span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;SMILES&quot;</span><span class="p">,</span> <span class="s2">&quot;smiles&quot;</span><span class="p">,</span> <span class="s2">&quot;CanonicalSMILES&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">soldata</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="kc">None</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">murcko_scaffold</span><span class="p">(</span><span class="n">smiles</span><span class="p">):</span>
    <span class="n">mol</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mol</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">MurckoScaffold</span><span class="o">.</span><span class="n">MurckoScaffoldSmiles</span><span class="p">(</span><span class="n">mol</span><span class="o">=</span><span class="n">mol</span><span class="p">)</span>


<span class="n">scaffold_data</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="n">smiles_col</span><span class="p">])</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">scaffold_data</span><span class="p">[</span><span class="s2">&quot;Scaffold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="p">[</span><span class="n">smiles_col</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">murcko_scaffold</span><span class="p">)</span>
<span class="n">scaffold_data</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="p">[</span><span class="n">scaffold_data</span><span class="p">[</span><span class="s2">&quot;Scaffold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">notna</span><span class="p">()]</span>

<span class="n">scaffold_counts</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="p">[</span><span class="s2">&quot;Scaffold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">test_target</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">scaffold_data</span><span class="p">))</span>
<span class="n">test_scaffolds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">running</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># iterate from rarest to most common</span>
<span class="k">for</span> <span class="n">scaffold</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">scaffold_counts</span><span class="o">.</span><span class="n">iloc</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">running</span> <span class="o">&gt;=</span> <span class="n">test_target</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">test_scaffolds</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">scaffold</span><span class="p">)</span>
    <span class="n">running</span> <span class="o">+=</span> <span class="n">count</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="p">[</span><span class="n">scaffold_data</span><span class="p">[</span><span class="s2">&quot;Scaffold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">test_scaffolds</span><span class="p">)]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="p">[</span><span class="o">~</span><span class="n">scaffold_data</span><span class="p">[</span><span class="s2">&quot;Scaffold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">test_scaffolds</span><span class="p">)]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">train_mean</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">train_std</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-</span> <span class="n">train_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">train_std</span>
<span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-</span> <span class="n">train_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">train_std</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
<span class="n">scaffold_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scaffold_data</span><span class="p">))</span>
<span class="n">rand_test_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)]</span>
<span class="n">rand_train_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)</span> <span class="p">:]</span>

<span class="n">random_train</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rand_train_idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">random_test</span> <span class="o">=</span> <span class="n">scaffold_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">rand_test_idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">rand_mean</span> <span class="o">=</span> <span class="n">random_train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">rand_std</span> <span class="o">=</span> <span class="n">random_train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">random_train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">random_train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-</span> <span class="n">rand_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">rand_std</span>
<span class="n">random_test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">random_test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span> <span class="o">-</span> <span class="n">rand_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">rand_std</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random_train</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">random_train</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">random_test</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">random_test</span><span class="p">[</span><span class="s2">&quot;Solubility&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">w</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
<span class="n">random_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scaffold split test MSE: </span><span class="si">{</span><span class="n">scaffold_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random split test MSE:   </span><span class="si">{</span><span class="n">random_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unique scaffolds in test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_scaffolds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test molecules: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)</span><span class="si">}</span><span class="s2">, Train molecules: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Scaffold split test MSE: 5.48
Random split test MSE:   2.83
Unique scaffolds in test: 1650
Test molecules: 1996, Train molecules: 7984
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show some common train scaffolds and rare test scaffolds</span>
<span class="n">train_scaffolds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scaffold_counts</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_scaffolds</span>
<span class="n">train_scaffold_counts</span> <span class="o">=</span> <span class="n">scaffold_counts</span><span class="p">[</span><span class="n">scaffold_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">train_scaffolds</span><span class="p">)]</span>
<span class="n">test_scaffold_counts</span> <span class="o">=</span> <span class="n">scaffold_counts</span><span class="p">[</span><span class="n">scaffold_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">test_scaffolds</span><span class="p">)]</span>

<span class="n">common_train</span> <span class="o">=</span> <span class="n">train_scaffold_counts</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">rare_test</span> <span class="o">=</span> <span class="n">test_scaffold_counts</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="n">scaffold_smiles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">common_train</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">rare_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">scaffold_mols</span> <span class="o">=</span> <span class="p">[</span><span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scaffold_smiles</span><span class="p">]</span>
<span class="n">legends</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Train (n=</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">)&quot;</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">common_train</span><span class="o">.</span><span class="n">values</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
    <span class="sa">f</span><span class="s2">&quot;Test (n=</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">)&quot;</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">rare_test</span><span class="o">.</span><span class="n">values</span>
<span class="p">]</span>

<span class="c1"># filter out any scaffolds that failed to parse</span>
<span class="n">valid</span> <span class="o">=</span> <span class="p">[(</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scaffold_mols</span><span class="p">,</span> <span class="n">legends</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">scaffold_mols</span><span class="p">,</span> <span class="n">legends</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">valid</span><span class="p">)</span> <span class="k">if</span> <span class="n">valid</span> <span class="k">else</span> <span class="p">([],</span> <span class="p">[])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top train scaffolds (most common) vs test scaffolds:&quot;</span><span class="p">)</span>
<span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">Draw</span><span class="o">.</span><span class="n">MolsToGridImage</span><span class="p">(</span>
    <span class="n">scaffold_mols</span><span class="p">,</span> <span class="n">molsPerRow</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">subImgSize</span><span class="o">=</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">250</span><span class="p">),</span> <span class="n">legends</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top train scaffolds (most common) vs test scaffolds:
</pre></div>
</div>
<img alt="../_images/bab45ef151d3d1513b86503d76ba2c9746b8fcb35d9fc04b90cbf41df0093dd8.svg" src="../_images/bab45ef151d3d1513b86503d76ba2c9746b8fcb35d9fc04b90cbf41df0093dd8.svg" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show example molecules from train and test splits</span>
<span class="n">train_examples</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">test_examples</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">example_smiles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_examples</span><span class="p">[</span><span class="n">smiles_col</span><span class="p">])</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">test_examples</span><span class="p">[</span><span class="n">smiles_col</span><span class="p">])</span>
<span class="n">example_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Train&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_examples</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Test&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_examples</span><span class="p">)</span>
<span class="n">example_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">l</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">example_smiles</span><span class="p">,</span> <span class="n">example_labels</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="p">]</span>
<span class="n">example_mols</span><span class="p">,</span> <span class="n">example_legends</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">example_pairs</span><span class="p">)</span> <span class="k">if</span> <span class="n">example_pairs</span> <span class="k">else</span> <span class="p">([],</span> <span class="p">[])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example molecules from each split:&quot;</span><span class="p">)</span>
<span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">Draw</span><span class="o">.</span><span class="n">MolsToGridImage</span><span class="p">(</span>
    <span class="n">example_mols</span><span class="p">,</span> <span class="n">molsPerRow</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">subImgSize</span><span class="o">=</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">250</span><span class="p">),</span> <span class="n">legends</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">example_legends</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Example molecules from each split:
</pre></div>
</div>
<img alt="../_images/33b5134c77c0a2150c2e5d23db065f1c5bbf80ee32e74a1e00f9095392929b58.svg" src="../_images/33b5134c77c0a2150c2e5d23db065f1c5bbf80ee32e74a1e00f9095392929b58.svg" />
</div>
</div>
<p>In many chemistry datasets, the scaffold-split error is higher than the random-split error. That gap is useful: it quantifies how much your model depends on seeing closely related chemotypes during training.</p>
</section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">3.9. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Regression is supervised learning where the labels are real numbers. We only considered scalars</p></li>
<li><p>To assess a regressed model, we split data into training and testing and only report error on testing data</p></li>
<li><p>Overfitting causes a mismatch between training and testing error</p></li>
<li><p>Overfitting can be understood via the bias-variance decomposition</p></li>
<li><p>Increasing model complexity can improve fit (reduce bias), but increases model variance and thus test error</p></li>
<li><p>Regularization is a strategy to decrease model variance. L2 is a good first choice</p></li>
<li><p>More rigorous assessment of models can be done via k-fold cross-validation or Jacknife+ when the training data is small enough that we can train multiple models</p></li>
<li><p>Much of our model assessments depends on the testing data being from the same distribution as the training data (similar values). This is often not true and can be measured with leave-one-class-out cross-validation.</p></li>
<li><p>Scaffold split is one way to predict a realistic test error</p></li>
</ul>
</section>
<section id="exercises">
<h2><span class="section-number">3.10. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="id9">
<h3><span class="section-number">3.10.1. </span>Overfitting<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>What happens if we have redundant features but no noise? Is it possible to overfit?</p></li>
<li><p>We said that increasing dataset size reduces model variance. Show this by using k-fold cross-validation on a few different dataset sizes.</p></li>
</ol>
</section>
<section id="id10">
<h3><span class="section-number">3.10.2. </span>Regularization<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Implement L1 regularization on the solubility data with <span class="math notranslate nohighlight">\(N = 35\)</span> data points. Increase the strength until some feature coefficients (<span class="math notranslate nohighlight">\(w_i\)</span>) go to zero. Which ones are they? Why do you think they go to zero first?</p></li>
<li><p>Repeat 1 with a few different sets of training data. Are your results consistent on which features disappear? Based on your results, do you think there is meaning to the features which go to zero?</p></li>
<li><p>Implement the L-infinity (supremum norm) regularization, which returns the maxmium of the absolute values of the elements.</p></li>
</ol>
</section>
<section id="model-assessment">
<h3><span class="section-number">3.10.3. </span>Model Assessment<a class="headerlink" href="#model-assessment" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Develop the best linear model for the complete solubility dataset and assess using your best judgment. Justify your choice of model and assessment.</p></li>
</ol>
</section>
</section>
<section id="cited-references">
<h2><span class="section-number">3.11. </span>Cited References<a class="headerlink" href="#cited-references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id11">
<div role="list" class="citation-list">
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">SKE19</a><span class="fn-bracket">]</span></span>
<p>MuratÂ Cihan Sorkun, Abhishek Khetan, and SÃ¼leyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. <em>Sci. Data</em>, 6(1):143, 2019. <a class="reference external" href="https://doi.org/10.1038/s41597-019-0151-1">doi:10.1038/s41597-019-0151-1</a>.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Bis06</a><span class="fn-bracket">]</span></span>
<p>ChristopherÂ M Bishop. <em>Pattern recognition and machine learning</em>. springer, 2006.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">NMB+18</a><span class="fn-bracket">]</span></span>
<p>Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks. <em>arXiv preprint arXiv:1810.08591</em>, 2018.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">BCRT19</a><span class="fn-bracket">]</span></span>
<p>RinaÂ Foygel Barber, EmmanuelÂ J Candes, Aaditya Ramdas, and RyanÂ J Tibshirani. Predictive inference with the jackknife+. <em>arXiv preprint arXiv:1905.02928</em>, 2019.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">SBG+20</a><span class="fn-bracket">]</span></span>
<p>Christopher Sutton, Mario Boley, LucaÂ M Ghiringhelli, Matthias Rupp, Jilles Vreeken, and Matthias Scheffler. Identifying domains of applicability of machine learning models for materials science. <em>Nature Communications</em>, 11(1):1â€“9, 2020.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Ras18</a><span class="fn-bracket">]</span></span>
<p>Sebastian Raschka. Model evaluation, model selection, and algorithm selection in machine learning. <em>arXiv preprint arXiv:1811.12808</em>, 2018.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">BM96</a><span class="fn-bracket">]</span></span>
<p>GuyÂ W Bemis and MarkÂ A Murcko. The properties of known drugs. 1. molecular frameworks. <em>Journal of Medicinal Chemistry</em>, 39(15):2887â€“2893, 1996.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">WRF+18</a><span class="fn-bracket">]</span></span>
<p>Zhenqin Wu, Bharath Ramsundar, EvanÂ N Feinberg, Joseph Gomes, Caleb Geniesse, AneeshÂ S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. <em>Chemical science</em>, 9(2):513â€“530, 2018.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Introduction to Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">3.1. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">3.2. Overfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-with-synthetic-data">3.2.1. Overfitting with Synthetic Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-conclusion">3.2.2. Overfitting Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-effect-of-feature-number">3.3. Exploring Effect of Feature Number</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-decomposition">3.4. Bias Variance Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">3.5. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization">3.5.1. L2 Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization">3.5.2. L1 Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strategies-to-assess-models">3.6. Strategies to Assess Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">3.6.1. k-Fold Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cv">3.6.2. Leave-one-out CV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-other-measures">3.7. Computing Other Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap-resampling">3.7.1. Bootstrap Resampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacknife">3.7.2. Jacknife+</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-distribution">3.8. Training Data Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-class-out-cross-validation">3.8.1. Leave One Class Out Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaffold-splits">3.8.2. Scaffold splits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">3.9. Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.10. Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.10.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.10.2. Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-assessment">3.10.3. Model Assessment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">3.11. Cited References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andrew D. White
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">âœ•</button> <img id="wh-modal-img"> </div>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>