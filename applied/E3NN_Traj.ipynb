{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDHJMVqgha5a"
   },
   "source": [
    "# Equivariant Neural Network for Predicting Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Foy9ZA_f6MZu"
   },
   "source": [
    "```{admonition} Authors:\n",
    "[Sam Cox](https://github.com/SamCox822)\n",
    "```\n",
    "\n",
    "In this example, we will train an equivariant neural network to predict the next frame in the trajectory alignment example in 10.6. As stated in 10.3.8, for time-dependent trajectories, we do not need to concern ourselves with permutation equivariance because it is implied that the order of the points does not change. Thus, we can treat this example as point cloud, meaning that any deep learning model that we train on this data should have rotation and translation equivariance. In other words, our model should be E(3) equivariant. [E3NN](https://e3nn.org) is a library built to create equivariant neural networks for the this group, so it's a great choice for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CzF_Lh_mfBb"
   },
   "source": [
    "## Retrieving Data from Trajectory Alignment Example\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maitjJ0J8FkZ"
   },
   "source": [
    "First, let's borrow a few cells from Chapter 10 to download our data and view the first frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix1niSUbhrKl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "OYD-Y4Ihhuw2",
    "outputId": "415326a7-edb4-4dbe-91cb-b53964a5b8fc"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/paths.npz\", \"paths.npz\"\n",
    ")\n",
    "paths = np.load(\"paths.npz\")[\"arr\"]\n",
    "# plot the first point\n",
    "plt.title(\"First Frame\")\n",
    "plt.plot(paths[0, :, 0], paths[0, :, 1], \"o-\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYGRiXdsqukN"
   },
   "source": [
    "## Additional Installations and Imports\n",
    "## Running This Notebook\n",
    "\n",
    "\n",
    "Click the &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; above to launch this page as an interactive Google Colab. See details below on installing packages.\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "To install packages, execute this code in a new cell. \n",
    "\n",
    "```\n",
    "!pip install dmol-book\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "HCMPyneStz1J",
    "outputId": "072d926e-485b-4d3c-e825-f9aeab0f75c9"
   },
   "outputs": [],
   "source": [
    "# additional imports\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import torch\n",
    "import e3nn\n",
    "import math\n",
    "from e3nn.nn.models.gate_points_2101 import Network\n",
    "from e3nn import o3\n",
    "import dmol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM7b2GuBtREx"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWClrr7YtUKB"
   },
   "source": [
    "Before we build our E3NN network, it's always a good idea to build a baseline model for comparision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS5-BvF3jvQb"
   },
   "source": [
    "First, let's discuss what the input and output should be for this model. The input should be the coordinates of the 12 points: one frame. What should the output be? We want to train a neural network to predict the next trajectory for each point, the next frame, so our output should actually be the same type and size as our input.\n",
    "\n",
    "Thus,\n",
    "\n",
    "**Inputs:**\n",
    "* 12 sets of coordinates\n",
    "\n",
    "**Outputs:**\n",
    "* 12 sets of coordinates\n",
    "\n",
    "Note: since we are trying to build an E(3)-equivariant neural network, which should be equivariant to transformations in 3D space, we need to make these coordinates 3D. This is easy, we will just put zero for the z-coordiantes. We'll do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqUhDfvZjrSY"
   },
   "outputs": [],
   "source": [
    "traj_3d = np.array([])\n",
    "for i in range(2048):\n",
    "    for j in range(12):\n",
    "        TBA = paths[i][j]\n",
    "        TBA = np.append(\n",
    "            TBA,\n",
    "            np.array(\n",
    "                [\n",
    "                    0.00,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        traj_3d = np.append(traj_3d, TBA)\n",
    "\n",
    "traj_3d = traj_3d.reshape(2048, 12, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsEANrCelJcm"
   },
   "source": [
    "Interestingly, for this example, we want our prediction from one frame to match the following frame. So our features and labels will be nearly identical, offset by one. For the features, we want to include everything except for the final frame, which has no \"next frame\" in our data. We can extrapolate with our model to predict this \"next frame\" as a final step if we want.  For our lables, we want to include everything except for the first step, which is not the \"next frame\" of anything in our data.  We can also go ahead and split our data into training and testing sets. Let's do approximately an 80:20 split here. We want to make sure not to shuffle our data, as we are predicting order-sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYkPIqO5lMUS"
   },
   "outputs": [],
   "source": [
    "features = traj_3d[:-1]\n",
    "labels = traj_3d[1:]\n",
    "\n",
    "# split data 80:20\n",
    "training_set = features[:1637]\n",
    "training_labels = labels[:1637]\n",
    "valid_set = features[1637:]\n",
    "valid_labels = labels[1637:]\n",
    "\n",
    "# convert to jnp arrays\n",
    "training_setbl = jnp.asarray(training_set)\n",
    "training_labelsbl = jnp.asarray(training_labels)\n",
    "valid_setbl = jnp.asarray(valid_set)\n",
    "valid_labelsbl = jnp.asarray(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-L0ZAiylQke"
   },
   "source": [
    "Let's check to make sure our data matches up. Frame 2 in the features set should be the same as Frame 1 in the labels set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yBv8SVXnO-g"
   },
   "outputs": [],
   "source": [
    "def mse(y, yhat):\n",
    "    return np.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viwGHptQlQEP",
    "outputId": "a4a287a4-558a-4328-f9f2-a776216a3c0c"
   },
   "outputs": [],
   "source": [
    "print(\"features, frame 2: \\n\", features[1])\n",
    "print(\"labels, frame 1: \\n\", labels[0])\n",
    "if mse(features[1], labels[0]) == 0:\n",
    "    print(\"success! they match!\")\n",
    "else:\n",
    "    print(mse(features[1], labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1zkI8WylWKp"
   },
   "source": [
    "Great, they match! Now we are ready to build our baseline model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knROsKpMPq9G"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def baseline_model(inputs, w, b):\n",
    "    yhat = inputs @ w + b\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def baseline_loss(inputs, y, w, b):\n",
    "    return mse(y, baseline_model(inputs, w, b))\n",
    "\n",
    "\n",
    "bl_loss_grad = jax.grad(baseline_loss, (2, 3))\n",
    "\n",
    "w = np.zeros((3, 3))\n",
    "w = jnp.asarray(w)\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "gwVTp4xqP6ox",
    "outputId": "668b4a2e-c846-4f7e-b0cb-3005a777d15e"
   },
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "eta = 1e-6\n",
    "\n",
    "baseline_val_loss = [0.0 for _ in range(epochs)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for d in range(1637):\n",
    "        inputs = training_setbl[d]\n",
    "        y = training_labelsbl[d]\n",
    "        grad_bl = bl_loss_grad(inputs, y, w, b)\n",
    "        # update w & b\n",
    "        w -= eta * grad_bl[0]\n",
    "        b -= eta * grad_bl[1]\n",
    "\n",
    "    for i in range(410):\n",
    "        inputs_v = valid_setbl[i]\n",
    "        y_v = valid_labelsbl[i]\n",
    "        baseline_val_loss[epoch] += baseline_loss(inputs_v, y_v, w, b)\n",
    "    baseline_val_loss[epoch] = jnp.sqrt(baseline_val_loss[epoch] / 410)\n",
    "\n",
    "\n",
    "plt.plot(baseline_val_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrJIJzQJuhcz",
    "outputId": "92c5b2c1-2841-4f31-9eaa-7d712f088b36"
   },
   "outputs": [],
   "source": [
    "print(\"Final loss value: \", baseline_val_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcsLpaa-9_og"
   },
   "source": [
    "Now let's view a parity plot to see if we're learning the right trend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "dTkdDKk_95oM",
    "outputId": "413ba181-30a2-46d6-bd93-517a6f9f6c76"
   },
   "outputs": [],
   "source": [
    "ys = jnp.array([])\n",
    "yhats = jnp.array([])\n",
    "\n",
    "for i in range(205):\n",
    "    inputs_v = valid_setbl[i]\n",
    "    ys = jnp.append(ys, valid_labelsbl[i])\n",
    "    yhat = baseline_model(inputs_v, w, b)\n",
    "    yhats = jnp.append(yhats, yhat)\n",
    "\n",
    "plt.plot(ys, ys, \"-\")\n",
    "plt.plot(ys, yhats, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_XufdCT-SrP"
   },
   "source": [
    "This is difficult to read, since our xyz coordinates are very different in magnitude. Instead, let's look at three plots, one for each coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "tR4cM9EA-dSY",
    "outputId": "e9f1669e-62f3-474d-912c-40c66140aa60"
   },
   "outputs": [],
   "source": [
    "ys_x = jnp.array([])\n",
    "ys_y = jnp.array([])\n",
    "ys_z = jnp.array([])\n",
    "yh_x = jnp.array([])\n",
    "yh_y = jnp.array([])\n",
    "yh_z = jnp.array([])\n",
    "\n",
    "for i in range(205):\n",
    "    inputs_v = valid_setbl[i]\n",
    "    y = valid_labelsbl[i]\n",
    "    yhat = baseline_model(inputs_v, w, b)\n",
    "\n",
    "    ys_x = jnp.append(ys_x, y[i][0])\n",
    "    ys_y = jnp.append(ys_y, y[i][1])\n",
    "    ys_z = jnp.append(ys_z, y[i][2])\n",
    "\n",
    "    yh_x = jnp.append(yh_x, yhat[i][0])\n",
    "    yh_y = jnp.append(yh_y, yhat[i][1])\n",
    "    yh_z = jnp.append(yh_z, yhat[i][2])\n",
    "\n",
    "plt.plot(ys_x, ys_x, \"-\")\n",
    "plt.plot(ys_x, yh_x, \".\")\n",
    "plt.xlabel(\"X-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"X-Coordinate of Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ys_y, ys_y, \"-\")\n",
    "plt.plot(ys_y, yh_y, \".\")\n",
    "plt.xlabel(\"Y-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"Y-Coordinate of Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ys_z, ys_z, \"-\")\n",
    "plt.plot(ys_z, yh_z, \".\")\n",
    "plt.xlabel(\"Z-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"Z-Coordinate of Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NouYVoRS-y_v"
   },
   "source": [
    "It looks like we are starting to get the right trend for some of the coordinates, but more training is definitely needed, especially for the z-coordinate, which should always be zero. However, as stated, we want any model that uses this data to be equivariant in 3D space. Let's check the equivariances now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRezFDFVCqEZ",
    "outputId": "db0be1dd-5ef4-433f-99f2-ace0b7e823b3"
   },
   "outputs": [],
   "source": [
    "# checking for rotation equivariance\n",
    "import scipy.spatial.transform as trans\n",
    "\n",
    "# rotate around x coordinate by 80 degrees\n",
    "rot = trans.Rotation.from_euler(\"x\", 80, degrees=True)\n",
    "\n",
    "input_point = jnp.asarray(np.random.normal(size=(12, 3)))\n",
    "w_test1 = jnp.asarray(np.random.normal(size=(3, 3)))\n",
    "\n",
    "input_rot = rot.apply(input_point)\n",
    "output_1 = baseline_model(input_rot, w_test1, b)\n",
    "output_prerot = baseline_model(input_point, w_test1, b)\n",
    "output_rot = []\n",
    "for xyz in output_prerot:\n",
    "    coord = rot.apply(xyz)\n",
    "    output_rot.append(coord)\n",
    "output_rot = jnp.array(output_rot)\n",
    "\n",
    "print(\"rotated first: \\n\", output_1)\n",
    "print(\"rotated last: \\n\", output_rot)\n",
    "print(\"\\033[1m\" + \"difference: \" + \"\\033[0m\", mse(output_1, output_rot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wL-4mgCFQMU"
   },
   "source": [
    "So it doesn't look like our baseline model is rotation-equivariant. This is important, because we if we give our model coordinates that are rotated, we expect the output should be rotated by the same degree. Likewise, we need translation equivariance. Let's check that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_Fg_KHyFm2q",
    "outputId": "e93d94ea-dba0-4fad-a791-79a8448780ae"
   },
   "outputs": [],
   "source": [
    "# checking for translation equivariance\n",
    "random_trans = jnp.asarray(np.random.normal(size=(12, 3)))\n",
    "\n",
    "input_trans = input_point + random_trans\n",
    "output_2 = baseline_model(input_trans, w_test1, b)\n",
    "output_trans = random_trans + baseline_model(input_point, w_test1, b)\n",
    "\n",
    "print(\"translated first: \", output_2)\n",
    "print(\"translated last: \", output_trans)\n",
    "print(\"\\033[1m\" + \"difference: \" + \"\\033[0m\", mse(output_2, output_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYCwyP87GtrB"
   },
   "source": [
    "As expected, our model isn't translation equviariant either. We can solve this problem a few ways. One way is to augment our data in order to teach our model equivariance. This requires more training and data storage, so let's look at a more compact approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7WKHiYthx86"
   },
   "source": [
    "## E3NN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIgZvoEjs3Q"
   },
   "source": [
    "E3NN is a library for creating equivariant neural networks, specifically in E(3). E3NN is built for spatial equvariance in 3-D space, giving us equivariance with respect to the E(3) group of rotations, inversions, and translations. As discussed before, the time-dependent trajectory points do not change order, so we do not need to worry about permutation equivariance/invariance in this case; we only need E(3)-equivariance. E3NN is a great tool for this problem because we have 3-dimensional points in space, and if we transform them in space, we want the output to transform the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfb8eErlXg7"
   },
   "source": [
    "E3NN works through the use of irreducible representations (irreps). In general, representations tell you how to interact with the data with repect to the group, and irreducible representations are the smallest and complete representations {cite}`geiger2022e3nn`. When creating a model, we give the model the irreps so that it knows how to handle the data we will give it during trianing. It's not necessary to understand what the irreps are; instead, just know that they are the smallest representations, which are similar to, and transform the same way as, the spherical harmonics. Any (reducible) representation can be decomposed into irreducible representations. If you want to know more, you can check out more on the E3NN documentation website [@e3nn]. Let's take a look at how the irreps are used in this context. \n",
    "\n",
    "For this group (O(3), which includes parity), we need to find the L and d for each piece of data, where $d = 2L + 1$ (d = dimension). Look at the table below. \n",
    "\n",
    "| **parity** | **L** | **d** | **name**      |\n",
    "|------------|-------|-------|---------------|\n",
    "| even       | 0     | 1     | scalar        |\n",
    "| odd        | 0     | 1     | pseudo scalar |\n",
    "| even       | 1     | 3     | pseudo vector |\n",
    "| odd        | 1     | 3     | vector        |\n",
    "| even       | 2     | 5     |       -       |\n",
    "| odd        | 2     | 5     |       -       |\n",
    "\n",
    "The general notation is **MxLp**, where M is the number, L is the L from the table above, and p corresponds to the parity (e: even, o: odd). \n",
    "\n",
    "For example, if you wanted to portray \"12 scalars, 4 vectors\" in this format, you would write **\"12x0e + 4x1o\"**. Take a minute to make sure you understand how to use this notation, as it's essential for E3NN. E3NN deals with equivariance by receiving the irreps as a model parameter. This allows the E3NN framework to know how each input feature/output transforms under symmetry, so that it can treat each piece appropriately. As a side note, the output of an E3NN model must always be of equal or higher symmetry than your input.\n",
    "\n",
    "Because E3NN is built to handle 3D spatial data, we do not need to tell the model that we are going to give it 3D coordinates; it's implicit and **required**. The irreps_in, instead, correspond to the input node features. In this example, we don't have input features, but as an example, you can imagine we could want our model to predict the next set of coordinates, given the intitial coordinates and the corresponding atom types. In that case, our irreps_in would be the atom types (one scalar per input if we have one-hot vectors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msab78GJlbrF"
   },
   "source": [
    "Since we don't have input features, we'll put \"None\" for that parameter, and we want our output to be the same shape as the input: 12 vectors. However, since we are trying to predict 12 vectors out for 12 vectors in, we only need to tell the model to predict 1 vector per input **\"1x1o\"**. Take a minute to make sure you understand why this is the case. You can think of the model recognizing 12 input vectors and predicting a vector for each. Again, E3NN expects coordinate inputs, so we don't specify this for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRq67aBWrXh0"
   },
   "source": [
    "## E3NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx55lICsrgTH"
   },
   "source": [
    "E3NN has several models within their library, which can be found [on the E3NN github page here](https://github.com/e3nn/e3nn/tree/main/e3nn/nn/models). For this example, we will use one of these models. \n",
    "\n",
    "To use the E3NN model, we need to turn our data into a torch_geometric dataset. We'll do that now. Then we can split our data into training and testing sets.\n",
    "\n",
    "Also, instead of directly computing the next frame, we'll change it here to predict the distance to the next frame. This is a small change, but having data centered nearer zero can be better for training. We'll need to undo this when we look at the frames later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a2CF_bbv4_N"
   },
   "outputs": [],
   "source": [
    "feat = torch.from_numpy(features)  # convert to pytorch tensors\n",
    "ys = torch.from_numpy(labels)  # convert to pytorch tensors\n",
    "traj_data = []\n",
    "distances = ys - feat  # compute distances to next frame\n",
    "\n",
    "\n",
    "# make torch_geometric dataset\n",
    "# we want this to be an iterable list\n",
    "# x = None because we have no input features\n",
    "for frame, label in zip(feat, distances):\n",
    "    traj_data += [\n",
    "        torch_geometric.data.Data(\n",
    "            x=None, pos=frame.to(torch.float32), y=label.to(torch.float32)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "train_split = 1637\n",
    "train_loader = torch_geometric.data.DataLoader(\n",
    "    traj_data[:train_split], batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "test_split = 1842\n",
    "test_loader = torch_geometric.data.DataLoader(\n",
    "    traj_data[train_split:test_split], batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjVxk_cwVBX-"
   },
   "source": [
    "Great! Now we're ready to define our model. Since this is a pre-built model in E3NN, so we just need to import it and define the model parameters. Note that the state of this model will save automatically, so you will need to reinitialize the model every time you want to start training. To see how these models work you can look at [this preprint](https://arxiv.org/abs/2207.09453) or [this video series](https://www.youtube.com/watch?v=q9EwZsHY1sk&list=PLx3xbphkO3qIlBoESkbafXaDtr0tq5iRd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcptjZtGB4Oc"
   },
   "outputs": [],
   "source": [
    "from e3nn.nn.models.gate_points_2101 import Network\n",
    "from e3nn import o3\n",
    "\n",
    "model_kwargs = {\n",
    "    \"irreps_in\": None,  # no input features\n",
    "    \"irreps_hidden\": o3.Irreps(\"5x0e + 5x0o + 5x1e + 5x1o\"),  # hyperparameter\n",
    "    \"irreps_out\": \"1x1o\",  # 12 vectors out, but only 1 vector out per input\n",
    "    \"irreps_node_attr\": None,\n",
    "    \"irreps_edge_attr\": o3.Irreps.spherical_harmonics(3),\n",
    "    \"layers\": 3,  # hyperparameter\n",
    "    \"max_radius\": 3.5,\n",
    "    \"number_of_basis\": 10,\n",
    "    \"radial_layers\": 1,\n",
    "    \"radial_neurons\": 128,\n",
    "    \"num_neighbors\": 11,  # average number of neighbors w/in max_radius\n",
    "    \"num_nodes\": 12,  # not important unless reduce_output is True\n",
    "    \"reduce_output\": False,  # setting this to true would give us one scalar as an output.\n",
    "}\n",
    "\n",
    "model = Network(**model_kwargs)  # initializing model with parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7UOAxB7xcFE"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "eta = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5YQ-GH6JnpB"
   },
   "outputs": [],
   "source": [
    "# this will print an outline of the model architecture!\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAJAqOY-s3TZ"
   },
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "\n",
    "val_loss = [0.0 for _ in range(epochs)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_loader):\n",
    "        yhat = model(data)\n",
    "        loss_1 = torch.mean((yhat - data.y) ** 2)\n",
    "        loss_1.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(test_loader):\n",
    "            yhat = model(data)\n",
    "            loss2 = torch.mean((yhat - data.y) ** 2)\n",
    "            val_loss[epoch] += (loss2).detach()\n",
    "    val_loss[epoch] = val_loss[epoch] / 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "262rPIy2QNi-",
    "outputId": "c61487f3-e954-4507-bfd2-459ab745b8b5"
   },
   "outputs": [],
   "source": [
    "v_loss = torch.tensor(val_loss)\n",
    "\n",
    "plt.plot(v_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjRx285tsQ6N",
    "outputId": "11f9309d-af9a-4a9c-f8e9-bcf8080e7fc0"
   },
   "outputs": [],
   "source": [
    "print(\"final loss value: \", val_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqu6o27P0KFA"
   },
   "source": [
    "Let's run each model on the last frame to get an extrapolated frame. Remember that for the E3NN model, we predicting displacements, so we'll just need to add our final displacement back to our final coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXoZa2iIXO78",
    "outputId": "d82c383b-1ce0-4b09-9525-d63307cadc9b"
   },
   "outputs": [],
   "source": [
    "last_frame_bl = y_v\n",
    "extrp_bl = baseline_model(last_frame_bl, w, b)\n",
    "\n",
    "last_frame_e3nn = yhat + data.pos\n",
    "lf_e3nn = []\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "lf_e3nn += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None,\n",
    "        pos=last_frame_e3nn.to(torch.float32),\n",
    "        y=last_frame_e3nn.to(torch.float32),\n",
    "    )\n",
    "]\n",
    "lf_loader = torch_geometric.data.DataLoader(lf_e3nn, batch_size=1, shuffle=False)\n",
    "\n",
    "# run through model\n",
    "for i in lf_loader:\n",
    "    extrp_e3nn = model(i)\n",
    "\n",
    "# add extrapolated displacements back into last frame\n",
    "extrp_e3nn += last_frame_e3nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSTEG_u8aePB",
    "outputId": "6ffb66a0-4a1e-49e5-9ce6-e7b50401fb80"
   },
   "outputs": [],
   "source": [
    "extrp_bl = np.array(extrp_bl)\n",
    "extrp_e3nn = extrp_e3nn.detach().numpy()\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(extrp_bl)\n",
    "print(extrp_e3nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isEXdrcezs15"
   },
   "source": [
    "These look pretty good! note that the baseline model has still not learned that the z-coordinate is always zero, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "185qmnmbcFNX"
   },
   "source": [
    "Now we can check for equivariance in the same way that we did before with the baseline model. Let's just take the final frame and translate by 10 in the x-direction, then compare to just the output translated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmgYDX78cEE9",
    "outputId": "f97ab1ec-c1b1-4411-cea2-6116a740431a"
   },
   "outputs": [],
   "source": [
    "# checking for rotation equivariance\n",
    "import scipy.spatial.transform as trans\n",
    "\n",
    "# rotate around x coordinate by 80 degrees\n",
    "rot = trans.Rotation.from_euler(\"x\", 80, degrees=True)\n",
    "\n",
    "input_point = np.asarray(np.random.normal(size=(12, 3)))\n",
    "input_rot = rot.apply(input_point)\n",
    "input_point = torch.from_numpy(input_point)\n",
    "input_rot = torch.from_numpy(input_rot)\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "rot_first = []\n",
    "rot_first += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None, pos=input_rot.to(torch.float32), y=input_rot.to(torch.float32)\n",
    "    )\n",
    "]\n",
    "rf_loader = torch_geometric.data.DataLoader(rot_first, batch_size=1, shuffle=False)\n",
    "# run through model\n",
    "for i in rf_loader:\n",
    "    output_1 = model(i)\n",
    "\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "rot_last = []\n",
    "rot_last += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None, pos=input_point.to(torch.float32), y=input_point.to(torch.float32)\n",
    "    )\n",
    "]\n",
    "rl_loader = torch_geometric.data.DataLoader(rot_last, batch_size=1, shuffle=False)\n",
    "# run through model\n",
    "for i in rl_loader:\n",
    "    output_2 = model(i)\n",
    "\n",
    "output_2 = output_2.detach().numpy()\n",
    "output_1 = output_1.detach().numpy()\n",
    "\n",
    "output_rot = []\n",
    "for xyz in output_2:\n",
    "    coord = rot.apply(xyz)\n",
    "    output_rot.append(coord)\n",
    "output_rot = np.array(output_rot)\n",
    "\n",
    "print(\"rotated first: \\n\", output_1)\n",
    "print(\"rotated last: \\n\", output_rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsgSyfwyZ1xc"
   },
   "source": [
    "Perfect! Our random array, when rotated first, gives the same results as when we rotated last! Now we know we have rotational equvivariances. I won't go further to test translational equivariances; I will leave that as an exercise. The E3NN model outperforms the baseline significantly, and it is E(3)-equivariant, unlike our baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Equivariant Neural Network for Predicting Trajectories.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
