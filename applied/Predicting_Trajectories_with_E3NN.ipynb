{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDHJMVqgha5a"
   },
   "source": [
    "# Equivariant Neural Network for Predicting Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Foy9ZA_f6MZu"
   },
   "source": [
    "In this example, we will train an equivariant neural network to predict the next frame in the trajectory alignment example (10.6). As stated in 10.3.8, for time-dependent trajectories, we do not need to concern ourselves with permutation equivariance because it is implied that the order of the points does not change. Thus, we can treat this example as a simple set of coordinates in 3D space, meaning that any deep learning model that we train on this data should have rotation, mirror/parity, and translation equivariance. In other words, our model should be O(3) equivariant [@wikipedia_2021]. E3NN [@e3nn] is a library built to create equivariant neural networks for the this group, so it's a great choice for this problem.\n",
    "\n",
    "We will use the trajectory data from that example to train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CzF_Lh_mfBb"
   },
   "source": [
    "## Retrieving Data from Trajectory Alignment Example\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maitjJ0J8FkZ"
   },
   "source": [
    "First, let's use the same imports and visualization used in Chapter 10 to download our data and view the first frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix1niSUbhrKl"
   },
   "outputs": [],
   "source": [
    "# new imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\n",
    "    \"dark\",\n",
    "    {\n",
    "        \"xtick.bottom\": True,\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.color\": \"#666666\",\n",
    "        \"ytick.color\": \"#666666\",\n",
    "        \"axes.edgecolor\": \"#666666\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"figure.dpi\": 300,\n",
    "    },\n",
    ")\n",
    "color_cycle = [\"#1BBC9B\", \"#F06060\", \"#5C4B51\", \"#F3B562\", \"#6e5687\"]\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=color_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "OYD-Y4Ihhuw2",
    "outputId": "5f87fd0c-7e22-4fb7-d031-a03fc24861da"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/paths.npz\", \"paths.npz\"\n",
    ")\n",
    "paths = np.load(\"paths.npz\")[\"arr\"]\n",
    "# plot the first point\n",
    "plt.title(\"First Frame\")\n",
    "plt.plot(paths[0, :, 0], paths[0, :, 1], \"o-\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYGRiXdsqukN"
   },
   "source": [
    "## Additional Installations\n",
    "\n",
    "The following cell sets up some additional installations we need. These may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_JUIBvhk6-m"
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbtnVVJ7ih3c"
   },
   "outputs": [],
   "source": [
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+${111}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wqb68H3GtpNv"
   },
   "outputs": [],
   "source": [
    "pip install torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dlx4WX-oi3nP",
    "outputId": "7dd386c1-4b53-4c6f-cf93-7291f37e04af"
   },
   "outputs": [],
   "source": [
    "pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_23TMFVei60v"
   },
   "outputs": [],
   "source": [
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGV_C9JMi_-K"
   },
   "outputs": [],
   "source": [
    "pip install e3nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM7b2GuBtREx"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-u8_bUZI3FB"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWClrr7YtUKB"
   },
   "source": [
    "Before we build our E3NN network, it's always a good idea to build a baseline model for comparision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS5-BvF3jvQb"
   },
   "source": [
    "First, let's discuss what the input and output should be for this model. The input should be the coordinates of the 12 points: one frame. What should the output be? We want to train a neural network to predict the next trajectory for each point, the next frame, so our output should actually be the same type as our input.\n",
    "\n",
    "Thus,\n",
    "\n",
    "**Inputs:**\n",
    "* 12 sets of coordinates\n",
    "\n",
    "**Outputs:**\n",
    "* 12 sets of coordinates\n",
    "\n",
    "Note: since we are trying to build an O(3) equivariant neural network, which should be equivariant to transformations in 3D space, we need to make these coordinates 3D. This is easy, we will just put zero for the z-coordiantes. We'll do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqUhDfvZjrSY"
   },
   "outputs": [],
   "source": [
    "traj_3d = np.array([])\n",
    "for i in range(2048):\n",
    "    for j in range(12):\n",
    "        TBA = paths[i][j]\n",
    "        TBA = np.append(\n",
    "            TBA,\n",
    "            np.array(\n",
    "                [\n",
    "                    0.00,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        traj_3d = np.append(traj_3d, TBA)\n",
    "\n",
    "traj_3d = traj_3d.reshape(2048, 12, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsEANrCelJcm"
   },
   "source": [
    "Interestingly, for this example, we want our prediction from one frame to match the following frame. So our features and labels will be nearly identical, offset by one.\n",
    "\n",
    "For the features, we want to include everything except for the final frame, which has no \"next frame\" in our data. We can extrapolate with our model to predict this \"next frame\" as a final step if we want. \n",
    "\n",
    "For our lables, we want to include everything except for the first step, which is not the \"next frame\" of anything in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYkPIqO5lMUS",
    "outputId": "7103b798-590f-473d-8d95-1b5de65692cd"
   },
   "outputs": [],
   "source": [
    "features = traj_3d[:-1]\n",
    "print(features.shape)\n",
    "\n",
    "labels = traj_3d[1:]\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38qE3XzPuFGA"
   },
   "source": [
    "Now we can split our data into training, validation, and testing sets. Let's do approximately an 80:10:10 split here. \n",
    "\n",
    "We want to make sure not to shuffle our data, as we are predicting time-series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evb66rhetSJW"
   },
   "outputs": [],
   "source": [
    "training_set = features[:1637]\n",
    "training_labels = labels[:1637]\n",
    "\n",
    "valid_set = features[1637:1842]\n",
    "valid_labels = labels[1637:1842]\n",
    "\n",
    "test_set = features[1842:]\n",
    "test_labels = labels[1842:]\n",
    "\n",
    "# convert to jnp arrays\n",
    "training_setbl = jnp.asarray(training_set)\n",
    "training_labelsbl = jnp.asarray(training_labels)\n",
    "\n",
    "valid_setbl = jnp.asarray(valid_set)\n",
    "valid_labelsbl = jnp.asarray(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-L0ZAiylQke"
   },
   "source": [
    "Let's check to make sure our data matches up. Frame 2 in the features set should be the same as Frame 1 in the labels set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viwGHptQlQEP",
    "outputId": "4154e171-d1cf-4ea9-eac7-a8ca0bc22fe6"
   },
   "outputs": [],
   "source": [
    "print(\"features, frame 2: \\n\", features[1])\n",
    "print(\"labels, frame 1: \\n\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1zkI8WylWKp"
   },
   "source": [
    "Great, they match! Now we are ready to build our baseline model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCaKwQaojzOz"
   },
   "source": [
    "Perhaps we should first discuss our loss function. We are predicting 3 values (xyz) per point number, so we can just find the Euclidean distance between the two frames. We can The formula is the standard distance formula in 3D:  $$L = \\sqrt{(x_{2}-x_{1})^2 + (y_{2}-y_{1})^2 + (z_{2}-z_{1})^2}$$\n",
    "\n",
    "Remember that we have 12 points per input/output, so we'll compute this loss function iteratively over the points. We will get 12 values for the loss. We can just take the sum to get a value for the loss. This is the total distance between the two frames. When we train our baseline model, our loss function will call the below function, which iteratively computes the distance between each point's initial and final coordinates and sums the distances to produce the total distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yBv8SVXnO-g"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def dist(inputs, yhats):\n",
    "    distance = 0\n",
    "    for i in range(12):\n",
    "        distance += jnp.sqrt(\n",
    "            (inputs[i][0] - yhats[i][0]) ** 2\n",
    "            + (inputs[i][1] - yhats[i][1]) ** 2\n",
    "            + (inputs[i][2] - yhats[i][2]) ** 2\n",
    "        )\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH3Q8BGh9T91"
   },
   "source": [
    "\n",
    "Note that the shape of our weight matrix is (12,3, 3). This is because we want our output shape to be (12,3). We can't use a shape (3,3) matrix, however, because we want each point to have its own set of parameters.\n",
    "\n",
    "We can just run the model to calculate the $\\hat{y}$ for each point, using the appropriate set of weights. This, too, will be an iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knROsKpMPq9G"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def baseline_model(inputs, w, b):\n",
    "    yhat = inputs @ w + b\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def run_blm(inputs, w, b):\n",
    "    output = jnp.array([])\n",
    "    for i in range(12):\n",
    "        xyz = inputs[i]\n",
    "        yhat = baseline_model(xyz, w[i], b)\n",
    "        output = jnp.append(output, yhat)\n",
    "    output1 = jnp.reshape(output, (12, 3))\n",
    "    return output1\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def dist(inputs, yhats):\n",
    "    distance = 0\n",
    "    for i in range(12):\n",
    "        distance += jnp.sqrt(\n",
    "            (inputs[i][0] - yhats[i][0]) ** 2\n",
    "            + (inputs[i][1] - yhats[i][1]) ** 2\n",
    "            + (inputs[i][2] - yhats[i][2]) ** 2\n",
    "        )\n",
    "    return distance\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def baseline_loss(inputs, yhats, w, b):\n",
    "    return dist(run_blm(inputs, w, b), yhats)\n",
    "\n",
    "\n",
    "w = np.zeros((12, 3, 3))\n",
    "w = jnp.asarray(w)\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwVTp4xqP6ox"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "eta = 1e-6\n",
    "\n",
    "baseline_val_loss = [0.0 for _ in range(epochs)]\n",
    "baseline_tr_loss = [0.0 for _ in range(epochs)]\n",
    "count = 0\n",
    "for epoch in range(epochs):\n",
    "    count += 1\n",
    "    for d in range(1637):\n",
    "        inputs = training_setbl[d]\n",
    "        yhats = training_labelsbl[d]\n",
    "        baseline_tr_loss[epoch] += baseline_loss(inputs, yhats, w, b)\n",
    "        grad_bl = jax.grad(baseline_loss, (2, 3))(inputs, yhats, w, b)\n",
    "        # update w & b\n",
    "        w -= eta * grad_bl[0]\n",
    "        b -= eta * grad_bl[1]\n",
    "    baseline_tr_loss[epoch] = baseline_tr_loss[epoch] / 1637\n",
    "\n",
    "    for i in range(205):\n",
    "        inputs_v = valid_setbl[i]\n",
    "        yhats_v = valid_labelsbl[i]\n",
    "        baseline_val_loss[epoch] += baseline_loss(inputs_v, yhats_v, w, b)\n",
    "    baseline_val_loss[epoch] = baseline_val_loss[epoch] / 205\n",
    "\n",
    "\n",
    "plt.plot(baseline_val_loss)\n",
    "plt.plot(baseline_tr_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcsLpaa-9_og"
   },
   "source": [
    "Now let's view a parity plot to see if we're learning the right trend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTkdDKk_95oM"
   },
   "outputs": [],
   "source": [
    "ys = jnp.array([])\n",
    "yhats = jnp.array([])\n",
    "\n",
    "for i in range(205):\n",
    "    yhatv = jnp.array([])\n",
    "    inputs_v = valid_setbl[i]\n",
    "    yhats_v = valid_labelsbl[i]\n",
    "    ys = jnp.append(ys, yhats_v)\n",
    "    yhat_raw = run_blm(inputs_v, w, b)\n",
    "    yhatv = jnp.append(yhatv, yhat_raw)\n",
    "    yhatv.reshape(12, 3)\n",
    "    yhats = jnp.append(yhats, yhatv)\n",
    "yhats.reshape(205, 12, 3)\n",
    "\n",
    "plt.plot(ys, ys, \"-\")\n",
    "plt.plot(ys, yhats, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_XufdCT-SrP"
   },
   "source": [
    "This is difficult to read, since our xyz coordinates are much different in magnitude. Instead, let's look at three plots, one for each coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tR4cM9EA-dSY"
   },
   "outputs": [],
   "source": [
    "ys_x = jnp.array([])\n",
    "ys_y = jnp.array([])\n",
    "ys_z = jnp.array([])\n",
    "yh_x = jnp.array([])\n",
    "yh_y = jnp.array([])\n",
    "yh_z = jnp.array([])\n",
    "\n",
    "for i in range(205):\n",
    "    inputs_v = valid_setbl[i]\n",
    "    y = valid_labelsbl[i]\n",
    "    yhat_raw = run_blm(inputs_v, w, b)\n",
    "\n",
    "    ys_x = jnp.append(ys_x, y[i][0])\n",
    "    ys_y = jnp.append(ys_y, y[i][1])\n",
    "    ys_z = jnp.append(ys_z, y[i][2])\n",
    "\n",
    "    yh_x = jnp.append(yh_x, yhat_raw[i][0])\n",
    "    yh_y = jnp.append(yh_y, yhat_raw[i][1])\n",
    "    yh_z = jnp.append(yh_z, yhat_raw[i][2])\n",
    "\n",
    "plt.plot(ys_x, ys_x, \"-\")\n",
    "plt.plot(ys_x, yh_x, \".\")\n",
    "plt.xlabel(\"X-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"X-Coordinate of Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ys_y, ys_y, \"-\")\n",
    "plt.plot(ys_y, yh_y, \".\")\n",
    "plt.xlabel(\"Y-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"Y-Coordinate of Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ys_z, ys_z, \"-\")\n",
    "plt.plot(ys_z, yh_z, \".\")\n",
    "plt.xlabel(\"Z-Coordinate of Trajectory\")\n",
    "plt.ylabel(\"Z-Coordinate of Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NouYVoRS-y_v"
   },
   "source": [
    "It looks like we're starting to get the right trend! This could definitely be improved with some tweeking and more training, but it is sufficient as a baseline. Let's move on to our real model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7WKHiYthx86"
   },
   "source": [
    "## E3NN Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H261fR4XhxaK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import e3nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIgZvoEjs3Q"
   },
   "source": [
    "E3NN is a library for creating equivariant neural networks, specifically in O(3). E3NN is built for spatial equvariance in 3-D space. Specifically, this library gives us equviariance with respect to the O(3) group of rotations, inversions, and translations. As discussed before, the time-dependent trajectory's points do not change order, so we do not need to worry about permutation equivariance/invariance in this case; We only need O(3) equivariance. E3NN is a great tool for this problem because we have 3-dimensional points in space, and if we transform them in space, we want the output to transform the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhKrDEojkKMp"
   },
   "source": [
    "One note about E3NN is that the library is built in PyTorch, so we need to convert our data from numpy arrays to pytorch tensors. We'll do that in the cell below. Note that the shape of the tensor does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glA6VV6Lvu8W"
   },
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(training_set)\n",
    "train_y = torch.from_numpy(training_labels)\n",
    "\n",
    "valid_x = torch.from_numpy(valid_set)\n",
    "valid_y = torch.from_numpy(valid_labels)\n",
    "\n",
    "\n",
    "test_x = torch.from_numpy(test_set)\n",
    "test_y = torch.from_numpy(test_labels)\n",
    "\n",
    "print(\n",
    "    \"original data type: \", type(training_set), \", original shape: \", training_set.shape\n",
    ")\n",
    "print(\"converted data type: \", type(train_x), \", converted shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfb8eErlXg7"
   },
   "source": [
    "E3NN works through the use of irreducible representations (irreps). In general, representations tell you how to interact with the data with repect to the group. When creating a model, we give the model the irreps so that it knows how to handle the data we will give it during trianing. It's not necessary to understand what the irreps are; instead, just know that they are the smallest representations, which are similar to, and transform the same way, as the spherical harmonics. Any (reducible) representation can be decomposed into irreducible representations. If you want to know more, you can check out more on the E3NN documentation website [@e3nn]. Let's take a look at how the irreps are used in this context. \n",
    "\n",
    "For this group, we need to find the L and d for each piece of data, where $d = 2L + 1$. Look at the table below. \n",
    "\n",
    "| **parity** | **L** | **d** | **name**      |\n",
    "|------------|-------|-------|---------------|\n",
    "| even       | 0     | 1     | scalar        |\n",
    "| odd        | 0     | 1     | pseudo scalar |\n",
    "| even       | 1     | 3     | pseudo vector |\n",
    "| odd        | 1     | 3     | vector        |\n",
    "| even       | 2     | 5     |       -       |\n",
    "| odd        | 2     | 5     |       -       |\n",
    "|            |       |       |               |\n",
    "\n",
    "The general notation is **MxLp**, where M is the number, L is the L from the table above, and p corresponds to the parity (e: even, o: odd). \n",
    "\n",
    "For example, if you wanted to portray \"12 scalars, 4 vectors\" in this format, you would write **\"12x0e + 4x1o\"**. Take a minute to make sure you understand how to use this notation, as it's essential for E3NN. E3NN deals with equivariance by recieving the irreps as a model parameter. This allows the E3NN framework to know how each input feature/output transform under symmetry, so that it can treat each piece appropriately. As a side note, the output of an E3NN model must always be of equal or higher symmetry than your input.\n",
    "\n",
    "Again, E3NN is built to handle 3D spatial data, so we do not need to tell the model that we are going to give it 3D coordinates; it's implicit. The irreps_in, instead, correspond to the input features. In this example, we don't have input features, but as an example, you can imagine we could want our model to predict the next set of coordinates, given the intitial coordinates and the velocity. In this case, our irreps_in would be the velocity. If we gave our velocity as vectors, we would have **\"12x1o\"\"** as our input features. If we just gave our model the magnitude of the velocity, we would represent our input features as **\"12x0e\"**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msab78GJlbrF"
   },
   "source": [
    "Since we don't have input features, we'll put None for that parameter, and we want our output to be the same shape as the input: **\"12x1o\"**. Take a minute to make sure you understand why this is the case. \n",
    "\n",
    "Again, E3NN expects coordinate inputs, so we don't specify this for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRq67aBWrXh0"
   },
   "source": [
    "## E3NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx55lICsrgTH"
   },
   "source": [
    "E3NN has several models within their library, which can be found on the E3NN github under e3nn/e3nn/nn/models/. For this example, we will use one of these models. To use this E3NN model, we need to turn our data into a torch_geometric dataset. We'll do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a2CF_bbv4_N"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "feat = torch.from_numpy(features)\n",
    "ys = torch.from_numpy(labels)\n",
    "\n",
    "traj_data = torch_geometric.data.Data(\n",
    "    pos=feat.to(torch.float32), x=ys.to(torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.nn.models.gate_points_2101 import Network\n",
    "from e3nn import o3\n",
    "\n",
    "model_kwargs = {\n",
    "    \"irreps_in\": None,  # no input features\n",
    "    \"irreps_hidden\": o3.Irreps(\"5x0e + 5x0o + 5x1e + 5x1o\"),  # hyperparameter\n",
    "    \"irreps_out\": \"12x1o\",  # 12 vectors out\n",
    "    \"irreps_node_attr\": None,\n",
    "    \"irreps_edge_attr\": o3.Irreps.spherical_harmonics(3),\n",
    "    \"layers\": 3,  # hyperparameter\n",
    "    \"max_radius\": 3.5,\n",
    "    \"number_of_basis\": 10,\n",
    "    \"radial_layers\": 1,\n",
    "    \"radial_neurons\": 128,\n",
    "    \"num_neighbors\": 12,  # average number of neighbors w/in max_radius\n",
    "    \"num_nodes\": 12,  # not important unless reduce_output is True\n",
    "    \"reduce_output\": False,\n",
    "}\n",
    "\n",
    "model = Network(**model_kwargs)  # initializing model with parameters above\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ3WxOHJ9ggQ"
   },
   "outputs": [],
   "source": [
    "output = model(traj_data.pos)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting_Trajectories_with_E3NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
