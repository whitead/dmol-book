{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Foy9ZA_f6MZu"
   },
   "source": [
    "# Equivariant Neural Network for Predicting Trajectories\n",
    "```{admonition} Authors:\n",
    "[Sam Cox](https://github.com/SamCox822)\n",
    "```\n",
    "\n",
    "```{admonition} Audience & Objectives\n",
    "This chapter builds on {doc}`../dl/Equivariant` and {doc}`../dl/data`. After completing this chapter, you should be able to \n",
    "\n",
    "  * Understand the importance of equivariance and how to check for equivariances in your model\n",
    "  * Understand how an equivariant model can be implemented in E3NN \n",
    "  * Be able to recognize and use the irrep notation used in E3NN\n",
    "```\n",
    "\n",
    "In this example, we will train an equivariant neural network to predict the next frame in the trajectory alignment example in {doc}`../dl/data`. As stated in {doc}`../dl/data`, for time-dependent trajectories, we do not need to concern ourselves with permutation equivariance because it is implied that the order of the points does not change. Thus, we can treat this example as point cloud, meaning that any deep learning model that we train on this data should have rotation and translation equivariance. In other words, our model should be E(3) equivariant. [E3NN](https://e3nn.org) is a library built to create equivariant neural networks for the this group, so it's a great choice for this problem {cite}`geiger2022e3nn`. We will look at E3NN in more detail later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QumlGwDmedj1"
   },
   "source": [
    "## Running This Notebook\n",
    "\n",
    "\n",
    "Click the &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; above to launch this page as an interactive Google Colab. See details below on installing packages.\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "To install packages, execute this code in a new cell. \n",
    "\n",
    "```\n",
    "!pip install dmol-book\n",
    "```\n",
    "\n",
    "If you find install problems, you can get the latest working versions of packages used in [this book here](https://github.com/whitead/dmol-book/blob/master/package/requirements.txt)\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D78bmI7pedj2"
   },
   "outputs": [],
   "source": [
    "# additional imports\n",
    "import torch\n",
    "import torch_geometric\n",
    "import e3nn\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import jax\n",
    "import dmol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CzF_Lh_mfBb"
   },
   "source": [
    "## Retrieving Data from Trajectory Alignment Example\n",
    "First, let's borrow a cell from Chapter 10 to download our data and view the first frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "OYD-Y4Ihhuw2",
    "outputId": "159d70f1-d42f-41b3-f573-1fbaeb68d25e"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/paths.npz\", \"paths.npz\"\n",
    ")\n",
    "paths = np.load(\"paths.npz\")[\"arr\"]\n",
    "# plot the first point\n",
    "plt.title(\"First Frame\")\n",
    "plt.plot(paths[0, :, 0], paths[0, :, 1], \"o-\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWClrr7YtUKB"
   },
   "source": [
    "## Baseline Model\n",
    "Before we build our E3NN network, it's always a good idea to build a baseline model for comparision. \n",
    "\n",
    "First, let's discuss what the input and output should be for this model. The input should be the coordinates of the 12 points: one frame. What should the output be? We want to train a neural network to predict the next trajectory for each point, the next frame, so our output should actually be the same type and size as our input.\n",
    "\n",
    "Thus,\n",
    "\n",
    "**Inputs:** 12 sets of coordinates\n",
    "\n",
    "**Outputs:** 12 sets of coordinates\n",
    "\n",
    "Note: since we are trying to build an E(3)-equivariant neural network, which should be equivariant to transformations in 3D space, we need to make these coordinates 3D. This is easy, we will just put zero for the z-coordiantes. We'll do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqUhDfvZjrSY"
   },
   "outputs": [],
   "source": [
    "traj_3d = np.array([])\n",
    "for i in range(2048):\n",
    "    for j in range(12):\n",
    "        TBA = paths[i][j]\n",
    "        TBA = np.append(TBA, np.array([0.00]))\n",
    "        traj_3d = np.append(traj_3d, TBA)\n",
    "\n",
    "traj_3d = traj_3d.reshape(2048, 12, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsEANrCelJcm"
   },
   "source": [
    "Interestingly, for this example, we want our prediction from one frame to match the following frame. So our features and labels will be nearly identical, offset by one. For the features, we want to include everything except for the final frame, which has no \"next frame\" in our data. We can extrapolate with our model to predict this \"next frame\" as a final step if we want.  For our labels, we want to include everything except for the first step, which is not the \"next frame\" of anything in our data.  We can also go ahead and split our data into training and testing sets. Let's do an 80:20 split here. We want to make sure not to shuffle our data, as we are predicting order-sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYkPIqO5lMUS"
   },
   "outputs": [],
   "source": [
    "features = traj_3d[:-1]\n",
    "labels = traj_3d[1:]\n",
    "\n",
    "# split data 80:20\n",
    "training_set = features[:1637]\n",
    "training_labels = labels[:1637]\n",
    "valid_set = features[1637:]\n",
    "valid_labels = labels[1637:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-L0ZAiylQke"
   },
   "source": [
    "Let's check to make sure our data matches up. Frame 2 in the features set should be the same as Frame 1 in the labels set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yBv8SVXnO-g"
   },
   "outputs": [],
   "source": [
    "def mse(y, yhat):\n",
    "    return np.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "viwGHptQlQEP",
    "outputId": "c341e818-3614-4c2d-ccfd-567fbe5c2546"
   },
   "outputs": [],
   "source": [
    "if mse(features[1], labels[0]) == 0:\n",
    "    print(\"success! they match!\")\n",
    "else:\n",
    "    print(mse(features[1], labels[0]))\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, squeeze=True, figsize=(16, 4))\n",
    "\n",
    "axs[0].set_title(\"Trajectory 1 end\")\n",
    "axs[1].set_title(\"Trajectory 2 beginning\")\n",
    "\n",
    "for i in range(0, 1, 16):\n",
    "    axs[0].plot(features[i, :, 0], features[i, :, 1], \".-\", alpha=0.2)\n",
    "    axs[1].plot(labels[i, :, 0], labels[i, :, 1], \".-\", alpha=0.2)\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1zkI8WylWKp"
   },
   "source": [
    "Great, they match! Now we are ready to build our baseline model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knROsKpMPq9G"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def baseline_model(inputs, w, b):\n",
    "    yhat = inputs @ w + b\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def baseline_loss(inputs, y, w, b):\n",
    "    return mse(y, baseline_model(inputs, w, b))\n",
    "\n",
    "\n",
    "bl_loss_grad = jax.grad(baseline_loss, (2, 3))\n",
    "\n",
    "w = np.zeros((3, 3))\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "gwVTp4xqP6ox",
    "outputId": "9ab01518-73b2-4fe5-e6be-52128a5eb69b"
   },
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "eta = 1e-6\n",
    "\n",
    "baseline_val_loss = [0.0 for _ in range(epochs)]\n",
    "ys = []\n",
    "yhats = []\n",
    "yst = []\n",
    "yhatst = []\n",
    "e = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    e += 1\n",
    "    for d in range(1637):\n",
    "        inputs = training_set[d]\n",
    "        y = training_labels[d]\n",
    "        if e == epochs:\n",
    "            yhatst.append(baseline_model(inputs, w, b))\n",
    "            yst.append(y)\n",
    "        grad_bl = bl_loss_grad(inputs, y, w, b)\n",
    "        # update w & b\n",
    "        w -= eta * grad_bl[0]\n",
    "        b -= eta * grad_bl[1]\n",
    "\n",
    "    for i in range(410):\n",
    "        inputs_v = valid_set[i]\n",
    "        y_v = valid_labels[i]\n",
    "        if e == epochs:\n",
    "            yhats.append(baseline_model(inputs_v, w, b))\n",
    "            ys.append(y_v)\n",
    "        baseline_val_loss[epoch] += baseline_loss(inputs_v, y_v, w, b)\n",
    "    baseline_val_loss[epoch] = np.sqrt(baseline_val_loss[epoch] / 410)\n",
    "\n",
    "\n",
    "plt.plot(baseline_val_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrJIJzQJuhcz",
    "outputId": "d777bc9f-e997-4c8b-88c8-e2ac71ba8a9e"
   },
   "outputs": [],
   "source": [
    "print(\"Final loss value: \", baseline_val_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcsLpaa-9_og"
   },
   "source": [
    "Now let's view a parity plot to see if we're learning the right trend here. Since the coordinates are so different in magnitude, we'll plot the x,y coordinates separately. Then we'll look at a parity plot for the center of mass for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp50lP8irZeC"
   },
   "outputs": [],
   "source": [
    "def centerofmass(arr):\n",
    "    com = []\n",
    "    for i in arr:\n",
    "        for j in i:\n",
    "            avg1 = (j[0] + j[1] + j[2]) / 3\n",
    "            com.append(avg1)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLcURt7p-fv8"
   },
   "outputs": [],
   "source": [
    "y_s = np.stack(ys, axis=0)\n",
    "yhat_s = np.stack(yhats, axis=0)\n",
    "\n",
    "x_ys = np.reshape(y_s[:, :, :1], (-1, 1))\n",
    "y_ys = np.reshape(y_s[:, :, 1:2], (-1, 1))\n",
    "z_ys = np.reshape(y_s[:, :, 2:], (-1, 1))\n",
    "\n",
    "x_yhs = np.reshape(yhat_s[:, :, :1], (-1, 1))\n",
    "y_yhs = np.reshape(yhat_s[:, :, 1:2], (-1, 1))\n",
    "z_yhs = np.reshape(yhat_s[:, :, 2:], (-1, 1))\n",
    "\n",
    "plt.title(\"x-coordinates\")\n",
    "plt.plot(x_ys, x_ys, \"-\")\n",
    "plt.plot(x_ys, x_yhs, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"y-coordinates\")\n",
    "plt.plot(y_ys, y_ys, \"-\")\n",
    "plt.plot(y_ys, y_yhs, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "2N6kXGgWuKUl",
    "outputId": "c7c2b9d7-5e3d-4b38-9928-f79c1bb5a9ec"
   },
   "outputs": [],
   "source": [
    "ys_centered = centerofmass(y_s)\n",
    "yhats_centered = centerofmass(yhat_s)\n",
    "\n",
    "plt.title(\"centered coordinates\")\n",
    "plt.plot(ys_centered, ys_centered, \"-\")\n",
    "plt.plot(ys_centered, yhats_centered, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NouYVoRS-y_v"
   },
   "source": [
    "It looks like we are starting to get the right trend for some of the coordinates, but more training is definitely needed. Let's look at the trajectory labels versus the predicted trajectories. To get a full picture, let's look add the training and validation data together here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "OU1ESpyvqn96",
    "outputId": "917ef921-29b3-4fa9-bed4-058f083db863"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, squeeze=True, figsize=(16, 4))\n",
    "\n",
    "y_st = np.stack(yst, axis=0)\n",
    "ys_total = np.concatenate([y_st, y_s])\n",
    "yhat_st = np.stack(yhatst, axis=0)\n",
    "yhats_total = np.concatenate([yhat_st, yhat_s])\n",
    "\n",
    "axs[0].set_title(\"Trajectory\")\n",
    "axs[1].set_title(\"Predicted Trajectory\")\n",
    "\n",
    "cmap = plt.get_cmap(\"cool\")\n",
    "for i in range(0, 2047, 40):\n",
    "    axs[0].plot(\n",
    "        ys_total[i, :, 0], ys_total[i, :, 1], \".-\", alpha=0.2, color=cmap(i / 2047)\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        yhats_total[i, :, 0],\n",
    "        yhats_total[i, :, 1],\n",
    "        \".-\",\n",
    "        alpha=0.2,\n",
    "        color=cmap(i / 2047),\n",
    "    )\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCXACqULYI7j"
   },
   "source": [
    "Yikes. Our model does not predict trajectories quite right. We expect this, since our baseline is simple machine learning. Importantly, as stated, we want any model that uses this data to be equivariant in 3D space. Let's check the equivariances now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRezFDFVCqEZ",
    "outputId": "bfc7e99d-046e-449d-c8a7-75ee1c4928b9"
   },
   "outputs": [],
   "source": [
    "# checking for rotation equivariance\n",
    "import scipy.spatial.transform as trans\n",
    "\n",
    "# rotate around x coordinate by 80 degrees\n",
    "rot = trans.Rotation.from_euler(\"x\", 80, degrees=True)\n",
    "key = jax.random.PRNGKey(52)\n",
    "\n",
    "input_point = jax.random.normal(key, (12, 3))\n",
    "w_test1 = jax.random.normal(key, (3, 3))\n",
    "\n",
    "input_rot = rot.apply(input_point)\n",
    "output_1 = baseline_model(input_rot, w_test1, b)\n",
    "output_prerot = baseline_model(input_point, w_test1, b)\n",
    "output_rot = []\n",
    "for xyz in output_prerot:\n",
    "    coord = rot.apply(xyz)\n",
    "    output_rot.append(coord)\n",
    "output_rot = np.asarray(output_rot)\n",
    "\n",
    "print(\"rotated first: \\n\", output_1, \"\\n\")\n",
    "print(\"rotated last: \\n\", output_rot, \"\\n\")\n",
    "print(\"\\033[1m\" + \"difference: \" + \"\\033[0m\", mse(output_1, output_rot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wL-4mgCFQMU"
   },
   "source": [
    "So it doesn't look like our baseline model is rotation-equivariant. This is important, because if we give our model coordinates that are rotated, we expect the output should be rotated by the same degree. Likewise, we need translation equivariance. Let's check that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_Fg_KHyFm2q",
    "outputId": "12f6e913-124d-495d-878e-c21de959a93e"
   },
   "outputs": [],
   "source": [
    "# checking for translation equivariance\n",
    "key2 = jax.random.PRNGKey(9)\n",
    "\n",
    "random_trans = jax.random.normal(key2, (12, 3))\n",
    "\n",
    "input_trans = input_point + random_trans\n",
    "output_2 = baseline_model(input_trans, w_test1, b)\n",
    "output_trans = random_trans + baseline_model(input_point, w_test1, b)\n",
    "\n",
    "print(\"translated first: \", output_2, \"\\n\")\n",
    "print(\"translated last: \", output_trans, \"\\n\")\n",
    "print(\"\\033[1m\" + \"difference: \" + \"\\033[0m\", mse(output_2, output_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYCwyP87GtrB"
   },
   "source": [
    "As expected, our model isn't translation equviariant either. We can solve this problem a few ways. One way is to augment our data in order to teach our model equivariance. This requires more training and data storage, so let's look at a more compact approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIgZvoEjs3Q"
   },
   "source": [
    "## E3NN Basics\n",
    "E3NN is a library for creating equivariant neural networks, specifically in E(3). E3NN is built for spatial equvariance in 3-D space, giving us equivariance with respect to the E(3) group of rotations, inversions, and translations. As discussed before, the time-dependent trajectory points do not change order, so we do not need to worry about permutation equivariance/invariance in this case; we only need E(3)-equivariance. E3NN is a great tool for this problem because we have 3-dimensional points in space, and if we transform them in space, we want the output to transform the same way.\n",
    "\n",
    "E3NN works through the use of irreducible representations (irreps). In general, representations tell you how to interact with the data with repect to the group, and irreducible representations are the smallest and complete representations. When creating a model, we give the model the irreps so that it knows how to handle the data we will give it during trianing. It's not necessary to understand what the irreps are; instead, just know that they are the smallest representations, which are similar to, and transform the same way as, the spherical harmonics. If you do want more information, you can read about irreps [here](https://arxiv.org/abs/2207.09453). Any (reducible) representation can be decomposed into irreducible representations. If you want to know more, you can check out more on the E3NN documentation website [@e3nn]. Let's take a look at how the irreps are used in this context. \n",
    "\n",
    "For this group (O(3), which includes parity), we need to find the L and d for each piece of data, where $d = 2L + 1$ (d = dimension). Look at the table below. \n",
    "\n",
    "| **parity** | **L** | **d** | **name**      |\n",
    "|------------|-------|-------|---------------|\n",
    "| even       | 0     | 1     | scalar        |\n",
    "| odd        | 0     | 1     | pseudo scalar |\n",
    "| even       | 1     | 3     | pseudo vector |\n",
    "| odd        | 1     | 3     | vector        |\n",
    "| even       | 2     | 5     |       -       |\n",
    "| odd        | 2     | 5     |       -       |\n",
    "\n",
    "The general notation is **MxLp**, where M is the number of 3-D coordinate per input, L is the L (spherical harmonic) from the table above, and p corresponds to the parity (e: even, o: odd). \n",
    "\n",
    "For example, if you wanted to portray \"12 scalars, 4 vectors\" in this format, you would write `12x0e + 4x1o`. Take a minute to make sure you understand how to use this notation, as it's essential for E3NN. E3NN deals with equivariance by receiving the irreps as a model parameter. This allows the E3NN framework to know how each input feature/output transforms under symmetry, so that it can treat each piece appropriately. As a side note, the output of an E3NN model must always be of equal or higher symmetry than your input.\n",
    "\n",
    "Because E3NN is built to handle 3D spatial data, we do not need to tell the model that we are going to give it 3D coordinates; it's implicit and **required**. The irreps_in, instead, correspond to the input node features. In this example, we don't have input features, but as an example, you can imagine we could want our model to predict the next set of coordinates, given the intitial coordinates and the corresponding atom types. In that case, our irreps_in would be the atom types (one scalar per input if we have one-hot vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msab78GJlbrF"
   },
   "source": [
    "Since we don't have input features, we'll put \"None\" for that parameter, and we want our output to be the same shape as the input: 12 vectors. However, since we are trying to predict 12 vectors out for 12 vectors in, we only need to tell the model to predict 1 vector per input `1x1o`. Take a minute to make sure you understand why this is the case. You can think of the model recognizing 12 input vectors and predicting a vector for each. Again, E3NN expects coordinate inputs, so we don't specify this for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRq67aBWrXh0"
   },
   "source": [
    "## E3NN Model\n",
    "E3NN has several models within their library, which can be found [on the E3NN github page here](https://github.com/e3nn/e3nn/tree/main/e3nn/nn/models). For this example, we will use one of these models. \n",
    "\n",
    "To use the E3NN model, we need to turn our data into a torch_geometric dataset. We'll do that now. Then we can split our data into training and testing sets.\n",
    "\n",
    "Also, instead of directly computing the next frame, we'll change it here to predict the distance to the next frame. This is a small change, but having data centered nearer zero can be better for training. We'll need to undo this when we look at the frames later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a2CF_bbv4_N"
   },
   "outputs": [],
   "source": [
    "feat = torch.from_numpy(features)  # convert to pytorch tensors\n",
    "ys = torch.from_numpy(labels)  # convert to pytorch tensors\n",
    "traj_data = []\n",
    "distances = ys - feat  # compute distances to next frame\n",
    "\n",
    "\n",
    "# make torch_geometric dataset\n",
    "# we want this to be an iterable list\n",
    "# x = None because we have no input features\n",
    "for frame, label in zip(feat, distances):\n",
    "    traj_data += [\n",
    "        torch_geometric.data.Data(\n",
    "            x=None, pos=frame.to(torch.float32), y=label.to(torch.float32)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "train_split = 1637\n",
    "train_loader = torch_geometric.loader.DataLoader(\n",
    "    traj_data[:train_split], batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch_geometric.loader.DataLoader(\n",
    "    traj_data[train_split:], batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjVxk_cwVBX-"
   },
   "source": [
    "Great! Now we're ready to define our model. Since this is a pre-built model in E3NN, so we just need to import it and define the model parameters. Note that the state of this model will save automatically, so you will need to reinitialize the model every time you want to start training. To see how these models work you can look at [this preprint](https://arxiv.org/abs/2207.09453) or [this video series](https://www.youtube.com/watch?v=q9EwZsHY1sk&list=PLx3xbphkO3qIlBoESkbafXaDtr0tq5iRd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below sets the model parameters for the model we are using. First, we tell the model our `irreps_in`, which in this case is `None`. Then, we specify the `irreps_hidden` and `layers`, which define the width and shape of our model. These are hyperparameters. `irreps_out` corresponds to our output shape, `1x1o`. We specify that our nodes have no attributes, and that we want to use spherical harmonics as our edge attributes. You don't need to be too concerned with the `number_of_basis`, `radial_layers`, or `radial_neurons`, as they don't change much between applications. The `max_radius` and `num_neighbors` are intuitive, just specify the average numbers in your max radius (a hyperparameter). If you do not know this, you can write a function that calculates an average number of neighbors. Lastly, the `num_nodes` is not important in this case since we set `reduce_output` to `False`. If we set this to `True`, that means we want to reduce our output over all `num_nodes` in our input to get a single scalar as an output. \n",
    "\n",
    "Then we just initialize our model with our defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcptjZtGB4Oc"
   },
   "outputs": [],
   "source": [
    "from e3nn.nn.models.gate_points_2101 import Network\n",
    "\n",
    "model_kwargs = {\n",
    "    \"irreps_in\": None,  # no input features\n",
    "    \"irreps_hidden\": e3nn.o3.Irreps(\"5x0e + 5x0o + 5x1e + 5x1o\"),  # hyperparameter\n",
    "    \"irreps_out\": \"1x1o\",  # 12 vectors out, but only 1 vector out per input\n",
    "    \"irreps_node_attr\": None,\n",
    "    \"irreps_edge_attr\": e3nn.o3.Irreps.spherical_harmonics(3),\n",
    "    \"layers\": 3,  # hyperparameter\n",
    "    \"max_radius\": 3.5,\n",
    "    \"number_of_basis\": 10,\n",
    "    \"radial_layers\": 1,\n",
    "    \"radial_neurons\": 128,\n",
    "    \"num_neighbors\": 11,  # average number of neighbors w/in max_radius\n",
    "    \"num_nodes\": 12,  # not important unless reduce_output is True\n",
    "    \"reduce_output\": False,  # setting this to true would give us one scalar as an output.\n",
    "}\n",
    "\n",
    "model = e3nn.nn.models.gate_points_2101.Network(\n",
    "    **model_kwargs\n",
    ")  # initializing model with parameters above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set our learning rate (hyperparameter), and our optimizer. In this case, we are using the Adam optimizer, and we initialize our gradients as zero. Since we chose Adam, we have to pass in our paramters and learning rate. Adam computes adaptive learning rates for the parameters {cite}`adam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7UOAxB7xcFE"
   },
   "outputs": [],
   "source": [
    "eta = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAJAqOY-s3TZ"
   },
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "\n",
    "val_loss = [0.0 for _ in range(epochs)]\n",
    "y_values = []\n",
    "yhat_values = []\n",
    "y_valuest = []\n",
    "yhat_valuest = []\n",
    "e = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    e += 1\n",
    "    for step, data in enumerate(train_loader):\n",
    "        yhat = model(data)\n",
    "        if e == epochs:\n",
    "            y_valuest.append(data.y)\n",
    "            yhat_valuest.append(yhat)\n",
    "        loss_1 = torch.mean((yhat - data.y) ** 2)\n",
    "        loss_1.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(test_loader):\n",
    "            yhat = model(data)\n",
    "            if e == epochs:\n",
    "                y_values.append(data.y)\n",
    "                yhat_values.append(yhat)\n",
    "            loss2 = torch.mean((yhat - data.y) ** 2)\n",
    "            val_loss[epoch] += (loss2).detach()\n",
    "    val_loss[epoch] = val_loss[epoch] / 410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "262rPIy2QNi-",
    "outputId": "5b5fd0f8-4b3a-46c0-c367-263916716999"
   },
   "outputs": [],
   "source": [
    "v_loss = torch.tensor(val_loss)\n",
    "\n",
    "plt.plot(v_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjRx285tsQ6N",
    "outputId": "a95dfa44-fc7e-4dc8-a339-3b85aedf9411"
   },
   "outputs": [],
   "source": [
    "print(\"final loss value: \", val_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqbguVztoaZK"
   },
   "outputs": [],
   "source": [
    "yhat_valuest = [y.detach().numpy() for y in yhat_valuest]\n",
    "yhat_values = [y.numpy() for y in yhat_values]\n",
    "\n",
    "yhat_total = np.concatenate([yhat_valuest, yhat_values])\n",
    "original = feat.numpy()\n",
    "\n",
    "y_arr = ys.numpy()\n",
    "yhat_arr = np.stack(yhat_total + original, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "bZ9dR00h6m33",
    "outputId": "ab8d583b-9ca8-4098-9c65-3031c6184e5f"
   },
   "outputs": [],
   "source": [
    "yhat_vs = np.stack(yhat_values + original[train_split:], axis=0)\n",
    "y_vs = np.stack(ys[train_split:], axis=0)\n",
    "\n",
    "x_coords_ys = np.reshape(y_vs[:, :, :1], (-1, 1))\n",
    "y_coords_ys = np.reshape(y_vs[:, :, 1:2], (-1, 1))\n",
    "z_coords_ys = np.reshape(y_vs[:, :, 2:], (-1, 1))\n",
    "\n",
    "x_coords_yhs = np.reshape(yhat_vs[:, :, :1], (-1, 1))\n",
    "y_coords_yhs = np.reshape(yhat_vs[:, :, 1:2], (-1, 1))\n",
    "z_coords_yhs = np.reshape(yhat_vs[:, :, 2:], (-1, 1))\n",
    "\n",
    "plt.title(\"x-coordinates\")\n",
    "plt.plot(x_coords_ys, x_coords_ys, \"-\")\n",
    "plt.plot(x_coords_ys, x_coords_yhs, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"y-coordinates\")\n",
    "plt.plot(y_coords_ys, y_coords_ys, \"-\")\n",
    "plt.plot(y_coords_ys, y_coords_yhs, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "4UkR5q1ty2jf",
    "outputId": "430bffe7-39c2-4da4-a251-7c59d10d8aa0"
   },
   "outputs": [],
   "source": [
    "yhat_centered = centerofmass(yhat_vs)\n",
    "y_center = centerofmass(y_vs)\n",
    "plt.title(\"centered coordinates\")\n",
    "plt.plot(y_center, y_center, \"-\")\n",
    "plt.plot(y_center, yhat_centered, \".\")\n",
    "plt.xlabel(\"Trajectory\")\n",
    "plt.ylabel(\"Predicted Trajectory\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3ZL_tcEaHGS"
   },
   "source": [
    "Wow! Clearly these parity plots look better than for the baseline model. Let's again look at the trajectory versus the trajectory predictions, to see visually how they compare. We will look at the training and the validation data together, just note that because the colors represent time, we can look at the purple section to see just the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "vcj_TyTyV5kD",
    "outputId": "1a6aca40-6c5b-47c6-9594-6f82a4e9de44"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, squeeze=True, figsize=(16, 4))\n",
    "\n",
    "ys_total = np.concatenate([y_st, y_arr])\n",
    "yhat_st = np.stack(yhatst, axis=0)\n",
    "yhats_total = np.concatenate([yhat_st, yhat_arr])\n",
    "\n",
    "\n",
    "axs[0].set_title(\"Trajectory\")\n",
    "axs[1].set_title(\"Predicted Trajectory\")\n",
    "\n",
    "cmap = plt.get_cmap(\"cool\")\n",
    "for i in range(0, 2047, 40):\n",
    "    axs[0].plot(y_arr[i, :, 0], y_arr[i, :, 1], \".-\", alpha=0.2, color=cmap(i / 2047))\n",
    "    axs[1].plot(\n",
    "        yhat_arr[i, :, 0], yhat_arr[i, :, 1], \".-\", alpha=0.2, color=cmap(i / 2047)\n",
    "    )\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqu6o27P0KFA"
   },
   "source": [
    "This looks pretty good!\n",
    "Let's run each model on the last frame to get an extrapolated frame. Remember that for the E3NN model, we predicting displacements, so we'll just need to add our final displacement back to our final coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXoZa2iIXO78"
   },
   "outputs": [],
   "source": [
    "last_frame_bl = y_v\n",
    "extrp_bl = baseline_model(last_frame_bl, w, b)\n",
    "\n",
    "last_frame_e3nn = yhat + data.pos\n",
    "lf_e3nn = []\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "lf_e3nn += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None,\n",
    "        pos=last_frame_e3nn.to(torch.float32),\n",
    "        y=last_frame_e3nn.to(torch.float32),\n",
    "    )\n",
    "]\n",
    "\n",
    "lf_loader = torch_geometric.loader.DataLoader(lf_e3nn, batch_size=1, shuffle=False)\n",
    "\n",
    "# run through model\n",
    "for i in lf_loader:\n",
    "    extrp_e3nn = model(i)\n",
    "\n",
    "# add extrapolated displacements back into last frame\n",
    "extrp_e3nn += last_frame_e3nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "cSTEG_u8aePB",
    "outputId": "b507ddd7-6379-48a5-83ce-ec157e92b72e"
   },
   "outputs": [],
   "source": [
    "extrp_bl = np.array(extrp_bl)\n",
    "extrp_e3nn = extrp_e3nn.detach().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, squeeze=True, figsize=(16, 4))\n",
    "\n",
    "axs[0].set_title(\"Baseline Model Extrapolated Frame\")\n",
    "axs[1].set_title(\"e3nn Model Extrapolated Frame\")\n",
    "\n",
    "cmap = plt.get_cmap(\"cool\")\n",
    "\n",
    "axs[0].plot(extrp_bl[:, 0], extrp_bl[:, 1], \".-\", alpha=0.2)\n",
    "axs[1].plot(extrp_e3nn[:, 0], extrp_e3nn[:, 1], \".-\", alpha=0.2)\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isEXdrcezs15"
   },
   "source": [
    "We can clearly see that the e3nn model predicts trajectories much closer to our data. Again, the baseline model has predicted poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "185qmnmbcFNX"
   },
   "source": [
    "Now we can check for equivariance in the same way that we did before with the baseline model. Let's just take the final frame and rotate, then compare to just the output rotated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmgYDX78cEE9",
    "outputId": "a05d8524-6c0c-4329-f759-5e506cea4ff3"
   },
   "outputs": [],
   "source": [
    "# checking for rotation equivariance\n",
    "import scipy.spatial.transform as trans\n",
    "\n",
    "# rotate around x coordinate by 80 degrees\n",
    "rot = trans.Rotation.from_euler(\"x\", 80, degrees=True)\n",
    "\n",
    "key3 = jax.random.PRNGKey(58)\n",
    "\n",
    "input_point = np.asarray(jax.random.normal(key3, (12, 3)))\n",
    "\n",
    "input_rot = rot.apply(input_point)\n",
    "input_point = torch.from_numpy(input_point)\n",
    "input_rot = torch.from_numpy(input_rot)\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "rot_first = []\n",
    "rot_first += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None, pos=input_rot.to(torch.float32), y=input_rot.to(torch.float32)\n",
    "    )\n",
    "]\n",
    "rf_loader = torch_geometric.loader.DataLoader(rot_first, batch_size=1, shuffle=False)\n",
    "# run through model\n",
    "for i in rf_loader:\n",
    "    output_1 = model(i)\n",
    "\n",
    "\n",
    "# format as torch geometric dataset (dummy y values)\n",
    "rot_last = []\n",
    "rot_last += [\n",
    "    torch_geometric.data.Data(\n",
    "        x=None, pos=input_point.to(torch.float32), y=input_point.to(torch.float32)\n",
    "    )\n",
    "]\n",
    "rl_loader = torch_geometric.loader.DataLoader(rot_last, batch_size=1, shuffle=False)\n",
    "# run through model\n",
    "for i in rl_loader:\n",
    "    output_2 = model(i)\n",
    "\n",
    "output_2 = output_2.detach().numpy()\n",
    "output_1 = output_1.detach().numpy()\n",
    "\n",
    "output_rot = []\n",
    "for xyz in output_2:\n",
    "    coord = rot.apply(xyz)\n",
    "    output_rot.append(coord)\n",
    "output_rot = np.array(output_rot)\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "print(\"rotated first: \\n\", output_1, \"\\n\")\n",
    "print(\"rotated last: \\n\", output_rot, \"\\n\")\n",
    "print(\"\\033[1m\" + \"difference: \" + \"\\033[0m\", np.array([mse(output_1, output_rot)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsgSyfwyZ1xc"
   },
   "source": [
    "Great! Our random array, when rotated first, gives the same results as when we rotated last! Now we know we have rotational equivariances. I won't go further to test translational equivariances; I will leave that as an exercise. The E3NN model outperforms the baseline significantly, and it is E(3)-equivariant, unlike our baseline model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
