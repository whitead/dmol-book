{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining and Transfer Learning\n",
    "\n",
    "Supervised deep learning models are generally trained on labeled data to achieve a single task. While this is useful when large amounts of labeled data are available, this approach of learning a single task leads to poor generalization when labeled data is scarce {cite}`mao2020pretraining`. For most practical problems, the number of labeled examples available are limited or expensive to obtain. Self-supervised learning (SSL) is an unsupervised learning approach which enables a model to learn from the limited amount of labeled data and automatically annotate unlabeled data. This eliminates the need for labeled data. The labeled data from SSL can then be used to fine-tune a pre-trained deep learning model for downstream tasks. This also allows for transfer learning in low data regimes.\n",
    "\n",
    "Pre-training involves training shallow neural networks using self-supervised approach before  stacking them together to create a deep neural network.\n",
    "\n",
    "Why does pretraining work?\n",
    "\n",
    "How does pretraining work?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer learning vs fine-tuning\n",
    "\n",
    "\n",
    "New dataset small but similar to original data - Transfer learning -  Freeze feature extraction layers and modify the last layer for the task at hand. \n",
    "\n",
    "New dataset large but similar to original data - Fine-tuning - Change the last layer to make it task-specific but also change parts of the feature extraction process.\n",
    "\n",
    "[image for TL vs FT]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Noisy student/teacher method\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ESM/Unirep work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretraining for graph models\n",
    "\n",
    "{cite}`xie2021graphpretraining`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chemberta - pretraining in context of chemistry and molecules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example for transfer learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Videos\n",
    "\n",
    "https://www.youtube.com/watch?v=3nbin3bT8ec&t=1s - TL vs FT\n",
    "\n",
    "https://www.youtube.com/watch?v=qWUslmU7BjY - TL in NLP and HuggingFace"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}