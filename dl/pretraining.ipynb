{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining\n",
    "\n",
    "Supervised deep learning models are generally trained on labeled data to achieve a single task. While this is useful when large amounts of labeled data are available, this approach of learning a single task leads to poor generalization when labeled data is scarce {cite}`mao2020pretraining`. For most practical problems, the number of labeled examples available are limited or expensive to obtain. Self-supervised learning (SSL) is an unsupervised learning approach which enables a model to learn from the limited amount of labeled data and automatically annotate unlabeled data. This eliminates the need for labeled data. The labeled data from SSL can then be used to fine-tune a pre-trained deep learning model for downstream tasks. This also allows for transfer learning in low data regimes.\n",
    "\n",
    "How does pretraining work?\n",
    "\n",
    "Pre-training involves training shallow neural networks using self-supervised approach before  stacking them together to create a deep neural network.\n",
    "\n",
    "Why does pretraining work?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer learning vs fine-tuning\n",
    "\n",
    "\n",
    "New dataset small but similar to original data - Transfer learning -  Freeze feature extraction layers and modify the last layer for the task at hand. \n",
    "\n",
    "New dataset large but similar to original data - Fine-tuning - Change the last layer to make it task-specific but also change parts of the feature extraction process.\n",
    "\n",
    "[image for TL vs FT]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretraining for graph models\n",
    "\n",
    "Contrastive learning and predictive learning\n",
    "{cite}`xie2021graphpretraining`\n",
    "\n",
    "Contrastive learning - \"Given training graphs, contrastive learning aims to learn one or more encoders such that representations of similar graph instances agree with each other, and that representations of dissimilar graph instances disagree with each other.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applications in chemistry"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Noisy student/teacher method (Alphafold)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ESM and Unirep for protein sequence embedding\n",
    "\n",
    "The protein representation and feature extraction layers in Unirep[cite] and ESM[cite] models have been used by researchers for transfer learning as well as fine tuning for downstream tasks. \n",
    "\n",
    "Unirep model was trained on 24 million Uniref50 primary amino-acid sequences. The model was trained to perform next amino-acid prediction (minimizing cross-entropy loss) and, in so doing, was forced to learn how to internally represent proteins. During application, the trained model is used to generate a single fixed-length vector representation of the input sequence by globally averaging intermediate mLSTM numerical summaries (the hidden states). A top model (for example, a sparse linear regression or random forest) trained on top of the representation, which acts as a featurization of the input sequence, enables supervised learning on diverse protein informatics tasks.\n",
    "\n",
    "ESM model by Facebook research uses UniRef dataset to create pretraining datasets using a recurrent LSTM bidirectional language model. [add details]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chemberta - pretraining in context of chemistry and molecules\n",
    "\n",
    "ChemBERTa is based on the RoBERTa transformer implementation in HuggingFace. Their implementation uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. They use the pretraining procedure from Roberta on the PubChem 77M dataset by masking 15% of all input strings. The pretrained dataset is then used to finetune several MoleculeNet classification models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Example\n",
    "\n",
    "Reference - Chemberta tutorials - possiblly use for solubility dataset used in the book."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Videos\n",
    "\n",
    "https://www.youtube.com/watch?v=3nbin3bT8ec&t=1s - TL vs FT\n",
    "\n",
    "https://www.youtube.com/watch?v=qWUslmU7BjY - TL in NLP and HuggingFace"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}