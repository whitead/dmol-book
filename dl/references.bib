@article{finn2017model,
  title   = {Model-agnostic meta-learning for fast adaptation of deep networks},
  author  = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal = {arXiv preprint arXiv:1703.03400},
  year    = {2017}
}

@article{45826,
  title   = {Neural Architecture Search with Reinforcement Learning},
  author  = {Barret Zoph and Quoc V. Le},
  journal = {arXiv preprint arXiv:1611.01578},
  year    = {2017},
  url     = {https://arxiv.org/abs/1611.01578}
}


@article{LeNet,
  author  = {Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
  journal = {Proceedings of the IEEE},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  volume  = {86},
  number  = {11},
  pages   = {2278-2324}
}


@article{hyperband,
  author  = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1-52},
  url     = {http://jmlr.org/papers/v18/16-558.html}
}

@inproceedings{zhao2020exploring,
  title     = {Exploring self-attention for image recognition},
  author    = {Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {10076--10085},
  year      = {2020}
}



@inproceedings{glorot2010understanding,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages     = {249--256},
  year      = {2010}
}




@article{noconv,
  title     = {Deep, big, simple neural nets for handwritten digit recognition},
  author    = {Cire{\c{s}}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  volume    = {22},
  number    = {12},
  pages     = {3207--3220},
  year      = {2010},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{solubility,
  title     = {Bioinformatics approaches for improved recombinant protein production in Escherichia coli: protein solubility prediction},
  author    = {Chang, Catherine Ching Han and Song, Jiangning and Tey, Beng Ti and Ramanan, Ramakrishnan Nagasundara},
  journal   = {Briefings in bioinformatics},
  volume    = {15},
  number    = {6},
  pages     = {953--962},
  year      = {2014},
  publisher = {Oxford University Press}
}

@article{deepsol,
  author   = {Khurana, Sameer and Rawi, Reda and Kunji, Khalid and Chuang, Gwo-Yu and Bensmail, Halima and Mall, Raghvendra},
  title    = {{DeepSol: a deep learning framework for sequence-based protein solubility prediction}},
  journal  = {Bioinformatics},
  volume   = {34},
  number   = {15},
  pages    = {2605-2613},
  year     = {2018},
  month    = {03},
  abstract = {{Protein solubility plays a vital role in pharmaceutical research and production yield. For a given protein, the extent of its solubility can represent the quality of its function, and is ultimately defined by its sequence. Thus, it is imperative to develop novel, highly accurate in silico sequence-based protein solubility predictors. In this work we propose, DeepSol, a novel Deep Learning-based protein solubility predictor. The backbone of our framework is a convolutional neural network that exploits k-mer structure and additional sequence and structural features extracted from the protein sequence.DeepSol outperformed all known sequence-based state-of-the-art solubility prediction methods and attained an accuracy of 0.77 and Matthew’s correlation coefficient of 0.55. The superior prediction accuracy of DeepSol allows to screen for sequences with enhanced production capacity and can more reliably predict solubility of novel proteins.DeepSol’s best performing models and results are publicly deposited at https://doi.org/10.5281/zenodo.1162886 (Khurana and Mall, 2018).Supplementary data are available at Bioinformatics online.}},
  issn     = {1367-4803},
  doi      = {10.1093/bioinformatics/bty166},
  url      = {https://doi.org/10.1093/bioinformatics/bty166}
}



@article{neal2018modern,
  title   = {A modern take on the bias-variance tradeoff in neural networks},
  author  = {Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal = {arXiv preprint arXiv:1810.08591},
  year    = {2018}
}

@inproceedings{gal2016dropout,
  title     = {Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author    = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = {international conference on machine learning},
  pages     = {1050--1059},
  year      = {2016}
}


@article{kipf2016semi,
  title   = {Semi-supervised classification with graph convolutional networks},
  author  = {Kipf, Thomas N and Welling, Max},
  journal = {arXiv preprint arXiv:1609.02907},
  year    = {2016}
}

@article{battaglia2018relational,
  title   = {Relational inductive biases, deep learning, and graph networks},
  author  = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal = {arXiv preprint arXiv:1806.01261},
  year    = {2018}
}

@article{li2015gated,
  title   = {Gated graph sequence neural networks},
  author  = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  journal = {arXiv preprint arXiv:1511.05493},
  year    = {2015}
}

@article{chung2014empirical,
  title   = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author  = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1412.3555},
  year    = {2014}
}

@article{gilmer2017neural,
  title   = {Neural message passing for quantum chemistry},
  author  = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  journal = {arXiv preprint arXiv:1704.01212},
  year    = {2017}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@article{luong2015effective,
  title   = {Effective approaches to attention-based neural machine translation},
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1508.04025},
  year    = {2015}
}
@article{zhang2018gaan,
  title   = {Gaan: Gated attention networks for learning on large and spatiotemporal graphs},
  author  = {Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
  journal = {arXiv preprint arXiv:1803.07294},
  year    = {2018}
}

@article{treisman1980feature,
  title     = {A feature-integration theory of attention},
  author    = {Treisman, Anne M and Gelade, Garry},
  journal   = {Cognitive psychology},
  volume    = {12},
  number    = {1},
  pages     = {97--136},
  year      = {1980},
  publisher = {Elsevier}
}

@article{BALUJA1997329,
  title    = {Expectation-based selective attention for visual monitoring and control of a robot vehicle},
  journal  = {Robotics and Autonomous Systems},
  volume   = {22},
  number   = {3},
  pages    = {329 - 344},
  year     = {1997},
  note     = {Robot Learning: The New Wave},
  issn     = {0921-8890},
  doi      = {https://doi.org/10.1016/S0921-8890(97)00046-8},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889097000468},
  author   = {Shumeet Baluja and Dean A. Pomerleau},
  keywords = {Expectation-based selective attention, Autonomous navigation, Temporal coherence, Saliency map, Artificial neural networks},
  abstract = {Reliable vision-based control of an autonomous vehicle requires the ability to focus attention on the important features in an input scene. Previous work with an autonomous lane following system, ALVINN (Pomerleau, 1993), has yielded good results in uncluttered conditions. This paper presents an artificial neural network based learning approach for handling difficult scenes which will confuse the ALVINN system. This work presents a mechanism for achieving task-specific focus of attention by exploiting temporal coherence. A saliency map, which is based upon a computed expectation of the contents of the inputs in the next time step, indicates which regions of the input retina are important for performing the task. The saliency map can be used to accentuate the features which are important for the task, and de-emphasize those which are not.}
}

@article{thomas2018tensor,
  title   = {Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author  = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  journal = {arXiv preprint arXiv:1802.08219},
  year    = {2018}
}

@inproceedings{weiler20183d,
  title     = {3d steerable cnns: Learning rotationally equivariant features in volumetric data},
  author    = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {10381--10392},
  year      = {2018}
}

@article{rupp2012fast,
  title     = {Fast and accurate modeling of molecular atomization energies with machine learning},
  author    = {Rupp, Matthias and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert and Von Lilienfeld, O Anatole},
  journal   = {Physical review letters},
  volume    = {108},
  number    = {5},
  pages     = {058301},
  year      = {2012},
  publisher = {APS}
}

@article{Bart,
  title     = {On representing chemical environments},
  author    = {Bart\'ok, Albert P. and Kondor, Risi and Cs\'anyi, G\'abor},
  journal   = {Phys. Rev. B},
  volume    = {87},
  issue     = {18},
  pages     = {184115},
  numpages  = {16},
  year      = {2013},
  month     = {May},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevB.87.184115},
  url       = {https://link.aps.org/doi/10.1103/PhysRevB.87.184115}
}


@article{behler2011atom,
  title     = {Atom-centered symmetry functions for constructing high-dimensional neural network potentials},
  author    = {Behler, J{\"o}rg},
  journal   = {The Journal of chemical physics},
  volume    = {134},
  number    = {7},
  pages     = {074106},
  year      = {2011},
  publisher = {American Institute of Physics}
}

@inproceedings{ravanbakhsh2017equivariance,
  title     = {Equivariance Through Parameter-Sharing},
  author    = {Ravanbakhsh, Siamak and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  booktitle = {International Conference on Machine Learning},
  pages     = {2892--2901},
  year      = {2017}
}

@article{mesquita2020rethinking,
  title   = {Rethinking pooling in graph neural networks},
  author  = {Mesquita, Diego and Souza, Amauri and Kaski, Samuel},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}

@article{luzhnica2019graph,
  title   = {On graph classification networks, datasets and baselines},
  author  = {Luzhnica, Enxhell and Day, Ben and Li{\`o}, Pietro},
  journal = {arXiv preprint arXiv:1905.04682},
  year    = {2019}
}

@inproceedings{xu2018powerful,
  title     = {How Powerful are Graph Neural Networks?},
  author    = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  booktitle = {International Conference on Learning Representations},
  year      = {2018}
}

@inproceedings{hamilton2017inductive,
  title     = {Inductive representation learning on large graphs},
  author    = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  booktitle = {Advances in neural information processing systems},
  pages     = {1024--1034},
  year      = {2017}
}


@article{shchur2018pitfalls,
  title   = {Pitfalls of graph neural network evaluation},
  author  = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  journal = {arXiv preprint arXiv:1811.05868},
  year    = {2018}
}

@inproceedings{errica2019fair,
  title     = {A Fair Comparison of Graph Neural Networks for Graph Classification},
  author    = {Errica, Federico and Podda, Marco and Bacciu, Davide and Micheli, Alessio},
  booktitle = {International Conference on Learning Representations},
  year      = {2019}
}
@article{dwivedi2020benchmarking,
  title   = {Benchmarking graph neural networks},
  author  = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal = {arXiv preprint arXiv:2003.00982},
  year    = {2020}
}

@article{bronstein2017geometric,
  title     = {Geometric deep learning: going beyond euclidean data},
  author    = {Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal   = {IEEE Signal Processing Magazine},
  volume    = {34},
  number    = {4},
  pages     = {18--42},
  year      = {2017},
  publisher = {IEEE}
}

@article{wu2020comprehensive,
  title     = {A comprehensive survey on graph neural networks},
  author    = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2020},
  publisher = {IEEE}
}

@article{kobyzev2020normalizing,
  title     = {Normalizing flows: An introduction and review of current methods},
  author    = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2020},
  publisher = {IEEE}
}

@inproceedings{papamakarios2017masked,
  title     = {Masked autoregressive flow for density estimation},
  author    = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2338--2347},
  year      = {2017}
}

@inproceedings{kingma2016improved,
  title     = {Improved variational inference with inverse autoregressive flow},
  author    = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle = {Advances in neural information processing systems},
  pages     = {4743--4751},
  year      = {2016}
}

@article{kim2018flowavenet,
  title   = {FloWaveNet: A generative flow for raw audio},
  author  = {Kim, Sungwon and Lee, Sang-gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  journal = {arXiv preprint arXiv:1811.02155},
  year    = {2018}
}

@article{dinh2016density,
  title   = {Density estimation using real nvp},
  author  = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal = {arXiv preprint arXiv:1605.08803},
  year    = {2016}
}

@article{das2019dimensionality,
  title   = {Dimensionality reduction flows},
  author  = {Das, Hari Prasanna and Abbeel, Pieter and Spanos, Costas J},
  journal = {arXiv preprint arXiv:1908.01686},
  year    = {2019}
}

@article{foote2000relation,
  title     = {A relation between the principal axes of inertia and ligand binding},
  author    = {Foote, Jefferson and Raman, Anandi},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {97},
  number    = {3},
  pages     = {978--983},
  year      = {2000},
  publisher = {National Acad Sciences}
}

@article{esteves2020theoretical,
  title   = {Theoretical Aspects of Group Equivariant Neural Networks},
  author  = {Esteves, Carlos},
  journal = {arXiv preprint arXiv:2004.05154},
  year    = {2020}
}

@article{krenn2020self,
  title     = {Self-Referencing Embedded Strings (SELFIES): A 100\% robust molecular string representation},
  author    = {Krenn, Mario and H{\"a}se, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alan},
  journal   = {Machine Learning: Science and Technology},
  volume    = {1},
  number    = {4},
  pages     = {045024},
  year      = {2020},
  publisher = {IOP Publishing}
}

@inproceedings{mathieu2019disentangling,
  title     = {Disentangling disentanglement in variational autoencoders},
  author    = {Mathieu, Emile and Rainforth, Tom and Siddharth, N and Teh, Yee Whye},
  booktitle = {International Conference on Machine Learning},
  pages     = {4402--4412},
  year      = {2019},
  publisher = {PMLR}
}


@inproceedings{klicpera2019directional,
  title     = {Directional Message Passing for Molecular Graphs},
  author    = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{jing2020learning,
  title   = {Learning from Protein Structure with Geometric Vector Perceptrons},
  author  = {Jing, Bowen and Eismann, Stephan and Suriana, Patricia and Townshend, Raphael JL and Dror, Ron},
  journal = {arXiv preprint arXiv:2009.01411},
  year    = {2020}
}

@article{li2020graph,
  title     = {Graph neural network based coarse-grained mapping prediction},
  author    = {Li, Zhiheng and Wellawatte, Geemi P and Chakraborty, Maghesree and Gandhi, Heta A and Xu, Chenliang and White, Andrew D},
  journal   = {Chemical Science},
  volume    = {11},
  number    = {35},
  pages     = {9524--9531},
  year      = {2020},
  publisher = {Royal Society of Chemistry}
}

@article{yang2020predicting,
  title     = {Predicting Chemical Shifts with Graph Neural Networks},
  author    = {Yang, Ziyue and Chakraborty, Maghesree and White, Andrew D},
  journal   = {bioRxiv},
  year      = {2020},
  publisher = {Cold Spring Harbor Laboratory}
}

@article{maziarka2020molecule,
  title   = {Molecule Attention Transformer},
  author  = {Maziarka, {\L}ukasz and Danel, Tomasz and Mucha, S{\l}awomir and Rataj, Krzysztof and Tabor, Jacek and Jastrz{\k{e}}bski, Stanis{\l}aw},
  journal = {arXiv preprint arXiv:2002.08264},
  year    = {2020}
}

@article{miller2020relevance,
  title   = {Relevance of rotationally equivariant convolutions for predicting molecular properties},
  author  = {Miller, Benjamin Kurt and Geiger, Mario and Smidt, Tess E and No{\'e}, Frank},
  journal = {arXiv preprint arXiv:2008.08461},
  year    = {2020}
}


@article{wu2020stochastic,
  title   = {Stochastic Normalizing Flows},
  author  = {Wu, Hao and K{\"o}hler, Jonas and No{\'e}, Frank},
  journal = {arXiv preprint arXiv:2002.06707},
  year    = {2020}
}

@inproceedings{papamakarios2019sequential,
  title        = {Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows},
  author       = {Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages        = {837--848},
  year         = {2019},
  organization = {PMLR}
}

@article{papamakarios2019normalizing,
  title   = {Normalizing flows for probabilistic modeling and inference},
  author  = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal = {arXiv preprint arXiv:1912.02762},
  year    = {2019}
}

@inproceedings{zaheer2017deep,
  title     = {Deep sets},
  author    = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  booktitle = {Advances in neural information processing systems},
  pages     = {3391--3401},
  year      = {2017}
}

@inproceedings{kondor2018generalization,
  title     = {On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups},
  author    = {Kondor, Risi and Trivedi, Shubhendu},
  booktitle = {International Conference on Machine Learning},
  pages     = {2747--2755},
  year      = {2018}
}


@article{cohen2019general,
  title   = {A general theory of equivariant cnns on homogeneous spaces},
  author  = {Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  pages   = {9145--9156},
  year    = {2019}
}


@article{winter2021auto,
  title   = {Auto-Encoding Molecular Conformations},
  author  = {Winter, Robin and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  journal = {arXiv preprint arXiv:2101.01618},
  year    = {2021}
}

@article{finzi2020generalizing,
  title   = {Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data},
  author  = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  journal = {arXiv preprint arXiv:2002.12880},
  year    = {2020}
}

@article{romero2020attentive,
  title   = {Attentive Group Equivariant Convolutional Networks},
  author  = {Romero, David W and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal = {arXiv},
  pages   = {arXiv--2002},
  year    = {2020}
}

@inproceedings{cohen2016group,
  title     = {Group equivariant convolutional networks},
  author    = {Cohen, Taco and Welling, Max},
  booktitle = {International conference on machine learning},
  pages     = {2990--2999},
  year      = {2016}
}

@article{wang2020equivariant,
  title   = {Equivariant Maps for Hierarchical Structures},
  author  = {Wang, Renhao and Albooyeh, Marjan and Ravanbakhsh, Siamak},
  journal = {arXiv preprint arXiv:2006.03627},
  year    = {2020}
}

@article{batzner2021se3equivariant,
  title   = {SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials},
  author  = {Simon Batzner and Tess E. Smidt and Lixin Sun and Jonathan P. Mailoa and Mordechai Kornbluth and Nicola Molinari and Boris Kozinsky},
  year    = {2021},
  journal = {arXiv preprint arXiv:2101.03164}
}

@article{klicpera2020directional,
  title   = {Directional message passing for molecular graphs},
  author  = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  journal = {arXiv preprint arXiv:2003.03123},
  year    = {2020}
}

@book{serre1977linear,
  title     = {Linear representations of finite groups},
  author    = {Serre, Jean-Pierre},
  volume    = {42},
  year      = {1977},
  publisher = {Springer}
}

@book{zee2016,
  title     = {Group theory in a nutshell for physicists},
  author    = {Zee, Anthony},
  publisher = {Princeton University Press},
  year      = {2016}
}

@article{musil2021physicsinspired,
  title   = {Physics-inspired structural representations for molecules and materials},
  author  = {Felix Musil and Andrea Grisafi and Albert P. Bartók and Christoph Ortner and Gábor Csányi and Michele Ceriotti},
  journal = {arXiv preprint arXiv:2101.04673},
  year    = {2021}
}


@article{chew2020fast,
  title     = {Fast predictions of liquid-phase acid-catalyzed reaction rates using molecular dynamics simulations and convolutional neural networks},
  author    = {Chew, Alex K and Jiang, Shengli and Zhang, Weiqi and Zavala, Victor M and Van Lehn, Reid C},
  journal   = {Chemical Science},
  volume    = {11},
  number    = {46},
  pages     = {12464--12476},
  year      = {2020},
  publisher = {Royal Society of Chemistry}
}

@article{lang2020wigner,
  title   = {A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels},
  author  = {Lang, Leon and Weiler, Maurice},
  journal = {arXiv preprint arXiv:2010.10952},
  year    = {2020}
}


@article{kondor2018clebsch,
  title   = {Clebsch-gordan nets: a fully fourier space spherical convolutional neural network},
  author  = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  journal = {arXiv preprint arXiv:1806.09231},
  year    = {2018}
}

@inproceedings{hoogeboom2021argmax,
  title     = {Argmax Flows: Learning Categorical Distributions with Normalizing Flows},
  author    = {Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
  booktitle = {Third Symposium on Advances in Approximate Bayesian Inference},
  year      = {2021},
  url       = {https://openreview.net/forum?id=fdsXhAy5Cp}
}

@article{Xie2018Crystal,
  title     = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
  author    = {Xie, Tian and Grossman, Jeffrey C.},
  journal   = {Phys. Rev. Lett.},
  volume    = {120},
  issue     = {14},
  pages     = {145301},
  numpages  = {6},
  year      = {2018},
  month     = {Apr},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.120.145301},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301}
}

@article{Krenn_2020,
  doi       = {10.1088/2632-2153/aba947},
  url       = {https://doi.org/10.1088/2632-2153/aba947},
  year      = 2020,
  month     = {nov},
  publisher = {{IOP} Publishing},
  volume    = {1},
  number    = {4},
  pages     = {045024},
  author    = {Mario Krenn and Florian Häse and AkshatKumar Nigam and Pascal Friederich and Alan Aspuru-Guzik},
  title     = {Self-referencing embedded strings ({SELFIES}): A 100{\%} robust molecular string representation},
  journal   = {Machine Learning: Science and Technology},
  abstract  = {The discovery of novel materials and functional molecules can help to solve some of society’s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering–generally denoted as inverse design–was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model’s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.}
}

@article{rajan2020decimer,
  title     = {DECIMER: towards deep learning for chemical image recognition},
  author    = {Rajan, Kohulan and Zielesny, Achim and Steinbeck, Christoph},
  journal   = {Journal of Cheminformatics},
  volume    = {12},
  number    = {1},
  pages     = {1--9},
  year      = {2020},
  publisher = {Springer}
}

@article{segler2018generating,
  title     = {Generating focused molecule libraries for drug discovery with recurrent neural networks},
  author    = {Segler, Marwin HS and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P},
  journal   = {ACS central science},
  volume    = {4},
  number    = {1},
  pages     = {120--131},
  year      = {2018},
  publisher = {ACS Publications}
}


@article{gomez2018automatic,
  title     = {Automatic chemical design using a data-driven continuous representation of molecules},
  author    = {G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n},
  journal   = {ACS central science},
  volume    = {4},
  number    = {2},
  pages     = {268--276},
  year      = {2018},
  publisher = {ACS Publications}
}


@article{chithrananda2020chemberta,
  title   = {ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
  author  = {Chithrananda, Seyone and Grand, Gabe and Ramsundar, Bharath},
  journal = {arXiv preprint arXiv:2010.09885},
  year    = {2020}
}

@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {arXiv preprint arXiv:2005.14165},
  year    = {2020}
}

@article{liu2019roberta,
  title   = {Roberta: A robustly optimized bert pretraining approach},
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1907.11692},
  year    = {2019}
}

@article{reynolds2021prompt,
  title   = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
  author  = {Laria Reynolds and Kyle McDonell},
  year    = {2021},
  journal = {arXiv preprint arXiv:2102.07350}
}

@article{tshitoyan2019unsupervised,
  title     = {Unsupervised word embeddings capture latent knowledge from materials science literature},
  author    = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A and Ceder, Gerbrand and Jain, Anubhav},
  journal   = {Nature},
  volume    = {571},
  number    = {7763},
  pages     = {95--98},
  year      = {2019},
  publisher = {Nature Publishing Group}
}

@article{friedrich2020sofc,
  title   = {The SOFC-Exp corpus and neural approaches to information extraction in the materials science domain},
  author  = {Friedrich, Annemarie and Adel, Heike and Tomazic, Federico and Hingerl, Johannes and Benteau, Renou and Maruscyk, Anika and Lange, Lukas},
  journal = {arXiv preprint arXiv:2006.03039},
  year    = {2020}
}


@article{weininger1988smiles,
  title     = {SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author    = {Weininger, David},
  journal   = {Journal of chemical information and computer sciences},
  volume    = {28},
  number    = {1},
  pages     = {31--36},
  year      = {1988},
  publisher = {ACS Publications}
}


@article{wang2021molclr,
  title   = {MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks},
  author  = {Yuyang Wang and Jianren Wang and Zhonglin Cao and Amir Barati Farimani},
  year    = {2021},
  journal = {arXiv preprint arXiv:2102.10056}
}

@article{heller2015inchi,
  title     = {InChI, the IUPAC international chemical identifier},
  author    = {Heller, Stephen R and McNaught, Alan and Pletnev, Igor and Stein, Stephen and Tchekhovskoi, Dmitrii},
  journal   = {Journal of cheminformatics},
  volume    = {7},
  number    = {1},
  pages     = {1--34},
  year      = {2015},
  publisher = {BioMed Central}
}


@article{butler2018machine,
  title     = {Machine learning for molecular and materials science},
  author    = {Butler, Keith T and Davies, Daniel W and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  journal   = {Nature},
  volume    = {559},
  number    = {7715},
  pages     = {547--555},
  year      = {2018},
  publisher = {Nature Publishing Group}
}

@article{swain2016chemdataextractor,
  title     = {ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature},
  author    = {Swain, Matthew C and Cole, Jacqueline M},
  journal   = {Journal of chemical information and modeling},
  volume    = {56},
  number    = {10},
  pages     = {1894--1904},
  year      = {2016},
  publisher = {ACS Publications}
}


@article{merk2018novo,
  title     = {De novo design of bioactive small molecules by artificial intelligence},
  author    = {Merk, Daniel and Friedrich, Lukas and Grisoni, Francesca and Schneider, Gisbert},
  journal   = {Molecular informatics},
  volume    = {37},
  number    = {1-2},
  pages     = {1700153},
  year      = {2018},
  publisher = {Wiley Online Library}
}


@article{schwaller2020prediction,
  title   = {Prediction of chemical reaction yields using deep learning},
  author  = {Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis},
  journal = {ChemRxiv Preprint},
  url     = {https://doi.org/10.26434/chemrxiv.12758474.v2},
  year    = {2020}
}

@article{schwaller2020predicting,
  title     = {Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy},
  author    = {Schwaller, Philippe and Petraglia, Riccardo and Zullo, Valerio and Nair, Vishnu H and Haeuselmann, Rico Andreas and Pisoni, Riccardo and Bekas, Costas and Iuliano, Anna and Laino, Teodoro},
  journal   = {Chemical Science},
  volume    = {11},
  number    = {12},
  pages     = {3316--3325},
  year      = {2020},
  publisher = {Royal Society of Chemistry}
}

@article{schwaller2019molecular,
  title     = {Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction},
  author    = {Schwaller, Philippe and Laino, Teodoro and Gaudin, Th{\'e}ophile and Bolgar, Peter and Hunter, Christopher A and Bekas, Costas and Lee, Alpha A},
  journal   = {ACS central science},
  volume    = {5},
  number    = {9},
  pages     = {1572--1583},
  year      = {2019},
  publisher = {ACS Publications}
}

@article{vaucher2020automated,
  title     = {Automated extraction of chemical synthesis actions from experimental procedures},
  author    = {Vaucher, Alain C and Zipoli, Federico and Geluykens, Joppe and Nair, Vishnu H and Schwaller, Philippe and Laino, Teodoro},
  journal   = {Nature communications},
  volume    = {11},
  number    = {1},
  pages     = {1--11},
  year      = {2020},
  publisher = {Nature Publishing Group}
}

@article{schwaller2021mapping,
  title     = {Mapping the space of chemical reactions using attention-based neural networks},
  author    = {Schwaller, Philippe and Probst, Daniel and Vaucher, Alain C and Nair, Vishnu H and Kreutter, David and Laino, Teodoro and Reymond, Jean-Louis},
  journal   = {Nature Machine Intelligence},
  pages     = {1--9},
  year      = {2021},
  publisher = {Nature Publishing Group}
}


@article{brown2019guacamol,
  title     = {GuacaMol: benchmarking models for de novo molecular design},
  author    = {Brown, Nathan and Fiscato, Marco and Segler, Marwin HS and Vaucher, Alain C},
  journal   = {Journal of chemical information and modeling},
  volume    = {59},
  number    = {3},
  pages     = {1096--1108},
  year      = {2019},
  publisher = {ACS Publications}
}

@article{shmilovich2020discovery,
  title     = {Discovery of self-assembling $\pi$-conjugated peptides by active learning-directed coarse-grained molecular simulation},
  author    = {Shmilovich, Kirill and Mansbach, Rachael A and Sidky, Hythem and Dunne, Olivia E and Panda, Sayak Subhra and Tovar, John D and Ferguson, Andrew L},
  journal   = {The Journal of Physical Chemistry B},
  volume    = {124},
  number    = {19},
  pages     = {3873--3891},
  year      = {2020},
  publisher = {ACS Publications}
}


@article{wang2019coarse,
  title     = {Coarse-graining auto-encoders for molecular dynamics},
  author    = {Wang, Wujie and G{\'o}mez-Bombarelli, Rafael},
  journal   = {npj Computational Materials},
  volume    = {5},
  number    = {1},
  pages     = {1--9},
  year      = {2019},
  publisher = {Nature Publishing Group}
}

@article{ribeiro2018reweighted,
  title     = {Reweighted autoencoded variational Bayes for enhanced sampling (RAVE)},
  author    = {Ribeiro, Jo{\~a}o Marcelo Lamim and Bravo, Pablo and Wang, Yihang and Tiwary, Pratyush},
  journal   = {The Journal of chemical physics},
  volume    = {149},
  number    = {7},
  pages     = {072301},
  year      = {2018},
  publisher = {AIP Publishing LLC}
}

@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  year    = {2021},
  journal = {arXiv preprint arXiv:2104.09864}
}

@article{tay2021pretrained,
  title   = {Are Pre-trained Convolutions Better than Pre-trained Transformers?},
  author  = {Yi Tay and Mostafa Dehghani and Jai Gupta and Dara Bahri and Vamsi Aribandi and Zhen Qin and Donald Metzler},
  year    = {2021},
  journal = {arXiv preprint arXiv:2105.03322}
}

@inproceedings{cohen2018spherical,
  title     = {Spherical CNNs},
  author    = {Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  booktitle = {International Conference on Learning Representations},
  year      = {2018}
}

@article{finzi2021emlp,
  title   = {A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups},
  author  = {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
  journal = {Arxiv},
  year    = {2021}
}

@article{doshi2017towards,
  title   = {Towards a rigorous science of interpretable machine learning},
  author  = {Doshi-Velez, Finale and Kim, Been},
  journal = {arXiv preprint arXiv:1702.08608},
  year    = {2017}
}


@article{lee2004trust,
  title     = {Trust in automation: Designing for appropriate reliance},
  author    = {Lee, John D and See, Katrina A},
  journal   = {Human factors},
  volume    = {46},
  number    = {1},
  pages     = {50--80},
  year      = {2004},
  publisher = {SAGE Publications Sage UK: London, England}
}

@inproceedings{caruana2015intelligible,
  title        = {{Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission}},
  year         = {2015},
  booktitle    = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  author       = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  pages        = {1721--1730},
  organization = {ACM}
}

@article{Murdoch2019,
  title   = {{Interpretable machine learning: definitions, methods, and applications}},
  year    = {2019},
  journal = {eprint arXiv},
  author  = {Murdoch, James W and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
  pages   = {1--11},
  url     = {http://arxiv.org/abs/1901.04592},
  arxivid = {1901.04592}
}

@article{montavon2018methods,
  title     = {{Methods for interpreting and understanding deep neural networks}},
  year      = {2018},
  journal   = {Digital Signal Processing},
  author    = {Montavon, Grégoire and Samek, Wojciech and M{\"{u}}ller, Klaus-Robert},
  pages     = {1--15},
  volume    = {73},
  publisher = {Elsevier}
}

@inproceedings{ribeiro2016should,
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages     = {1135--1144},
  year      = {2016}
}

@article{ribeiro2016model,
  title   = {Model-agnostic interpretability of machine learning},
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1606.05386},
  year    = {2016}
}

@inproceedings{koh2017understanding,
  title        = {Understanding black-box predictions via influence functions},
  author       = {Koh, Pang Wei and Liang, Percy},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1885--1894},
  year         = {2017},
  organization = {PMLR}
}

@article{wachter2017counterfactual,
  title     = {Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author    = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal   = {Harv. JL \& Tech.},
  volume    = {31},
  pages     = {841},
  year      = {2017},
  publisher = {HeinOnline}
}

@article{goodman2017european,
  title   = {{European Union regulations on algorithmic decision-making and a “right to explanation”}},
  year    = {2017},
  journal = {AI Magazine},
  author  = {Goodman, Bryce and Flaxman, Seth},
  number  = {3},
  pages   = {50--57},
  volume  = {38}
}

@misc{Development2019,
  title     = {{Recommendation of the Council on Artificial Intelligence}},
  year      = {2019},
  author    = {Development, Organisation for Economic Co-operation and},
  pages     = {0449},
  publisher = {Development, Organisation for Economic Co-operation and},
  url       = {https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449}
}

@article{satorras2021en,
  title   = {E(n) Equivariant Normalizing Flows for Molecule Generation in 3D},
  author  = {Victor Garcia Satorras and Emiel Hoogeboom and Fabian B. Fuchs and Ingmar Posner and Max Welling},
  year    = {2021},
  journal = {arXiv preprint arXiv:2105.09016}
}


@article{9369420,
  author  = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  journal = {Proceedings of the IEEE},
  title   = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  year    = {2021},
  volume  = {109},
  number  = {3},
  pages   = {247-278},
  doi     = {10.1109/JPROC.2021.3060483}
}


@book{molnar2019,
  title     = {Interpretable Machine Learning},
  author    = {Christoph Molnar},
  note      = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year      = {2019},
  publisher = {Lulu.com},
  subtitle  = {A Guide for Making Black Box Models Explainable}
}

@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1409.0473},
  year    = {2014}
}

 @inproceedings{pmlr-v70-balduzzi17b,
  title     = {The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author    = {David Balduzzi and Marcus Frean and Lennox Leary and J. P. Lewis and Kurt Wan-Duo Ma and Brian McWilliams},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {342--350},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf},
  url       = { http://proceedings.mlr.press/v70/balduzzi17b.html },
  abstract  = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.}
}

 @article{jang2016categorical,
  title   = {Categorical reparameterization with gumbel-softmax},
  author  = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal = {arXiv preprint arXiv:1611.01144},
  year    = {2016}
}

@inproceedings{sundararajan2017axiomatic,
  title        = {Axiomatic attribution for deep networks},
  author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3319--3328},
  year         = {2017},
  organization = {PMLR}
}

@article{smilkov2017smoothgrad,
  title   = {Smoothgrad: removing noise by adding noise},
  author  = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal = {arXiv preprint arXiv:1706.03825},
  year    = {2017}
}

@inbook{Montavon2019,
  author    = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  editor    = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
  title     = {Layer-Wise Relevance Propagation: An Overview},
  booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  year      = {2019},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {193--209},
  isbn      = {978-3-030-28954-6},
  doi       = {10.1007/978-3-030-28954-6_10},
  url       = {https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10}
}



@article{sturmfels2020visualizing,
  author  = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  title   = {Visualizing the Impact of Feature Attribution Baselines},
  journal = {Distill},
  year    = {2020},
  note    = {https://distill.pub/2020/attribution-baselines},
  doi     = {10.23915/distill.00022}
}

@article{vstrumbelj2014explaining,
  title     = {Explaining prediction models and individual predictions with feature contributions},
  author    = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
  journal   = {Knowledge and information systems},
  volume    = {41},
  number    = {3},
  pages     = {647--665},
  year      = {2014},
  publisher = {Springer}
}

@article{barrett2020investigating,
  author  = {Barrett, Rainier and White, Andrew D.},
  title   = {Investigating Active Learning and Meta-Learning for Iterative Peptide Design},
  journal = {Journal of Chemical Information and Modeling},
  volume  = {61},
  number  = {1},
  pages   = {95-105},
  year    = {2021},
  doi     = {10.1021/acs.jcim.0c00946},
  url     = {
        https://doi.org/10.1021/acs.jcim.0c00946
}
}




@article{numeroso2020explaining,
  title   = {Explaining Deep Graph Networks with Molecular Counterfactuals},
  author  = {Numeroso, Danilo and Bacciu, Davide},
  journal = {arXiv preprint arXiv:2011.05134},
  year    = {2020}
}


@article{agarwal2021towards,
  title   = {Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations},
  author  = {Agarwal, Chirag and Zitnik, Marinka and Lakkaraju, Himabindu},
  journal = {arXiv preprint arXiv:2106.09078},
  year    = {2021}
}


@article{yuan2020explainability,
  title   = {Explainability in graph neural networks: A taxonomic survey},
  author  = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
  journal = {arXiv preprint arXiv:2012.15445},
  year    = {2020}
}


 @article{wellawatte_seshadri_white_2021,
  place     = {Cambridge},
  title     = {Model agnostic generation of counterfactual explanations for molecules},
  doi       = {10.33774/chemrxiv-2021-4qkg8},
  journal   = {ChemRxiv},
  publisher = {Cambridge Open Engage},
  author    = {Wellawatte, Geemi P and Seshadri, Aditi and White, Andrew D},
  year      = {2021}
}


@article{miller2019explanation,
  title     = {Explanation in artificial intelligence: Insights from the social sciences},
  author    = {Miller, Tim},
  journal   = {Artificial intelligence},
  volume    = {267},
  pages     = {1--38},
  year      = {2019},
  publisher = {Elsevier}
}

@article{madsen2021post,
  title   = {Post-hoc Interpretability for Neural NLP: A Survey},
  author  = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  journal = {arXiv preprint arXiv:2108.04840},
  year    = {2021}
}

@article{sanchez-lengeling2021a,
  author  = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alex},
  title   = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year    = {2021},
  note    = {https://distill.pub/2021/gnn-intro},
  doi     = {10.23915/distill.00033}
}
