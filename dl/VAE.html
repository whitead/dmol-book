
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub üìñ" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>14. Variational Autoencoder &#8212; deep learning for molecules &amp; materials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dmol.pub/dl/VAE.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="15. Normalizing Flows" href="flows.html" />
    <link rel="prev" title="13. Deep Learning on Sequences" href="NLP.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">deep learning for molecules & materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   3. Regression &amp; Model Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   4. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/kernel.html">
   5. Kernel Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   6. Deep Learning Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   7. Standard Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gnn.html">
   8. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   9. Input Data &amp; Equivariances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Equivariant.html">
   10. Equivariant Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xai.html">
   11. Explaining Predictions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   12. Attention Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   13. Deep Learning on Sequences
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   14. Variational Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="flows.html">
   15. Normalizing Flows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  D. Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/QM9.html">
   16. Predicting DFT Energies with GNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/MolGenerator.html">
   17. Generative RNN in Browser
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  E. Contributed Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Hyperparameter_tuning.html">
   18. Hyperparameter Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/e3nn_traj.html">
   19. Equivariant Neural Network for Predicting Trajectories
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pretraining.html">
   20. Pretraining
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  F. Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../style.html">
   21. Style Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   22. Changelog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  G. In Progress
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="molnets.html">
   23. Modern Molecular NNs
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <script async defer src="https://api.dmol.pub/latest.js"></script><noscript><img src="https://api.dmol.pub/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript> By <a href="https://twitter.com/andrewwhite01">Andrew White</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/VAE.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/whitead/dmol-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/VAE.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/dl/VAE.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vae-loss-function">
   14.1. VAE Loss function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivation">
     14.1.1. Derivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-approximation">
     14.1.2. Log-Likelihood Approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   14.2. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vae-for-discrete-data">
   14.3. VAE for Discrete Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data">
     14.3.1. The Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-encoder">
     14.3.2. The encoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   14.4. Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluating-the-vae">
     14.4.1. Evaluating the VAE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#re-balancing-vae-reconstruction-and-kl-divergence">
   14.5. Re-balancing VAE Reconstruction and KL-Divergence
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#disentangling-beta-vae">
     14.5.1. Disentangling
     <span class="math notranslate nohighlight">
      \(\beta\)
     </span>
     -VAE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-vae">
   14.6. Regression VAE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bead-spring-polymer-vae">
   14.7. Bead-Spring Polymer VAE
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vae-model">
     14.7.1. VAE Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss">
     14.7.2. Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     14.7.3. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-vae-on-a-trajectory">
   14.8. Using VAE on  a Trajectory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-trajectory">
     14.8.1. Latent Trajectory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-new-samples">
     14.8.2. Generate New Samples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-on-latent-space">
     14.8.3. Optimization on Latent Space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   14.9. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-vae-for-coarse-grained-molecular-simulation">
     14.9.1. Using VAE for Coarse-Grained Molecular Simulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-vae-for-molecular-graph-generation">
     14.9.2. Using VAE for Molecular Graph Generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-of-molecular-graph-generative-models-including-vae">
     14.9.3. Review of Molecular Graph Generative Models (including VAE)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   14.10. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   14.11. Cited References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Variational Autoencoder</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vae-loss-function">
   14.1. VAE Loss function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivation">
     14.1.1. Derivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-approximation">
     14.1.2. Log-Likelihood Approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   14.2. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vae-for-discrete-data">
   14.3. VAE for Discrete Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data">
     14.3.1. The Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-encoder">
     14.3.2. The encoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   14.4. Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluating-the-vae">
     14.4.1. Evaluating the VAE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#re-balancing-vae-reconstruction-and-kl-divergence">
   14.5. Re-balancing VAE Reconstruction and KL-Divergence
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#disentangling-beta-vae">
     14.5.1. Disentangling
     <span class="math notranslate nohighlight">
      \(\beta\)
     </span>
     -VAE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-vae">
   14.6. Regression VAE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bead-spring-polymer-vae">
   14.7. Bead-Spring Polymer VAE
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vae-model">
     14.7.1. VAE Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss">
     14.7.2. Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     14.7.3. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-vae-on-a-trajectory">
   14.8. Using VAE on  a Trajectory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-trajectory">
     14.8.1. Latent Trajectory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-new-samples">
     14.8.2. Generate New Samples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-on-latent-space">
     14.8.3. Optimization on Latent Space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   14.9. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-vae-for-coarse-grained-molecular-simulation">
     14.9.1. Using VAE for Coarse-Grained Molecular Simulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-vae-for-molecular-graph-generation">
     14.9.2. Using VAE for Molecular Graph Generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#review-of-molecular-graph-generative-models-including-vae">
     14.9.3. Review of Molecular Graph Generative Models (including VAE)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   14.10. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   14.11. Cited References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="variational-autoencoder">
<h1><span class="section-number">14. </span>Variational Autoencoder<a class="headerlink" href="#variational-autoencoder" title="Permalink to this headline">#</a></h1>
<p>A variational autoencoder (VAE) is a kind of <strong>generative</strong> deep learning model that is capable of <strong>unsupervised learning</strong> <span id="id1">[<a class="reference internal" href="#id173" title="Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.">KW13</a>]</span>. Unsupervised learning is the process of fitting models to unlabeled data. A generative model is a specific kind of unsupervised learning model that is capable of <em>generating</em> new data points that were not seen in training. Generative models can be viewed as a trained probability distribution over that data: <span class="math notranslate nohighlight">\(\hat{\textrm{P}}(x)\)</span>. You can then draw samples from this distribution. It is generally too difficult to construct <span class="math notranslate nohighlight">\(\hat{\textrm{P}}(x)\)</span> directly, and so most generative models make some changes to the structure.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> and <a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a>. It also assumes a good knowledge of probability theory, including conditional probabilities. You can read <a class="reference external" href="https://raw.githubusercontent.com/whitead/numerical_stats/master/unit_2/lectures/lecture_3.pdf">my notes</a> or any introductory probability text to get an overview. After completing this chapter, you should be able to</p>
<ul class="simple">
<li><p>Understand the derivation for the loss function of a VAE</p></li>
<li><p>Construct an encoder/decoder pair in JAX and train it with the VAE loss function</p></li>
<li><p>Sample from the decoder</p></li>
<li><p>Rebalance VAE loss for reconstruction or disentangling</p></li>
</ul>
</div>
<p>A VAE approaches this problem by introducing a dummy random variable <span class="math notranslate nohighlight">\(z\)</span>, which we define to have a known distribution (e.g., normal). We can then rewrite <span class="math notranslate nohighlight">\(\hat{\textrm{P}}(x)\)</span> as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4683ad4b-1f9a-436b-9b09-d157c54bb04d">
<span class="eqno">(14.1)<a class="headerlink" href="#equation-4683ad4b-1f9a-436b-9b09-d157c54bb04d" title="Permalink to this equation">#</a></span>\[\begin{equation}
\hat{\textrm{P}}(x) = \int\,\hat{\textrm{P}}\left(x | z \right) \textrm{P}(z)\,dz
\end{equation}\]</div>
<p>using the definition of a marginal and conditional probability. Training <span class="math notranslate nohighlight">\(\hat{\textrm{P}}\left(x | z \right)\)</span> directly is not really possible either, but we can create a symmetric distribution <span class="math notranslate nohighlight">\(\hat{\textrm{P}}\left(z | x \right)\)</span> and train both simultaneously. This symmetric distribution only is created to help us train; our end goal is to find <span class="math notranslate nohighlight">\(\hat{\textrm{P}}\left(x | z \right)\)</span> so that we can obtain <span class="math notranslate nohighlight">\(\hat{\textrm{P}}(x)\)</span>. VAEs were first introduced in <span id="id2">[<a class="reference internal" href="#id173" title="Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.">KW13</a>]</span>.</p>
<p>A VAE is thus a set of two trained conditional probability distributions that operate on the data <span class="math notranslate nohighlight">\(x\)</span> and latent variables <span class="math notranslate nohighlight">\(z\)</span>. The first conditional is <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> indicates the trainable parameters that we will be fitting. <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span> is known as the ‚Äúdecoder‚Äù because it goes from the latent variable <span class="math notranslate nohighlight">\(z\)</span> to <span class="math notranslate nohighlight">\(x\)</span>. The decoder analogy is because you can view <span class="math notranslate nohighlight">\(z\)</span> as a kind of encoded compression of <span class="math notranslate nohighlight">\(x\)</span>. The other conditional is <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> and is known as the encoder.</p>
<p>Remember we always know <span class="math notranslate nohighlight">\(p(z)\)</span> because we chose it to be a defined distribution ‚Äî that is the key idea. We‚Äôre grounding our encoder/decoder by having them communicate through <span class="math notranslate nohighlight">\(p(z)\)</span>, which we know. For the rest of this chapter we‚Äôll take <span class="math notranslate nohighlight">\(p(z)\)</span> to be a <strong>standard normal distribution</strong>. <span class="math notranslate nohighlight">\(p(z)\)</span> can be other distributions though. It can even be trained using the techniques from the <a class="reference internal" href="flows.html"><span class="doc">Normalizing Flows</span></a>.</p>
<section id="vae-loss-function">
<h2><span class="section-number">14.1. </span>VAE Loss function<a class="headerlink" href="#vae-loss-function" title="Permalink to this headline">#</a></h2>
<p>To see how <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> enables us to train, let‚Äôs construct our loss. The loss function should only take in a value <span class="math notranslate nohighlight">\(x_i\)</span> and trainable parameters. There are no labels. Our goal is to make our VAE model be able to generate <span class="math notranslate nohighlight">\(x_i\)</span>, so the loss is the log likelihood that we saw <span class="math notranslate nohighlight">\(x_i\)</span>: <span class="math notranslate nohighlight">\(\log\left[\hat{\textrm{P}}(x_i)\right]\)</span>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Log likelihood is the loss of choice for fitting distributions to data. It is a likelihood, not a probability, because the distribution parameters are changing, not the random variables (which are set to be the data). We take a log so that we can sum/average over data to aggregate multiple points due to properties of logs.</p>
</aside>
<section id="derivation">
<h3><span class="section-number">14.1.1. </span>Derivation<a class="headerlink" href="#derivation" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The derivation below is a little unusual. Most derivations rely on Bayes‚Äô theorem following a principle of evidence lower bound (ELBO). I thought I‚Äôd give a different derivation since you can readily find examples of <a class="reference external" href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">the ELBO in many places</a>.</p>
</div>
<p>Remember we do not have an expression for <span class="math notranslate nohighlight">\(\hat{\textrm{P}}(x_i)\)</span>. We have <span class="math notranslate nohighlight">\(p_\theta(x_i | z)\)</span>. To connect them we‚Äôll use the following expression:</p>
<div class="amsmath math notranslate nohighlight" id="equation-56c506c1-c558-4395-ae4b-8fd365fb5a86">
<span class="eqno">(14.2)<a class="headerlink" href="#equation-56c506c1-c558-4395-ae4b-8fd365fb5a86" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log\left[\hat{\textrm{P}}(x_i)\right]= \log\left[\int\,p_\theta(x_i | z) \textrm{P}(z)\,dz\right] = \log \textrm{E}_z\left[p_\theta(x_i | z)\right]
\end{equation}\]</div>
<p>where we have rewritten the integral more compactly by using the definition of expectation. This expression requires integrating over the latent variable, which is not easy since as you can guess <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span> is a neural network and it‚Äôs not straightforward to integrate over the input (<span class="math notranslate nohighlight">\(z\)</span>) of a neural network. Instead, we can approximate this integral by sampling some <span class="math notranslate nohighlight">\(z\)</span>s from <span class="math notranslate nohighlight">\(P(z)\)</span></p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>We actually could just integrate over the latent variables. This is called a normalizing flow and is a class of generative models we‚Äôll see later.</p>
</aside>
<div class="amsmath math notranslate nohighlight" id="equation-b703c804-a4e5-41e8-9c36-ed108cd232e0">
<span class="eqno">(14.3)<a class="headerlink" href="#equation-b703c804-a4e5-41e8-9c36-ed108cd232e0" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \log\textrm{E}_z\left[ p_\theta(x_i | z)\right]\approx \log \left[\frac{1}{N}\sum_j^N  p_\theta(x_i | z_j)\right],\, z_j \sim P(z_j)
\end{equation}\]</div>
<p>You‚Äôll find though that grabbing <span class="math notranslate nohighlight">\(z\)</span>‚Äôs from <span class="math notranslate nohighlight">\(P(z)\)</span> is not so efficient at approximating this integral, because you need the <span class="math notranslate nohighlight">\(z\)</span>‚Äôs to be likely to have led to the observed <span class="math notranslate nohighlight">\(x_i\)</span>. The integral is dominated by the <span class="math notranslate nohighlight">\(p_\theta(x_i | z_j)\)</span> terms. This is where we use <span class="math notranslate nohighlight">\(q(z | x)\)</span>: it can provide efficient guesses for <span class="math notranslate nohighlight">\(z_j\)</span>. To approximate <span class="math notranslate nohighlight">\(\log \textrm{E}_z\left[p_\theta(x_i | z)\right]\)</span> with samples from <span class="math notranslate nohighlight">\(q(z | x_i)\)</span>, we need to account for the fact that sampling from <span class="math notranslate nohighlight">\(q(z | x_i)\)</span> is not identical to sampling from <span class="math notranslate nohighlight">\(P(z)\)</span> by adding their ratio to the expression (<a class="reference external" href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>).</p>
<div class="amsmath math notranslate nohighlight" id="equation-1c73e675-7239-46b2-b49e-039b3a87177a">
<span class="eqno">(14.4)<a class="headerlink" href="#equation-1c73e675-7239-46b2-b49e-039b3a87177a" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \log\textrm{E}_z\left[ p_\theta(x_i | z)\right]\approx \log \left[\frac{1}{N}\sum^N_j  p_\theta(x_i | z_j) \frac{P(z_j)}{q_\phi(z_j | x_i)}\right],\, z_j \sim q_\phi(z_j | x_i)
\end{equation}\]</div>
<p>The ratio of <span class="math notranslate nohighlight">\(P(z) / q_\phi(z | x)\)</span> enables our numerical approximation of the expectation. For notational purposes though I‚Äôll go back to the exact expression, with the understanding that when we go to implementation we‚Äôll use the numerical approximation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e2aa395-a16a-4f92-bdc3-8d1ae2f475ee">
<span class="eqno">(14.5)<a class="headerlink" href="#equation-8e2aa395-a16a-4f92-bdc3-8d1ae2f475ee" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \log\textrm{E}_z\left[ p_\theta(x_i | z)\right] = \log\textrm{E}_{z \sim q_\phi(z | x_i)}\left[ p_\theta(x_i | z) \frac{P(z)}{q_\phi(z | x_i)}\right]
\end{equation}\]</div>
<p>Notice how the expectation now is wrt <span class="math notranslate nohighlight">\(z \sim q_\phi(z | x_i)\)</span> since we have that importance sampling ratio in the expression.</p>
<p>Now if the log was on the inside of our expectation, we could simplify this. We can actually swap the order of expectation and the log using Jensen‚Äôs Inequality for the concave log function. The consequence is that our loss is no longer an exact estimate of the log likelihood, but a lower bound.</p>
<div class="amsmath math notranslate nohighlight" id="equation-304891ae-0f14-4211-9deb-0ccf2452fedf">
<span class="eqno">(14.6)<a class="headerlink" href="#equation-304891ae-0f14-4211-9deb-0ccf2452fedf" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log \textrm{E}\left[\ldots\right]\geq \textrm{E}\left[\log \ldots\right]
\end{equation}\]</div>
<p>We‚Äôll use that and can now separate into two terms by properties of the log</p>
<div class="amsmath math notranslate nohighlight" id="equation-4bff53f0-ae18-4dc9-b25a-acffa8ff8434">
<span class="eqno">(14.7)<a class="headerlink" href="#equation-4bff53f0-ae18-4dc9-b25a-acffa8ff8434" title="Permalink to this equation">#</a></span>\[\begin{equation}
\textrm{E}_{z \sim q_\phi(z | x_i)}\left[ \log\left(p_\theta(x_i | z) \frac{P(z)}{q_\phi(z | x_i)}\right)\right] = \textrm{E}_{z \sim q_\phi(z | x_i)}\left[ \log p_\theta(x_i | z)\right] + \textrm{E}_{z \sim q_\phi(z | x_i)}\left[ \log \left(\frac{P(z)}{q_\phi(z | x_i)}\right)\right]
\end{equation}\]</div>
<p>Remember we always planned to re-introduce numerically approximate the expectation. However, the right-hand side does not involve <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span>, so we do not need to integrate over a neural network input. We just need to integrate over the output of <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> and <span class="math notranslate nohighlight">\(P(z)\)</span>, which is a standard normal distribution. We‚Äôll see later on that we can make the output of <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> specifically be a normal distribution to make sure we can easily compute the integral. Finally, we can use an identity that relates the Kullback‚ÄìLeibler divergence (KL divergence) (a binary functional of two probabilities) to the right-hand side term:</p>
<div class="amsmath math notranslate nohighlight" id="equation-859e65c9-dd60-47eb-ab1c-08c9db3730ab">
<span class="eqno">(14.8)<a class="headerlink" href="#equation-859e65c9-dd60-47eb-ab1c-08c9db3730ab" title="Permalink to this equation">#</a></span>\[\begin{equation}
\textrm{E}_{p(x)}\left[ \ln\left(\frac{q(x)}{p(x)}\right)\right] = -\textrm{KL}\left[p(x)|| q(x)\right]
\end{equation}\]</div>
<p>arriving at our final result:</p>
</section>
<section id="log-likelihood-approximation">
<h3><span class="section-number">14.1.2. </span>Log-Likelihood Approximation<a class="headerlink" href="#log-likelihood-approximation" title="Permalink to this headline">#</a></h3>
<div class="amsmath math notranslate nohighlight" id="equation-76d7edc2-f229-4437-a805-23c03c3c57ea">
<span class="eqno">(14.9)<a class="headerlink" href="#equation-76d7edc2-f229-4437-a805-23c03c3c57ea" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log\left[\hat{\textrm{P}}(x_i)\right] \geq \textrm{E}_{z \sim q_\phi(z | x_i)}\left[ \log p_\theta(x_i | z)\right] -\textrm{KL}\left[q_\phi(z | x_i)|| P(z)\right]
\end{equation}\]</div>
<p>The left term is called the <strong>reconstruction loss</strong> and assess how close we come after going from <span class="math notranslate nohighlight">\(x \rightarrow z \rightarrow x\)</span> in expectation. The right-hand term is the <strong>KL-divergence</strong> and measures how close our encoder is to our defined <span class="math notranslate nohighlight">\(P(z)\)</span> (normal distribution). The right-hand term involves an integral that can be computed analytically and no sampling is required to estimate it. Remember, in the derivation the KL-divergence term appeared as a correction term to account for the fact that our loss doesn‚Äôt use <span class="math notranslate nohighlight">\(P(z)\)</span> directly, but instead uses the encoder <span class="math notranslate nohighlight">\(q_\phi(z | x_i)\)</span> which generates <span class="math notranslate nohighlight">\(z\)</span>‚Äôs from our training data point <span class="math notranslate nohighlight">\(x_i\)</span>. The last step is that we want to minimize our loss, so we need to add a minus sign.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The log-likelihood equation we‚Äôve derived for VAE training is also sometimes called the evidence lower bound (ELBO). ELBO is a general equation used in Bayesian modeling, which usually has nothing to do with VAEs.</p>
</div>
<div class="amsmath math notranslate nohighlight" id="equation-5d112568-a3e0-419b-a2d8-291b8d84fef3">
<span class="eqno">(14.10)<a class="headerlink" href="#equation-5d112568-a3e0-419b-a2d8-291b8d84fef3" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathcal{L}(x_i, \phi, \theta) =  -\textrm{E}_{z \sim q_\phi(z | x_i)}\left[ \log p_\theta(x_i | z)\right] +\textrm{KL}\left[q_\phi(z | x_i)|| P(z)\right]
\end{equation}\]</div>
<p>Remember that in practice, we will approximate the expectation in the reconstruction loss by sampling <span class="math notranslate nohighlight">\(z\)</span>‚Äôs from the decoder <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span>. We‚Äôll only use a single sample.</p>
</section>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">14.2. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Permalink to this headline">#</a></h2>
<p>Click the ¬†<i aria-label="Launch interactive content" class="fas fa-rocket"></i>¬† above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
</section>
<section id="vae-for-discrete-data">
<h2><span class="section-number">14.3. </span>VAE for Discrete Data<a class="headerlink" href="#vae-for-discrete-data" title="Permalink to this headline">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The features are classes; we <em>are not</em> trying to make a classifier that takes in features and outputs classes. VAEs are for unlabeled data.</p>
</aside>
<p>Our first example will be to generate new example classes from a distribution of possible classes. An application for this might be to sample conditions of an experiment. Our features <span class="math notranslate nohighlight">\(x\)</span> are one-hot vectors indicating class and our goal is to learn the distribution <span class="math notranslate nohighlight">\(P(x)\)</span> so that we can sample new <span class="math notranslate nohighlight">\(x\)</span>‚Äôs. Learning the latent space can also provide a way to embed your features into low dimensional continuous vectors, allowing you to do things like optimization because you‚Äôve moved from discrete classes to continuous vectors. That is an extra benefit, our loss and training goal are to create a new <span class="math notranslate nohighlight">\(P(x)\)</span>.</p>
<p>Let‚Äôs think for a moment about our encoder and decoder. <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span>, the encoder, should give out a <em>probability distribution</em> for vectors of real numbers <span class="math notranslate nohighlight">\(z\)</span> and take an input of a one-hot vector <span class="math notranslate nohighlight">\(x\)</span>. This sounds difficult; we‚Äôve never seen a neural network output a probability distribution over real number vectors. We can simplify though. We defined <span class="math notranslate nohighlight">\(P(z)\)</span> to be normally distributed, let‚Äôs assume that the form of <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> should be normal. Then our neural network could output the parameters to a normal distribution (mean/variance) for <span class="math notranslate nohighlight">\(z\)</span>, rather than trying to output a probability at every possible <span class="math notranslate nohighlight">\(z\)</span> value. It‚Äôs up to you if you want to have <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> output a D-dimensional Gaussian distribution with a covariance matrix or just output D independent normal distributions. Having <span class="math notranslate nohighlight">\(q_\phi(z | x)\)</span> output a normal distribution also allows us to analytically simplify the expectation/integral in the KL-divergence term.</p>
<p>The decoder <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span> should output a probability distribution over classes given a real vector <span class="math notranslate nohighlight">\(z\)</span>. We can use the same form we used for classification: softmax activation. Just remember that we‚Äôre not trying to output a specific <span class="math notranslate nohighlight">\(x\)</span>, just a probability distribution of <span class="math notranslate nohighlight">\(x\)</span>‚Äôs.</p>
<p>Choices we have to make are the hyperparameters of the encoder and decoder and the size of <span class="math notranslate nohighlight">\(z\)</span>. It makes sense to have the encoder and decoder share as many hyperparameters as possible, since they‚Äôre somewhat symmetric. Just remember that the encoder in our example is outputting a mean and variance, which means using regression, and the decoder is outputting a normalized probability vector, which means using softmax. Let‚Äôs get started!</p>
<section id="the-data">
<h3><span class="section-number">14.3.1. </span>The Data<a class="headerlink" href="#the-data" title="Permalink to this headline">#</a></h3>
<p>The data is 1024 points <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> where each <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> is a 32 dimensional one-hot vector indicating class. We won‚Äôt define the classes ‚Äì the data is synthetic. Since a VAE is unsupervised learning, there are no labels. Let‚Äôs start by examining the data. We‚Äôll sum the occurrences of each class to see what the distribution of classes looks like. <em>The hidden cells show how the data was generated</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sampled_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">((</span><span class="n">sampled_z</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">sampled_z</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">loc</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span> <span class="o">+</span> <span class="n">sampled_z</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sampled_z</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">nbins</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">_</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">nbins</span><span class="p">)</span>
<span class="n">class_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">nclasses</span> <span class="o">=</span> <span class="n">nbins</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nclasses</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_9_0.png" src="../_images/VAE_9_0.png" />
</div>
</div>
</section>
<section id="the-encoder">
<h3><span class="section-number">14.3.2. </span>The encoder<a class="headerlink" href="#the-encoder" title="Permalink to this headline">#</a></h3>
<p>Our encoder will be a basic two hidden layer network. We will output a <span class="math notranslate nohighlight">\(D\times2\)</span> matrix, where the first column is means and the second is standard deviations for independent normal distributions that make up our guess for <span class="math notranslate nohighlight">\(q(z | x)\)</span>. Outputting a mean is simple, just use no activation. Outputting a standard deviation is unusual because they should be on <span class="math notranslate nohighlight">\((0, \infty)\)</span>. <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softplus.html#jax.nn.softplus" title="(in JAX)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jax.nn.softplus</span></code></a> can accomplish this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax.example_libraries</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">functools</span>


<span class="k">def</span> <span class="nf">random_vec</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="n">nclasses</span>


<span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The encoder takes as input x and gives out probability of z,</span>
<span class="sd">    expressed as normal distribution parameters. Assuming each z dim is independent,</span>
<span class="sd">    output |z| x 2 matrix&quot;&quot;&quot;</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w1</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w2</span> <span class="o">@</span> <span class="n">hx</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">w3</span> <span class="o">@</span> <span class="n">hx</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="c1"># slice out stddeviation and make it positive</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="c1"># we slice with &#39;:&#39; to keep rank same</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">reshaped</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">reshaped</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create inital theta parameters&quot;&quot;&quot;</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="c1"># need to params per dim (mean, std)</span>
    <span class="n">w3</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span><span class="p">]</span>


<span class="c1"># test them</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">encoder</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[0.67200387, 0.00897247]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>The decoder should output a vector of probabilities for <span class="math notranslate nohighlight">\(\vec{x}\)</span>. This can be achieved by just adding a softmax to the output. The rest is nearly identical to the encoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;decoder takes as input the latant variable z and gives out probability of x.</span>
<span class="sd">    Decoder outputes a real number, then we use softmax activation to get probability across</span>
<span class="sd">    possible values of x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">phi</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w1</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w2</span> <span class="o">@</span> <span class="n">hz</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">w3</span> <span class="o">@</span> <span class="n">hz</span> <span class="o">+</span> <span class="n">b3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create inital phi parameters&quot;&quot;&quot;</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">w3</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span><span class="p">]</span>


<span class="c1"># test it out</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">decoder</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.2</span><span class="p">]</span> <span class="o">*</span> <span class="n">latent_dim</span><span class="p">),</span> <span class="n">phi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([1.4369683e-11, 2.8801292e-29, 3.2273541e-20, 7.1850895e-22,
       1.0793007e-22, 2.8247908e-20, 8.5903684e-09, 5.0419994e-28,
       3.8993281e-25, 1.9217204e-23, 1.3062071e-12, 2.6221546e-16,
       4.2119552e-23, 1.1967079e-20, 4.3358453e-27, 3.8699083e-20,
       1.3168897e-22, 3.3939088e-20, 5.1175348e-27, 3.9091000e-24,
       1.0000000e+00, 1.6622006e-19, 2.5878642e-29, 3.6575650e-17,
       5.0655268e-25, 3.9531148e-23, 5.9112239e-20, 3.3607102e-19,
       6.1983621e-12, 2.7988031e-19, 9.9489904e-13, 3.6622517e-27],      dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training">
<h2><span class="section-number">14.4. </span>Training<a class="headerlink" href="#training" title="Permalink to this headline">#</a></h2>
<p>We use ELBO equation for training:</p>
<div class="math notranslate nohighlight">
\[
l = -\textrm{E}_{z \sim q_\phi(z | x_i)}\left[\log p_{\theta}(x_i | z)\right] + \textrm{KL}\left[(q_\phi(z | x))|| P(z)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(P(z)\)</span> is the standard normal distribution and we approximate expectations using a single sample from the encoder. We need to expand the KL-divergence term to implement. Both <span class="math notranslate nohighlight">\(P(z)\)</span> and <span class="math notranslate nohighlight">\(q_\theta(z | x)\)</span> are normal. You can look-up the KL-divergence between two normal distributions:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8a60e419-43bc-462e-8665-f633eb59dd51">
<span class="eqno">(14.11)<a class="headerlink" href="#equation-8a60e419-43bc-462e-8665-f633eb59dd51" title="Permalink to this equation">#</a></span>\[\begin{equation}
KL(q, p) = \log \frac{\sigma_p}{\sigma_q} + \frac{\sigma_q^2 + (\mu_q - \mu_p)^2}{2 \sigma_p^2} - \frac{1}{2}
\end{equation}\]</div>
<p>we can simplify because <span class="math notranslate nohighlight">\(P(z)\)</span> is standard normal (<span class="math notranslate nohighlight">\(\sigma = 1, \mu = 0\)</span>)</p>
<div class="amsmath math notranslate nohighlight" id="equation-4f9075ba-c573-460c-80da-063d44546dc9">
<span class="eqno">(14.12)<a class="headerlink" href="#equation-4f9075ba-c573-460c-80da-063d44546dc9" title="Permalink to this equation">#</a></span>\[\begin{equation}
\textrm{KL}\left[(q_\theta(z | x_i))|| P(z)\right] = -\log \sigma_i + \frac{\sigma_i^2}{2} + \frac{\mu_i^2}{2} - \frac{1}{2}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_i, \sigma_i\)</span> are the output from <span class="math notranslate nohighlight">\(q_\phi(z | x_i)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VAE Loss&quot;&quot;&quot;</span>
    <span class="c1"># reconstruction loss</span>
    <span class="n">sampled_z_params</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
    <span class="c1"># reparameterization trick</span>
    <span class="c1"># we use standard normal sample and multiply by parameters</span>
    <span class="c1"># to ensure derivatives correctly propogate to encoder</span>
    <span class="n">sampled_z</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># log of prob</span>
    <span class="n">rloss</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">decoder</span><span class="p">(</span><span class="n">sampled_z</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="c1"># LK loss</span>
    <span class="n">klloss</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span>
        <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="c1"># combined</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rloss</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">klloss</span><span class="p">)])</span>


<span class="c1"># test it out</span>
<span class="n">loss</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([18.420681,  4.439429], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Our loss works! Now we need to make it batched so we can train in batches. Luckily this is easy with <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap" title="(in JAX)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vmap</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batched_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_decoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_encoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># test batched loss</span>
<span class="n">batched_loss</span><span class="p">(</span><span class="n">class_data</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[18.420681 ,  4.4394283],
       [18.420681 , 32.165703 ],
       [ 0.1743061, 73.97651  ],
       [18.420681 ,  4.4394283]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>We‚Äôll make our gradient take the average over the batch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">batched_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">)),</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">fast_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="n">fast_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">batched_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Alright, great! An important detail we‚Äôve skipped so far is that when using <code class="docutils literal notranslate"><span class="pre">jax</span></code> to generate random numbers, we must step our random number generator forward. You can do that using <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax.random.split" title="(in JAX)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jax.random.split</span></code></a>. Otherwise, you‚Äôll get the same random numbers at each draw.</p>
<p>We‚Äôre going to use a <code class="docutils literal notranslate"><span class="pre">jax</span></code> optimizer here. This is to simplify parameter updates. We have a lot of parameters and they are nested, which will be complex for treating with python for loops.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">phi0</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">((</span><span class="n">theta0</span><span class="p">,</span> <span class="n">phi0</span><span class="p">))</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">bi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="c1"># make a batch into shape B x 1</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">class_data</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)]</span>
        <span class="c1"># udpate random number key</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># get current parameter values from optimizer</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">last_state</span> <span class="o">=</span> <span class="n">opt_state</span>
        <span class="c1"># compute gradient and update</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">fast_grad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">bi</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">lvalue</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fast_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">subkey</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lvalue</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Reconstruction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;KL&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_23_0.png" src="../_images/VAE_23_0.png" />
</div>
</div>
<section id="evaluating-the-vae">
<h3><span class="section-number">14.4.1. </span>Evaluating the VAE<a class="headerlink" href="#evaluating-the-vae" title="Permalink to this headline">#</a></h3>
<p>Remember our goal with the VAE is to reproduce <span class="math notranslate nohighlight">\(P(x)\)</span>. We can sample from our VAE using the chosen <span class="math notranslate nohighlight">\(P(z)\)</span> and our decoder. Let‚Äôs compare that distribution with our training distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">sampled_x</span> <span class="o">=</span> <span class="n">batched_decoder</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nbins</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;VAE Samples&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nbins</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampled_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_25_0.png" src="../_images/VAE_25_0.png" />
</div>
</div>
<p>It appears we have succeeded! There were two more goals of the VAE model: making the encoder give output similar to <span class="math notranslate nohighlight">\(P(z)\)</span> and be able to reconstruct. These goals are often opposed and they represent the two terms in the loss: reconstruction and KL-divergence. Let‚Äôs examine the KL-divergence term, which causes the encoder to give output similar to a standard normal. We‚Äôll sample from our training data in histogram look at the resulting average mean and std dev.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">batched_encoder</span><span class="p">(</span><span class="n">class_data</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average mu = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;Average std dev = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average mu =  0.00038799975 Average std dev =  0.9998034
</pre></div>
</div>
</div>
</div>
<p>Wow! Very close to a standard normal. So our model satisfied the match between the decoder and the <span class="math notranslate nohighlight">\(P(z)\)</span>. The last thing to check is reconstruction. These are distributions, so I‚Äôll only look at the maximum <span class="math notranslate nohighlight">\(z\)</span> value to do the reconstruction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">theta</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;P(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_29_0.png" src="../_images/VAE_29_0.png" />
</div>
</div>
<p>The reconstruction is not great, it puts a lot of probability mass on other points. In fact, the reconstruction seems to not use the encoder‚Äôs information at all ‚Äì it looks like <span class="math notranslate nohighlight">\(P(x)\)</span>. The reason for this is that our KL-divergence term dominates. It has a very good fit.</p>
</section>
</section>
<section id="re-balancing-vae-reconstruction-and-kl-divergence">
<h2><span class="section-number">14.5. </span>Re-balancing VAE Reconstruction and KL-Divergence<a class="headerlink" href="#re-balancing-vae-reconstruction-and-kl-divergence" title="Permalink to this headline">#</a></h2>
<p>Often we desire more reconstruction at the cost of making the latent space less normal. This can be done by adding a term that adjusts the balance between the reconstruction loss and the KL-divergence. You would choose to do this if you want to use the latent space for something and are not just interested in creating a model <span class="math notranslate nohighlight">\(\hat{P}(x)\)</span>. Here is the modified ELBO equation for training:</p>
<div class="math notranslate nohighlight">
\[
l = -\textrm{E}_{z \sim q_\phi(z | x_i)}\left[\log p_{\theta}(x_i | z)\right] + \beta\cdot\textrm{KL}\left[(q_\phi(z | x))|| P(z)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta &gt; 1\)</span> emphasizes the encoder distribution matching chosen latent distribution (standard normal) and <span class="math notranslate nohighlight">\(\beta &lt; 1\)</span> emphasizes reconstruction accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modified_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This loss allows you to vary which term is more important</span>
<span class="sd">    with beta. Beta = 0 - all reconstruction, beta = 1 - ELBO&quot;&quot;&quot;</span>
    <span class="n">bl</span> <span class="o">=</span> <span class="n">batched_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">bl</span> <span class="o">@</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>


<span class="n">new_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">modified_loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">fast_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">new_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># note we used a lower step size for this loss</span>
<span class="c1"># and more epochs</span>
<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">phi0</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">((</span><span class="n">theta0</span><span class="p">,</span> <span class="n">phi0</span><span class="p">))</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">bi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="c1"># make a batch into shape B x 1</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">class_data</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)]</span>
        <span class="c1"># udpate random number key</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># get current parameter values from optimizer</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">last_state</span> <span class="o">=</span> <span class="n">opt_state</span>
        <span class="c1"># compute gradient and update</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">fast_grad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">bi</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">lvalue</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fast_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">subkey</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lvalue</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Reconstruction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;KL&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_33_0.png" src="../_images/VAE_33_0.png" />
</div>
</div>
<p>You can see the error is higher, but let‚Äôs see how it did at our three metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">sampled_x</span> <span class="o">=</span> <span class="n">batched_decoder</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nbins</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;VAE Samples&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nbins</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampled_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_35_0.png" src="../_images/VAE_35_0.png" />
</div>
</div>
<p>A little bit worse on <span class="math notranslate nohighlight">\(P(x)\)</span>, but overall not bad. What about our goal, the reconstruction?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">theta</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;P(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">class_data</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_37_0.png" src="../_images/VAE_37_0.png" />
</div>
</div>
<p>What about our encoder‚Äôs agreement with a standard normal?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">batched_encoder</span><span class="p">(</span><span class="n">class_data</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average mu = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;Average std dev = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average mu =  0.17831202 Average std dev =  0.11638321
</pre></div>
</div>
</div>
</div>
<p>The standard deviation is much smaller! So we squeezed our latent space a little at the cost of better reconstruction.</p>
<section id="disentangling-beta-vae">
<h3><span class="section-number">14.5.1. </span>Disentangling <span class="math notranslate nohighlight">\(\beta\)</span>-VAE<a class="headerlink" href="#disentangling-beta-vae" title="Permalink to this headline">#</a></h3>
<p>You can adjust <span class="math notranslate nohighlight">\(\beta\)</span> the opposite direction, to value matching the prior Gaussian distribution more strongly. This can better condition the encoder so that each of the latent dimensions are truly independent. This can be important if you want to disengatngle your input features to arrive at an orthogonal projection. This of course comes at the loss of reconstruction accuracy, but can be more important if you‚Äôre interested in the latent space rather than generating new samples <span id="id3">[<a class="reference internal" href="#id79" title="Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in variational autoencoders. In International Conference on Machine Learning, 4402‚Äì4412. PMLR, 2019.">MRST19</a>]</span>.</p>
</section>
</section>
<section id="regression-vae">
<h2><span class="section-number">14.6. </span>Regression VAE<a class="headerlink" href="#regression-vae" title="Permalink to this headline">#</a></h2>
<p>We‚Äôll now work with continuous features <span class="math notranslate nohighlight">\(x\)</span>. We need to make a few key changes. The encoder will remain the same, but the decoder now must output a <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span> that gives a probability to all possible <span class="math notranslate nohighlight">\(x\)</span> values. Above, we only had a finite number of classes but now any <span class="math notranslate nohighlight">\(x\)</span> is possible. As we did for the encoder, we‚Äôll assume that <span class="math notranslate nohighlight">\(p_\theta(x | z)\)</span> should be normal and we‚Äôll output the parameters of the normal distribution from our network. This requires an update to the reconstruction loss to be a log of a normal, but otherwise things will be identical.</p>
<p>One of the mistakes I always make is that the log-likelihood for a normal distribution with a single observation cannot have unknown standard deviation. Our new normal distribution parameters for the decoder will have a single observation for a single <span class="math notranslate nohighlight">\(x\)</span> in training. If you make the standard deviation trainable, it will just pick infinity as the standard deviation since that will for sure capture the point and you only have one point. Thus, I‚Äôll make the decoder standard deviation be a hyperparameter fixed at 0.1. We don‚Äôt see this issue with the encoder, which also outputs a normal distribution, because we training the encoder with the KL-divergence term and not likelihood of observations (reconstruction loss).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># make encoder parameters</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="c1"># test it</span>
<span class="n">encoder</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[-0.48632216,  0.6864413 ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;decoder takes as input the latant variable z and gives out probability of x.</span>
<span class="sd">    Decoder outputes parameters for a normal distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">phi</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w1</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w2</span> <span class="o">@</span> <span class="n">hz</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">w3</span> <span class="o">@</span> <span class="n">hz</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="c1"># slice out stddeviation and make it positive</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="c1"># we slice with &#39;:&#39; to keep rank same</span>
    <span class="c1"># std = jax.nn.softplus(reshaped[:,1:])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">reshaped</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">reshaped</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create inital phi parameters&quot;&quot;&quot;</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">w3</span> <span class="o">=</span> <span class="n">random_vec</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span><span class="p">]</span>


<span class="c1"># test it out</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">decoder</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.2</span><span class="p">]</span> <span class="o">*</span> <span class="n">latent_dim</span><span class="p">),</span> <span class="n">phi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[8.568987, 0.1     ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VAE Loss&quot;&quot;&quot;</span>
    <span class="c1"># reconstruction loss</span>
    <span class="n">sampled_z_params</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
    <span class="c1"># reparameterization trick</span>
    <span class="c1"># we use standard normal sample and multiply by parameters</span>
    <span class="c1"># to ensure derivatives correctly propogate to encoder</span>
    <span class="n">sampled_z</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># log of normal dist</span>
    <span class="n">out_params</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">sampled_z</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
    <span class="n">rloss</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">out_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">out_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="n">klloss</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span>
        <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="c1"># combined</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rloss</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">klloss</span><span class="p">)])</span>


<span class="c1"># test it out</span>
<span class="n">loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># update compiled functions</span>
<span class="n">batched_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_decoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_encoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">batched_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">)),</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">fast_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="n">fast_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">batched_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">phi0</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">((</span><span class="n">theta0</span><span class="p">,</span> <span class="n">phi0</span><span class="p">))</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">bi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="c1"># make a batch into shape B x 1</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># udpate random number key</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># get current parameter values from optimizer</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">last_state</span> <span class="o">=</span> <span class="n">opt_state</span>
        <span class="c1"># compute gradient and update</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">fast_grad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">bi</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">lvalue</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fast_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">subkey</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lvalue</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Reconstruction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;KL&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_46_0.png" src="../_images/VAE_46_0.png" />
</div>
</div>
<p>This model still has training to be done, but hopefully you get the idea for working with continuous numbers! We can examine the final result below. Note that I must sample from the output parameters to compare with the real training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bins</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">sampled_x_params</span> <span class="o">=</span> <span class="n">batched_decoder</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;VAE Samples&quot;</span><span class="p">)</span>
<span class="c1"># Now we have to sample from output paramters!!</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sampled_x_params</span><span class="p">:</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="n">s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">)))</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="c1"># make them use same bins</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_48_0.png" src="../_images/VAE_48_0.png" />
</div>
</div>
<p>The distribution is alright, not great. Comparing reconstruction is a little different because we only compare the mean of the predicted <span class="math notranslate nohighlight">\(P(x)\)</span>. We‚Äôll plot our predicted <span class="math notranslate nohighlight">\(\mu\)</span> from the decoder against the real <span class="math notranslate nohighlight">\(x\)</span> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mus</span> <span class="o">=</span> <span class="n">batched_decoder</span><span class="p">(</span><span class="n">batched_encoder</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">theta</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">)[</span>
    <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$\mu$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_50_0.png" src="../_images/VAE_50_0.png" />
</div>
</div>
<p>The reconstruction is actually quite good! There is some odd behavior near the top, but otherwise quite reasonable. Finally check how well we did with getting our latent space to be standard normal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">batched_encoder</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">theta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average mu = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;Average std dev = &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average mu =  0.509954 Average std dev =  0.41120744
</pre></div>
</div>
</div>
</div>
<p>Surprisingly poor. This gets at one of the issues with VAEs: sometimes your KL will dominate and you have poor reconstruction and other times reconstruction will dominate. It just depends on the variance of your features, dimensions, and hyperparameters. You‚Äôll often want to explicitly balance those terms to better agree with your goals for constructing the VAE.</p>
</section>
<section id="bead-spring-polymer-vae">
<h2><span class="section-number">14.7. </span>Bead-Spring Polymer VAE<a class="headerlink" href="#bead-spring-polymer-vae" title="Permalink to this headline">#</a></h2>
<p>Now we‚Äôll move on to a more realistic system. We‚Äôll use a bead-spring polymer as shown in the short trajectory snippet below.</p>
<div>
    <video width="500" autoplay loop controls src="../_static/images/traj.mp4" alt="movie of point trajectory"></video>
</div>
<p>This polymer has each bead (atom) joined by a harmonic bond, a harmonic angle between each three, and a Lennard-Jones interaction potential. Knowing these items will not be necessary for the example. We‚Äôll construct a VAE that can compress the trajectory to some latent space and generate new conformations.</p>
<p>To begin, we‚Äôll use the lessons learned from <a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a> about how to align points from a trajectory. This will then serve as our training data. The space of our problem will be 12 2D vectors. Our system need not be permutation invariant, so we can flatten these vectors into a 24 dimensional input. The code belows loads and aligns the trajectory</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="c1">###---------Transformation Functions----###</span>
<span class="k">def</span> <span class="nf">center_com</span><span class="p">(</span><span class="n">paths</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Align paths to COM at each frame&quot;&quot;&quot;</span>
    <span class="n">coms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">paths</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">paths</span> <span class="o">-</span> <span class="n">coms</span>


<span class="k">def</span> <span class="nf">make_2drot</span><span class="p">(</span><span class="n">angle</span><span class="p">):</span>
    <span class="n">mats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)]])</span>
    <span class="c1"># swap so batch axis is first</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">mats</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">find_principle_axis</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute single principle axis for points&quot;&quot;&quot;</span>
    <span class="n">inertia</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">points</span>
    <span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">inertia</span><span class="p">)</span>
    <span class="c1"># get biggest eigenvalue</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">evecs</span><span class="p">[:,</span> <span class="n">order</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">align_principle</span><span class="p">(</span><span class="n">paths</span><span class="p">,</span> <span class="n">axis_finder</span><span class="o">=</span><span class="n">find_principle_axis</span><span class="p">):</span>
    <span class="c1"># This is a degenarate version, I removed mirror disambiguation</span>
    <span class="c1"># to make latent space jump less. Data augmentation will</span>
    <span class="c1"># have to overcome this issue</span>
    <span class="c1"># the code is commented out below</span>
    <span class="n">vecs</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis_finder</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">]</span>
    <span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vecs</span><span class="p">)</span>
    <span class="c1"># find angle to rotate so these are pointed towards pos x</span>
    <span class="n">cur_angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">vecs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">vecs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="c1"># cross = np.cross(vecs[:,0], vecs[:,1])</span>
    <span class="n">rot_angle</span> <span class="o">=</span> <span class="o">-</span><span class="n">cur_angle</span>  <span class="c1"># - (cross &lt; 0) * np.pi</span>
    <span class="n">rot_mat</span> <span class="o">=</span> <span class="n">make_2drot</span><span class="p">(</span><span class="n">rot_angle</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">paths</span> <span class="o">@</span> <span class="n">rot_mat</span>


<span class="c1">###-----------------------------------###</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/main/data/long_paths.npz&quot;</span><span class="p">,</span>
    <span class="s2">&quot;long_paths.npz&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;long_paths.npz&quot;</span><span class="p">)[</span><span class="s2">&quot;arr&quot;</span><span class="p">]</span>
<span class="c1"># transform to be rot/trans invariant</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">align_principle</span><span class="p">(</span><span class="n">center_com</span><span class="p">(</span><span class="n">paths</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;cool&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">16</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;All Frames&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_56_0.png" src="../_images/VAE_56_0.png" />
</div>
</div>
<p>Before training, let‚Äôs examine some of the <strong>marginals</strong> of the data. Marginals mean we‚Äôve transformed (by integration) our probability distribution to be a function of only 1-2 variables so that we can plot nicely. We‚Äôll look at the pairwise distance between points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dist between 0-</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_58_0.png" src="../_images/VAE_58_0.png" />
</div>
</div>
<p>These look a little like the chi distribution with two degrees of freedom. Notice that the support (x-axis) changes between them though. We‚Äôll keep an eye on these when we evaluate the efficacy of our VAE.</p>
<section id="vae-model">
<h3><span class="section-number">14.7.1. </span>VAE Model<a class="headerlink" href="#vae-model" title="Permalink to this headline">#</a></h3>
<p>We‚Äôll build the VAE like above. I will make two changes. I will use JAX‚Äôs random number generator and I will make the number of layers variable. The code is hidden below, but you can expand to see the details. We‚Äôll be starting with 4 layers total (3 hidden) with a hidden layer dimension of 256. Another detail is that we flatten the input/output since the order is preserved and thus we do not worry about separating the x,y dimension out.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">12</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">hidden_units</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="n">theta</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">key</span>


<span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">w</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span>
        <span class="n">phi</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">phi</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">phi</span><span class="p">,</span> <span class="n">key</span>


<span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">w</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">hz</span> <span class="o">=</span> <span class="n">hz</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">hz</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">hz</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss">
<h3><span class="section-number">14.7.2. </span>Loss<a class="headerlink" href="#loss" title="Permalink to this headline">#</a></h3>
<p>The loss function is similar to above, but I will not even bother with the Gaussian outputs. You can see the only change is that we drop the output Gaussian standard deviation from the loss, which remember was not trainable anyway.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VAE Loss&quot;&quot;&quot;</span>
    <span class="c1"># reconstruction loss</span>
    <span class="n">sampled_z_params</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
    <span class="c1"># reparameterization trick</span>
    <span class="c1"># we use standard normal sample and multiply by parameters</span>
    <span class="c1"># to ensure derivatives correctly propogate to encoder</span>
    <span class="n">sampled_z</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># MSE now instead</span>
    <span class="n">xp</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">sampled_z</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">rloss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">xp</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># LK loss</span>
    <span class="n">klloss</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span>
        <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sampled_z_params</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="c1"># combined</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rloss</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">klloss</span><span class="p">)])</span>


<span class="c1"># update compiled functions</span>
<span class="n">batched_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_decoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">batched_encoder</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">modified_loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">fast_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="n">fast_loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">batched_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3><span class="section-number">14.7.3. </span>Training<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p>Finally comes the training. The only changes to this code are to flatten our input data and shuffle to prevent the each batch from having similar conformations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">flat_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
<span class="c1"># scramble it</span>
<span class="n">flat_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">flat_data</span><span class="p">)</span>


<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">theta0</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">phi0</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">init_phi</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">((</span><span class="n">theta0</span><span class="p">,</span> <span class="n">phi0</span><span class="p">))</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># KL/Reconstruction balance</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">bi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_data</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="c1"># make a batch into shape B x 1</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">flat_data</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
        <span class="c1"># udpate random number key</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># get current parameter values from optimizer</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">last_state</span> <span class="o">=</span> <span class="n">opt_state</span>
        <span class="c1"># compute gradient and update</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">fast_grad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">bi</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
    <span class="c1"># use large batch for tracking progress</span>
    <span class="n">lvalue</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fast_loss</span><span class="p">(</span><span class="n">flat_data</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">theta</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">subkey</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lvalue</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Reconstruction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;KL&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_65_0.png" src="../_images/VAE_65_0.png" />
</div>
</div>
<p>As usual, this model is undertrained. A latent space of 2, which we chose for plotting convenience, is also probably a little too compressed. Let‚Äôs sample a few conformation and see how they look.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sampled_data</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">latent_dim</span><span class="p">]),</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sampled_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sampled_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_67_0.png" src="../_images/VAE_67_0.png" />
</div>
</div>
<p>These look reasonable compared with the trajectory video showing the training conformations.</p>
</section>
</section>
<section id="using-vae-on-a-trajectory">
<h2><span class="section-number">14.8. </span>Using VAE on  a Trajectory<a class="headerlink" href="#using-vae-on-a-trajectory" title="Permalink to this headline">#</a></h2>
<p>There are three main things to do with a VAE on a trajectory. The first is to go from a trajectory in the feature dimension to the latent dimension. This can simplify analysis of dynamics or act as a reaction coordinate for free energy methods. The second is to generate new conformations. This could be used to fill-in under sampling or perhaps extrapolate to new regions of latent space. You can also use the VAE to examine marginals that are perhaps under-sampled. Finally, you can do optimization on the latent space. For example, you could try to find the most compact structure. We‚Äôll examine these examples but there are many other things you could examine. For a more complete model example with attention and 3D coordinates, see Winter et al. <span id="id5">[<a class="reference internal" href="#id92" title="Robin Winter, Frank No√©, and Djork-Arn√© Clevert. Auto-encoding molecular conformations. arXiv preprint arXiv:2101.01618, 2021.">WNoeC21</a>]</span>. You can find applications of VAEs on trajectories for molecular design <span id="id6">[<a class="reference internal" href="#id130" title="Kirill Shmilovich, Rachael A Mansbach, Hythem Sidky, Olivia E Dunne, Sayak Subhra Panda, John D Tovar, and Andrew L Ferguson. Discovery of self-assembling œÄ-conjugated peptides by active learning-directed coarse-grained molecular simulation. The Journal of Physical Chemistry B, 124(19):3873‚Äì3891, 2020.">SMS+20</a>]</span>, coarse-graining <span id="id7">[<a class="reference internal" href="#id131" title="Wujie Wang and Rafael G√≥mez-Bombarelli. Coarse-graining auto-encoders for molecular dynamics. npj Computational Materials, 5(1):1‚Äì9, 2019.">WGomezB19</a>]</span>, and identifying rare-events <span id="id8">[<a class="reference internal" href="#id132" title="Jo√£o Marcelo Lamim Ribeiro, Pablo Bravo, Yihang Wang, and Pratyush Tiwary. Reweighted autoencoded variational bayes for enhanced sampling (rave). The Journal of chemical physics, 149(7):072301, 2018.">RBWT18</a>]</span>.</p>
<section id="latent-trajectory">
<h3><span class="section-number">14.8.1. </span>Latent Trajectory<a class="headerlink" href="#latent-trajectory" title="Permalink to this headline">#</a></h3>
<p>Let‚Äôs start by computing a latent trajctory. I‚Äôm going to load a shorter trajectory which has the frames closer together in time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/main/data/paths.npz&quot;</span><span class="p">,</span> <span class="s2">&quot;paths.npz&quot;</span>
<span class="p">)</span>
<span class="n">paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;paths.npz&quot;</span><span class="p">)[</span><span class="s2">&quot;arr&quot;</span><span class="p">]</span>
<span class="n">short_data</span> <span class="o">=</span> <span class="n">align_principle</span><span class="p">(</span><span class="n">center_com</span><span class="p">(</span><span class="n">paths</span><span class="p">))</span>

<span class="c1"># get latent params</span>
<span class="c1"># throw away standard deviation</span>
<span class="n">latent_traj</span> <span class="o">=</span> <span class="n">batched_encoder</span><span class="p">(</span><span class="n">short_data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">),</span> <span class="n">phi</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">latent_traj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">latent_traj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_71_0.png" src="../_images/VAE_71_0.png" />
</div>
</div>
<p>You can see that the trajectory is relatively continuous, except for a few wide jumps. We‚Äôll see below that this is because the alignment process can have big jumps as our principle axis rapidly moves when the points rearrange. Let‚Äôs compare the video and the z-path side-by-side. You can find the code for this movie on the github repo.</p>
<div>
    <video width="500" autoplay loop controls src="../_static/images/latent_traj.mp4" alt="movie of point trajectory"></video>
</div>
<p>You can see the quick change is due to our alignment quickly changing. This is why aligning on the principle axis isn‚Äôt always perfect: your axis can flip 90 degrees because the internal points change the moment of inertia enough to change.</p>
</section>
<section id="generate-new-samples">
<h3><span class="section-number">14.8.2. </span>Generate New Samples<a class="headerlink" href="#generate-new-samples" title="Permalink to this headline">#</a></h3>
<p>Let‚Äôs see how our samples look.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sampled_data</span> <span class="o">=</span> <span class="n">batched_decoder</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)),</span> <span class="n">theta</span>
<span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">sampled_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sampled_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span>
    <span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Generated&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_77_0.png" src="../_images/VAE_77_0.png" />
</div>
</div>
<p>The samples are not perfect, but we‚Äôre close. Let‚Äôs examine the marginals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dist between 0-</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">sampled_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">sampled_data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_79_0.png" src="../_images/VAE_79_0.png" />
</div>
</div>
<p>You can see that there are some issues here as well. Remember that our latent space is quite small: 2D. So we should not be that surprised that we‚Äôre losing information from our 24D input space.</p>
</section>
<section id="optimization-on-latent-space">
<h3><span class="section-number">14.8.3. </span>Optimization on Latent Space<a class="headerlink" href="#optimization-on-latent-space" title="Permalink to this headline">#</a></h3>
<p>Finally, let us examine how we can optimize in the latent space. Let‚Äôs say I want to find the most compact structure. We‚Äôll define our loss function as the radius of gyration and take its derivative with respect to <span class="math notranslate nohighlight">\(z\)</span>. Recall the definition of radius of gyration is</p>
<div class="amsmath math notranslate nohighlight" id="equation-901380a9-a1c6-4698-ab49-1779b4d2f6f5">
<span class="eqno">(14.13)<a class="headerlink" href="#equation-901380a9-a1c6-4698-ab49-1779b4d2f6f5" title="Permalink to this equation">#</a></span>\[\begin{equation}
R_g = \frac{1}{N}\sum_i r_i^2
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_i\)</span> is distance to center of mass. Our generated samples are, by definition, centered at the origin though so we do not need to worry about center of mass. We want to take derivatives in <span class="math notranslate nohighlight">\(z\)</span>, but need samples in <span class="math notranslate nohighlight">\(x\)</span> to compute radius of gyration. We use the decoder to get an <span class="math notranslate nohighlight">\(x\)</span> and can propagate derivatives through it, because it is a differentiable neural network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rg_loss</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rg</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rg</span><span class="p">)</span>


<span class="n">rg_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">rg_loss</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will find the <span class="math notranslate nohighlight">\(z\)</span> that minimizes the radius of gyration by using gradient descent with the derivative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">latent_dim</span><span class="p">])</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rg_loss</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">rg_grad</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$R_g$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_84_0.png" src="../_images/VAE_84_0.png" />
</div>
</div>
<p>We have a <span class="math notranslate nohighlight">\(z\)</span> with a very low radius of gyration. How good is it? Well, we can also see what was the lowest radius of gyration <em>observed</em> structure in our trajectory. We compare them below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get min from training</span>
<span class="n">train_rgmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="c1"># use new z</span>
<span class="n">opt_rgmin</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="n">train_rgmin</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">train_rgmin</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">opt_rgmin</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">opt_rgmin</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Optimized&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/VAE_86_0.png" src="../_images/VAE_86_0.png" />
</div>
</div>
<p>What is remarkable about this is that the optimized one has no overlaps and still reasonable bond-lengths. It is also more compact than the lowest radius of gyration found in the training example.</p>
</section>
</section>
<section id="relevant-videos">
<h2><span class="section-number">14.9. </span>Relevant Videos<a class="headerlink" href="#relevant-videos" title="Permalink to this headline">#</a></h2>
<section id="using-vae-for-coarse-grained-molecular-simulation">
<h3><span class="section-number">14.9.1. </span>Using VAE for Coarse-Grained Molecular Simulation<a class="headerlink" href="#using-vae-for-coarse-grained-molecular-simulation" title="Permalink to this headline">#</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/l_NfukhR2XU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
<section id="using-vae-for-molecular-graph-generation">
<h3><span class="section-number">14.9.2. </span>Using VAE for Molecular Graph Generation<a class="headerlink" href="#using-vae-for-molecular-graph-generation" title="Permalink to this headline">#</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/VXNjCAmb6Zw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
<section id="review-of-molecular-graph-generative-models-including-vae">
<h3><span class="section-number">14.9.3. </span>Review of Molecular Graph Generative Models (including VAE)<a class="headerlink" href="#review-of-molecular-graph-generative-models-including-vae" title="Permalink to this headline">#</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/z0lh4kXWt5E" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">14.10. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A variational autoencoder is a generative deep learning model capable of unsupervised learning. It is capable of of generating new data points not seen in training.</p></li>
<li><p>A VAE is a set of two trained conditional probability distributions that operate on examples from the data <span class="math notranslate nohighlight">\(x\)</span> and the latent space <span class="math notranslate nohighlight">\(z\)</span>. The encoder goes from data to latent and the decoder goes from latent to data.</p></li>
<li><p>The loss function is the log likelihood that we observed the training point <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
<li><p>Taking the log allows us to sum/average over data to aggregate multiple points.</p></li>
<li><p>The VAE can be used for both discrete or continuous features.</p></li>
<li><p>The goal with VAE is to reproduce the probability distribution of <span class="math notranslate nohighlight">\(x\)</span>. Comparing the distribution over <span class="math notranslate nohighlight">\(z\)</span> and that of <span class="math notranslate nohighlight">\(x\)</span> allows us to evaluate how well the VAE operates.</p></li>
<li><p>A bead-spring polymer VAE example shows how VAEs operate on a trajectory.</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">14.11. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id9">
<dl class="citation">
<dt class="label" id="id173"><span class="brackets">KW13</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>Diederik¬†P Kingma and Max Welling. Auto-encoding variational bayes. <em>arXiv preprint arXiv:1312.6114</em>, 2013.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id3">MRST19</a></span></dt>
<dd><p>Emile Mathieu, Tom Rainforth, N¬†Siddharth, and Yee¬†Whye Teh. Disentangling disentanglement in variational autoencoders. In <em>International Conference on Machine Learning</em>, 4402‚Äì4412. PMLR, 2019.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id5">WNoeC21</a></span></dt>
<dd><p>Robin Winter, Frank No√©, and Djork-Arn√© Clevert. Auto-encoding molecular conformations. <em>arXiv preprint arXiv:2101.01618</em>, 2021.</p>
</dd>
<dt class="label" id="id130"><span class="brackets"><a class="fn-backref" href="#id6">SMS+20</a></span></dt>
<dd><p>Kirill Shmilovich, Rachael¬†A Mansbach, Hythem Sidky, Olivia¬†E Dunne, Sayak¬†Subhra Panda, John¬†D Tovar, and Andrew¬†L Ferguson. Discovery of self-assembling œÄ-conjugated peptides by active learning-directed coarse-grained molecular simulation. <em>The Journal of Physical Chemistry B</em>, 124(19):3873‚Äì3891, 2020.</p>
</dd>
<dt class="label" id="id131"><span class="brackets"><a class="fn-backref" href="#id7">WGomezB19</a></span></dt>
<dd><p>Wujie Wang and Rafael G√≥mez-Bombarelli. Coarse-graining auto-encoders for molecular dynamics. <em>npj Computational Materials</em>, 5(1):1‚Äì9, 2019.</p>
</dd>
<dt class="label" id="id132"><span class="brackets"><a class="fn-backref" href="#id8">RBWT18</a></span></dt>
<dd><p>Jo√£o Marcelo¬†Lamim Ribeiro, Pablo Bravo, Yihang Wang, and Pratyush Tiwary. Reweighted autoencoded variational bayes for enhanced sampling (rave). <em>The Journal of chemical physics</em>, 149(7):072301, 2018.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="NLP.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">13. </span>Deep Learning on Sequences</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="flows.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Normalizing Flows</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Andrew D. White<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">‚úï</button> <img id="wh-modal-img"> </div>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>