
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub üìñ" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>15. Normalizing Flows &#8212; deep learning for molecules &amp; materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cb1cce99" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css?v=ffeaf963" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl/flows';</script>
    <script src="../_static/custom.js?v=3f5092eb"></script>
    <link rel="canonical" href="https://dmol.pub/dl/flows.html" />
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="16. Modern Molecular NNs" href="molnets.html" />
    <link rel="prev" title="14. Variational Autoencoder" href="VAE.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="deep learning for molecules & materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="deep learning for molecules & materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">A. Math Review</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/tensors-and-shapes.html">1. Tensors and Shapes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">B. Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ml/introduction.html">2. Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/regression.html">3. Regression &amp; Model Assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/classification.html">4. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/kernel.html">5. Kernel Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C. Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">6. Deep Learning Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">7. Standard Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">8. Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">9. Input Data &amp; Equivariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="Equivariant.html">10. Equivariant Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="xai.html">11. Explaining Predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention.html">12. Attention Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="NLP.html">13. Deep Learning on Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="VAE.html">14. Variational Autoencoder</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">15. Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="molnets.html">16. Modern Molecular NNs</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">D. Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/QM9.html">17. Predicting DFT Energies with GNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied/MolGenerator.html">18. Generative RNN in Browser</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">E. Contributed Chapters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/e3nn_traj.html">19. Equivariant Neural Network for Predicting Trajectories</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">20. Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">F. Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../style.html">21. Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">22. Changelog</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/flows.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/whitead/dmol-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/flows.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dl/flows.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Normalizing Flows</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-equation">15.1. Flow Equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bijectors">15.2. Bijectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bijector-chains">15.2.1. Bijector Chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">15.3. Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-bijectors">15.4. Common Bijectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">15.5. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moon-example">15.6. Moon Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-data">15.6.1. Generating Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#z-distribution">15.6.2. Z Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normalizing-flow">15.6.3. The Normalizing Flow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">15.6.4. Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-videos">15.7. Relevant Videos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-flow-for-molecular-conformation">15.7.1. Normalizing Flow for Molecular Conformation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">15.8. Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">15.9. Cited References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="normalizing-flows">
<h1><span class="section-number">15. </span>Normalizing Flows<a class="headerlink" href="#normalizing-flows" title="Link to this heading">#</a></h1>
<p>The VAE was our first example of a generative model that is capable of sampling from <span class="math notranslate nohighlight">\(P(x)\)</span>. A VAE can also estimate <span class="math notranslate nohighlight">\(P(x)\)</span> by going from the encoder to <span class="math notranslate nohighlight">\(z\)</span>, and then using the known distribution <span class="math notranslate nohighlight">\(P(z)\)</span>. However, VAEs typically are not great at either tasks. Their samples are often described as ‚Äúblurry‚Äù because they have mean-seeking behavior. That is, the individual samples do not correspond to a very likely example (not mode-seeking). Instead the distribution of samples is good (mean-seeking). <strong>Generative adversarial networks</strong> (GANs) are an alternative to VAEs which have high probability samples. However, both methods suffer often have training problems and also give poor estimates of <span class="math notranslate nohighlight">\(P(x)\)</span> because of both lack of normalization and assumptions of normal distributions. An alternative is a <strong>normalizing flow</strong> that has better stability training and better estimates of <span class="math notranslate nohighlight">\(P(x)\)</span>. Normalizing flows are also used as components in other networks, like it can act as <span class="math notranslate nohighlight">\(P(z)\)</span> of a latent space for a VAE instead of standard normal distributions.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You can construct interesting bijective functions between <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, but in the case of finite precision on a computer this is not possible so we can work with the slightly imprecise heuristic that functions need to have the same number of inputs and outputs to be bijective.</p>
</aside>
<p>A <strong>normalizing flow</strong> is similar to a VAE in that we try to build up <span class="math notranslate nohighlight">\(P(x)\)</span> by starting from a simple known distribution <span class="math notranslate nohighlight">\(P(z)\)</span>. We use functions, like the decoder from a VAE, to go from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(z\)</span>. However, we make sure that the functions we choose keep the probability mass normalized (<span class="math notranslate nohighlight">\(\sum P(x) = 1\)</span>) and can be used forward (to sample from x) and backward (to compute <span class="math notranslate nohighlight">\(P(x)\)</span>). We call these functions <strong>bijectors</strong> because they are bijective (surjective and injective). Recall surjective (onto) means every output has a corresponding input and injective (onto) means each output has exactly one corresponding input.</p>
<p>An example of a bijector is an element-wise cosine <span class="math notranslate nohighlight">\(y_i = \cos x_i\)</span> (assuming <span class="math notranslate nohighlight">\(x_i\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(\pi\)</span>). A non-bijective function would be <span class="math notranslate nohighlight">\(y_i = \cos x_i\)</span> on the interval from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(2\pi\)</span>, because it outputs all values from <span class="math notranslate nohighlight">\([0,1]\)</span> twice and hence is not injective. Any function which changes the number of elements is automatically not bijective (see margin note). A consequence of using only bijectors in constructing our normalizing flow is that the size of the latent space must be equal to the size of the feature space. Remember the VAE used a smaller latent space than the feature space.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="VAE.html"><span class="doc">Variational Autoencoder</span></a> and assumes the same background of probability theory. This chapter is an introduction to the key ideas, but is not fully developed yet. Some knowledge of vector calculus (Jacobians) is assumed as well. After completing it, you should be able to</p>
<ul class="simple">
<li><p>Understand the trade-offs between a VAE, GAN, and normalizing flow.</p></li>
<li><p>Identify a bijector and construct a bijector chain</p></li>
<li><p>Construct a normalizing flow using common bijectors types and train it</p></li>
<li><p>Sample from a normalizing flow and compute sample probabilities</p></li>
</ul>
</div>
<p>You can find a recent review of normalizing <a class="reference external" href="https://arxiv.org/pdf/1908.09257.pdf">flows here</a> <span id="id1">[<a class="reference internal" href="#id75" title="Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: an introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.">KPB20</a>]</span> and <a class="reference external" href="https://arxiv.org/abs/1912.02762">here</a><span id="id2">[<a class="reference internal" href="#id92" title="George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762, 2019.">PNR+19</a>]</span>. Although generating images and sound is the most popular application of normalizing flows, some of their biggest scientific impact has been on more efficient sampling from posteriors or likelihoods and other complex probability distributions <span id="id3">[<a class="reference internal" href="#id91" title="George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: fast likelihood-free inference with autoregressive flows. In The 22nd International Conference on Artificial Intelligence and Statistics, 837‚Äì848. PMLR, 2019.">PSM19</a>]</span>. You find details on how to do normalizing flows on categorical (discrete) data in Hoogeboom et al. <span id="id4">[<a class="reference internal" href="#id109" title="Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows: learning categorical distributions with normalizing flows. In Third Symposium on Advances in Approximate Bayesian Inference. 2021. URL: https://openreview.net/forum?id=fdsXhAy5Cp.">HNJ+21</a>]</span>.</p>
<section id="flow-equation">
<h2><span class="section-number">15.1. </span>Flow Equation<a class="headerlink" href="#flow-equation" title="Link to this heading">#</a></h2>
<p>Recall for the VAE decoder, we had an explicit formula for <span class="math notranslate nohighlight">\(p(x | z)\)</span>. This allowed us to compute <span class="math notranslate nohighlight">\(p(x) = \int\,dz p(x | z)p(z)\)</span> which is the quantity of interest. The VAE decoder is a conditional probability density function. In the normalizing flow, we do not use probability density functions. We use bijective functions. So we cannot just compute an integral to change variables. We can use the change of variable formula. Consider our normalizing flow to be defined by our bijector <span class="math notranslate nohighlight">\(x = f(z)\)</span>, its inverse <span class="math notranslate nohighlight">\(z = g(x)\)</span>, and the starting probability distribution <span class="math notranslate nohighlight">\(P_z(z)\)</span>. Then the formula for probability of <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="amsmath math notranslate nohighlight" id="equation-2a86f621-51ad-46f9-a947-e04185096b17">
<span class="eqno">(15.1)<a class="headerlink" href="#equation-2a86f621-51ad-46f9-a947-e04185096b17" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(x) = P_z\left(g(x)\right) \,\left| \textrm{det}\left[\mathbf{J}_g\right]\right|
\end{equation}\]</div>
<p>where the term on the right is the absolute value of the determinant of the Jacobian of <span class="math notranslate nohighlight">\(g\)</span>. <a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobians</a> are matrices that describe how infinitesimal changes in each domain dimension change each range dimension. This term corrects for the volume change of the distribution. For example, if <span class="math notranslate nohighlight">\(f(z) = 2z\)</span>, then <span class="math notranslate nohighlight">\(g(x) = x / 2\)</span>, and the Jacobian determinant is <span class="math notranslate nohighlight">\(1 / 2\)</span>. The intuition is that we are stretching out <span class="math notranslate nohighlight">\(z\)</span> by 2, so we need to account for the increase in volume to keep the probability normalized. You can read more about the change of variable formula for <a class="reference external" href="https://cranmer.github.io/stats-ds-book/distributions/change-of-variables.html">probability distributions here</a></p>
</section>
<section id="bijectors">
<h2><span class="section-number">15.2. </span>Bijectors<a class="headerlink" href="#bijectors" title="Link to this heading">#</a></h2>
<p>A bijector is a function that is <a class="reference external" href="https://en.wikipedia.org/wiki/Injective_function">injective</a> (1 to 1) and <a class="reference external" href="https://en.wikipedia.org/wiki/Surjective_function">surjective</a> (onto). An equivalent way to view a bijective function is if it has an inverse. For example, a sum reduction has no inverse and is thus not bijective. <span class="math notranslate nohighlight">\(\sum [1,0] = 1\)</span> and <span class="math notranslate nohighlight">\(\sum [-1, 2] = 1\)</span>. Multiplying by a matrix which has an inverse is bijective. <span class="math notranslate nohighlight">\(y = x^2\)</span> is not bijective, since <span class="math notranslate nohighlight">\(y = 4\)</span> has two solutions.</p>
<p>Remember that we must compute the determinant of the bijector Jacobian. If the Jacobian is dense (all output elements depend on all input elements), computing this quantity will be <span class="math notranslate nohighlight">\(O\left(|x|_0^3\right)\)</span> where <span class="math notranslate nohighlight">\(|x|_0\)</span> is the number of dimensions of <span class="math notranslate nohighlight">\(x\)</span> because a determinant scales by <span class="math notranslate nohighlight">\(O(n^3)\)</span>. This would make computing normalizing flows impractical in high-dimensions. However, in practice we restrict ourselves to bijectors that have easy to calculate Jacobians. For example, if the bijector is <span class="math notranslate nohighlight">\(x_i = \cos z_i\)</span> then the Jacobian will be diagonal. Such a diagonal Jacobian means that each dimension is independent of the other though.</p>
<p>One way to get faster determinants without just making each dimension independent is to get a triangular Jacobian. Then <span class="math notranslate nohighlight">\(x_0\)</span> only depends on <span class="math notranslate nohighlight">\(z_0\)</span>, <span class="math notranslate nohighlight">\(x_1\)</span> depends on <span class="math notranslate nohighlight">\(z_0, z_1\)</span>, and <span class="math notranslate nohighlight">\(x_2\)</span> depends on <span class="math notranslate nohighlight">\(z_0, z_1, z_2\)</span>, etc. This enables fitting high-dimensional correlations for some of the dimensions (like <span class="math notranslate nohighlight">\(x_{n}\)</span>). The matrix determinant of a triangular matrix is computed in linear time with respect to the number of dimensions ‚Äì because it is just the product of the matrix diagonal.</p>
<section id="bijector-chains">
<h3><span class="section-number">15.2.1. </span>Bijector Chains<a class="headerlink" href="#bijector-chains" title="Link to this heading">#</a></h3>
<p>Just like in deep neural networks, multiple bijectors are chained together to increase how complex of the final fit distribution <span class="math notranslate nohighlight">\(\hat{P}(x)\)</span> can be. The change of variable equation can be repeatedly applied:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7a42315f-a16f-4245-b060-ef9a39e00e43">
<span class="eqno">(15.2)<a class="headerlink" href="#equation-7a42315f-a16f-4245-b060-ef9a39e00e43" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(x) = P_z\left[g_1\left(g_0(x)\right)\right] \,\left| \textrm{det}\left[\mathbf{J}_{g_1}\right]\right|  \left|\textrm{det}\left[\mathbf{J}_{g_0}\right]\right|
\end{equation}\]</div>
<p>where we would compute <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(f_0\left(f_1(z)\right)\)</span>. One critical point is that you should also include a <strong>permute bijector</strong> that swaps the order of dimensions. Since the bijectors typically have triangular Jacobians, certain output dimensions will depend on many input dimensions and others will only depend on a single one. By applying a permutation, you allow each dimension to influence each other.</p>
</section>
</section>
<section id="training">
<h2><span class="section-number">15.3. </span>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<p>At this point, you may be wondering how you could possibly train a normalizing flow. The trainable parameters appear in the bijectors. They have adjustable parameters. The loss equation is quite simple: the negative log-likelihood (negative to make it minimization). Explicitly:</p>
<div class="amsmath math notranslate nohighlight" id="equation-98cd0b23-0031-470e-be33-67f488bd54bc">
<span class="eqno">(15.3)<a class="headerlink" href="#equation-98cd0b23-0031-470e-be33-67f488bd54bc" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathcal{l} = -\log P_z\left[g_1\left(g_0(x)\right)\right] - \sum_i \log\left| \textrm{det}\left[\mathbf{J}_{g_i}\right]\right|
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the training point and when you take the gradient of the loss, it is with respect to the parameters of the bijectors.</p>
</section>
<section id="common-bijectors">
<h2><span class="section-number">15.4. </span>Common Bijectors<a class="headerlink" href="#common-bijectors" title="Link to this heading">#</a></h2>
<p>The choice of bijector functions is a fast changing area. I will thus only mention a few. You can of course use any bijective function or matrix, but these become inefficient at high-dimension due to the Jacobian calculation. One class of efficient bijectors are autoregressive bijectors. These have triangular Jacobians because each output dimension can only depend on the dimensions with a lower index. There are two variants: masked autoregressive flows (MAF)<span id="id5">[<a class="reference internal" href="#id76" title="George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, 2338‚Äì2347. 2017.">PPM17</a>]</span> and inverse autoregressive flows (IAF) <span id="id6">[<a class="reference internal" href="#id77" title="Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in neural information processing systems, 4743‚Äì4751. 2016.">KSJ+16</a>]</span>. MAFs are efficient at training and computing probabilities, but are slow for sampling from <span class="math notranslate nohighlight">\(P(x)\)</span>. IAFs are slow at training and computing probabilities but efficient for sampling. Wavenets combine the advantages of both <span id="id7">[<a class="reference internal" href="#id78" title="Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: a generative flow for raw audio. arXiv preprint arXiv:1811.02155, 2018.">KLS+18</a>]</span>. I‚Äôll mention one other common bijector which is not autoregressive: real non-volume preserving (RealNVPs) <span id="id8">[<a class="reference internal" href="#id79" title="Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.">DSDB16</a>]</span>. RealNVPs are less expressive than IAFs/MAFs, meaning they have trouble replicating complex distributions, but are efficient at all three tasks: training, sampling, and computing probabilities. Another interesting variant is the Glow bijector,which is able to expand the rank of the normalizing flow, for example going from a matrix to an RGB image <span id="id9">[<a class="reference internal" href="#id80" title="Hari Prasanna Das, Pieter Abbeel, and Costas J Spanos. Dimensionality reduction flows. arXiv preprint arXiv:1908.01686, 2019.">DAS19</a>]</span>. What are the equations for all these bijectors? Most are variants of standard neural network layers but with special rules about which outputs depend on which inputs.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Remember to add permute bijectors between autoregressive bijectors to ensure the dependence between dimensions is well-mixed.</p>
</div>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">15.5. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Link to this heading">#</a></h2>
<p>Click the ¬†<i aria-label="Launch interactive content" class="fas fa-rocket"></i>¬† above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
<p>The hidden code below imports the normflows package and other necessary packages. The normflows package provides distributions, bijective flows, and normalizing flow models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">normflows</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7f4a28bc4650&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="moon-example">
<h2><span class="section-number">15.6. </span>Moon Example<a class="headerlink" href="#moon-example" title="Link to this heading">#</a></h2>
<p>We‚Äôll start with a basic 2D example to learn the two moons distribution with a normalizing flow. Two moons is a common example dataset that is hard to cluster and model as a probability distribution.</p>
<p>When doing normalizing flows you have two options to implement them. You can do all the Jacobians, inverses, and likelihood calculations analytically and implement them in a normal ML framework like Jax, PyTorch, or TensorFlow. This is actually most common. The second option is to utilize a probability library that knows how to use bijectors and distributions. We‚Äôll use <code class="docutils literal notranslate"><span class="pre">normflows</span></code>, a PyTorch-based library for normalizing flows.</p>
<section id="generating-data">
<h3><span class="section-number">15.6.1. </span>Generating Data<a class="headerlink" href="#generating-data" title="Link to this heading">#</a></h3>
<p>In the code below, I set-up my imports and sample points which will be used for training. Remember, this code has nothing to do with normalizing flows ‚Äì it‚Äôs just to generate data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">moon_n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">ndim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">moon_n</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f48cddd8e10&gt;]
</pre></div>
</div>
<img alt="../_images/ee04abbbaf32f0db4e451faaa06c3d4e27125591f02677d5e10817496dd8cb2d.png" src="../_images/ee04abbbaf32f0db4e451faaa06c3d4e27125591f02677d5e10817496dd8cb2d.png" />
</div>
</div>
</section>
<section id="z-distribution">
<h3><span class="section-number">15.6.2. </span>Z Distribution<a class="headerlink" href="#z-distribution" title="Link to this heading">#</a></h3>
<p>Our Z distribution should always be as simple as possible. I‚Äôll create a 2D Gaussian with unit variance, no covariance, and centered at 0. We‚Äôll use the <code class="docutils literal notranslate"><span class="pre">normflows</span></code> package, which provides distributions, flows (bijectors), and the normalizing flow model class that handles change of variable calculations automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zdist</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">DiagGaussian</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">zdist</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DiagGaussian()
</pre></div>
</div>
</div>
</div>
<p>This is a 2D diagonal Gaussian. Let‚Äôs now sample from this distribution and view it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">zdist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/535e3dae661798e34f6c48bcd2549d87696d55a1fd1ca47e1e319a328a00ccb5.png" src="../_images/535e3dae661798e34f6c48bcd2549d87696d55a1fd1ca47e1e319a328a00ccb5.png" />
</div>
</div>
<p>As expected, our starting distribution looks nothing like are target distribution. Let‚Äôs demonstrate a bijector now. We‚Äôll implement the following bijector:</p>
<div class="math notranslate nohighlight">
\[
x = \vec{z} \times (1, 0.5)^T + (0.5, 0.25)
\]</div>
<p>This is bijective because the operations are element-wise and invertible. We‚Äôll use the <code class="docutils literal notranslate"><span class="pre">Affine</span></code> flow from normflows, which implements <span class="math notranslate nohighlight">\(x = s \odot z + t\)</span> where <span class="math notranslate nohighlight">\(s\)</span> is a scale and <span class="math notranslate nohighlight">\(t\)</span> is a shift.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">flows</span><span class="o">.</span><span class="n">AffineConstFlow</span><span class="p">((</span><span class="n">ndim</span><span class="p">,))</span>
<span class="n">b</span><span class="o">.</span><span class="n">s</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]))</span>
<span class="n">b</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>To now apply the change of variable formula, we create a normalizing flow model. What is important about this is that we can compute log-likelihoods and sample from it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">td</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">NormalizingFlow</span><span class="p">(</span><span class="n">zdist</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">])</span>
<span class="n">td</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NormalizingFlow(
  (q0): DiagGaussian()
  (flows): ModuleList(
    (0): AffineConstFlow()
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ad02f89e7dceb4d8e786e75a86c2fc72c2a7d8556ae7e5566fc86f7a5bfea02b.png" src="../_images/ad02f89e7dceb4d8e786e75a86c2fc72c2a7d8556ae7e5566fc86f7a5bfea02b.png" />
</div>
</div>
<p>We show above the sampling from this new distribution. We can also plot it‚Äôs probability, which is impossible for a VAE-like model!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make points for grid</span>
<span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># compute P(x)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">zgrid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="c1"># plot and set axes limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/942c3665e3fb5ed714d7d5badbcb2cb0655087c3658c350825f9cf8e309bd4cc.png" src="../_images/942c3665e3fb5ed714d7d5badbcb2cb0655087c3658c350825f9cf8e309bd4cc.png" />
</div>
</div>
</section>
<section id="the-normalizing-flow">
<h3><span class="section-number">15.6.3. </span>The Normalizing Flow<a class="headerlink" href="#the-normalizing-flow" title="Link to this heading">#</a></h3>
<p>Now we will build bijectors that are expressive enough to capture the moon distribution. I will use 3 sets of a MAF and permutation for 6 total bijectors. MAF‚Äôs have dense neural network layers in them, so I will also set the usual parameters for a neural network: dimension of hidden layer and activation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It may seem counterintuitive that we use ReLU activation functions, which are not bijective, inside a bijector. The reason this works is that the neural network (which uses ReLU) is used to output the <em>parameters</em> (scale and shift) of the affine transformation. The transformation itself is <span class="math notranslate nohighlight">\(y = x \odot \exp(s) + t\)</span>, which is bijective regardless of how <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t\)</span> are computed (as long as they are deterministic functions of the input).</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">my_flows</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">my_flows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nf</span><span class="o">.</span><span class="n">flows</span><span class="o">.</span><span class="n">MaskedAffineAutoregressive</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">my_flows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nf</span><span class="o">.</span><span class="n">flows</span><span class="o">.</span><span class="n">LULinearPermute</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">DiagGaussian</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">NormalizingFlow</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">my_flows</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we have not actually trained but we can still view our distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">zgrid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fd91ddc133405a1efe1196b2fd2ad1c92670342cd762be1cdf908a6be78ad014.png" src="../_images/fd91ddc133405a1efe1196b2fd2ad1c92670342cd762be1cdf908a6be78ad014.png" />
</div>
</div>
<p>You can already see that the distribution looks more complex than a Gaussian.</p>
</section>
<section id="id10">
<h3><span class="section-number">15.6.4. </span>Training<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>To train, we minimize the negative log-likelihood using PyTorch‚Äôs optimizer. The <code class="docutils literal notranslate"><span class="pre">forward_kld</span></code> method computes the forward KL divergence, which is equivalent to negative log-likelihood on the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We train by computing the forward KL divergence on mini-batches of our training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">moon_n</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">perm</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">forward_kld</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span> <span class="o">/</span> <span class="p">(</span><span class="n">moon_n</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/360d658b99727f3b0ad5ae9fdb79e8112eab5dd57caa5cb8f65c976c33a9faf2.png" src="../_images/360d658b99727f3b0ad5ae9fdb79e8112eab5dd57caa5cb8f65c976c33a9faf2.png" />
</div>
</div>
<p>Training looks reasonable. Let‚Äôs now see our distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">zgrid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/010e5b639e86c27a1bd69dfc48db8f6759a5a282e1d078eddf18b3f08ff2d5d6.png" src="../_images/010e5b639e86c27a1bd69dfc48db8f6759a5a282e1d078eddf18b3f08ff2d5d6.png" />
</div>
</div>
<p>Wow! We now can compute the probability of any point in this distribution. You can see there are some oddities that could be fixed with further training. One issue that cannot be overcome is the connection between the two curves ‚Äì it is not possible to get fully disconnected densities. This is because of our requirement that the bijectors are invertible and volume preserving ‚Äì you can only squeeze volume so far but cannot completely disconnect. Some work has been done on addressing this issue by adding sampling to the flow and this gives more expressive normalizing flows <span id="id11">[<a class="reference internal" href="#id90" title="Hao Wu, Jonas K√∂hler, and Frank No√©. Stochastic normalizing flows. arXiv preprint arXiv:2002.06707, 2020.">WKohlerNoe20</a>]</span>.</p>
<p>Finally, we‚Äôll sample from our model just to show that indeed it is generative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/649672fdc1b9290caba8cb3f4259fc3300eb15c6bd6c01791bc039caad546195.png" src="../_images/649672fdc1b9290caba8cb3f4259fc3300eb15c6bd6c01791bc039caad546195.png" />
</div>
</div>
</section>
</section>
<section id="relevant-videos">
<h2><span class="section-number">15.7. </span>Relevant Videos<a class="headerlink" href="#relevant-videos" title="Link to this heading">#</a></h2>
<section id="normalizing-flow-for-molecular-conformation">
<h3><span class="section-number">15.7.1. </span>Normalizing Flow for Molecular Conformation<a class="headerlink" href="#normalizing-flow-for-molecular-conformation" title="Link to this heading">#</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/XhAP2VNPVhg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">15.8. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A normalizing flow builds up a probability distribution of <span class="math notranslate nohighlight">\(x\)</span> by starting from a known distribution on <span class="math notranslate nohighlight">\(z\)</span>. Bijective functions are used to go from <span class="math notranslate nohighlight">\(z\)</span> to <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>Bijectors are functions that keep the probability mass normalized and are used to go forward and backward (because they have well-defined inverses).</p></li>
<li><p>To find the probability distribution of <span class="math notranslate nohighlight">\(x\)</span> we use the change of variable formula, which requires a function inverse and Jacobian.</p></li>
<li><p>The bijector function has trainable parameters, which can be trained using a negative log-likelihood function.</p></li>
<li><p>Multiple bijectors can be chained together, but typically must include a permute bijector to swap the order of dimensions.</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">15.9. </span>Cited References<a class="headerlink" href="#cited-references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id12">
<div role="list" class="citation-list">
<div class="citation" id="id75" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">KPB20</a><span class="fn-bracket">]</span></span>
<p>Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: an introduction and review of current methods. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2020.</p>
</div>
<div class="citation" id="id92" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">PNR+19</a><span class="fn-bracket">]</span></span>
<p>George Papamakarios, Eric Nalisnick, Danilo¬†Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. <em>arXiv preprint arXiv:1912.02762</em>, 2019.</p>
</div>
<div class="citation" id="id91" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">PSM19</a><span class="fn-bracket">]</span></span>
<p>George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: fast likelihood-free inference with autoregressive flows. In <em>The 22nd International Conference on Artificial Intelligence and Statistics</em>, 837‚Äì848. PMLR, 2019.</p>
</div>
<div class="citation" id="id109" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">HNJ+21</a><span class="fn-bracket">]</span></span>
<p>Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows: learning categorical distributions with normalizing flows. In <em>Third Symposium on Advances in Approximate Bayesian Inference</em>. 2021. URL: <a class="reference external" href="https://openreview.net/forum?id=fdsXhAy5Cp">https://openreview.net/forum?id=fdsXhAy5Cp</a>.</p>
</div>
<div class="citation" id="id76" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">PPM17</a><span class="fn-bracket">]</span></span>
<p>George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In <em>Advances in Neural Information Processing Systems</em>, 2338‚Äì2347. 2017.</p>
</div>
<div class="citation" id="id77" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">KSJ+16</a><span class="fn-bracket">]</span></span>
<p>Durk¬†P Kingma, Tim Salimans, Rafal Jozefowicz, Xi¬†Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In <em>Advances in neural information processing systems</em>, 4743‚Äì4751. 2016.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">KLS+18</a><span class="fn-bracket">]</span></span>
<p>Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: a generative flow for raw audio. <em>arXiv preprint arXiv:1811.02155</em>, 2018.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">DSDB16</a><span class="fn-bracket">]</span></span>
<p>Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. <em>arXiv preprint arXiv:1605.08803</em>, 2016.</p>
</div>
<div class="citation" id="id80" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">DAS19</a><span class="fn-bracket">]</span></span>
<p>Hari¬†Prasanna Das, Pieter Abbeel, and Costas¬†J Spanos. Dimensionality reduction flows. <em>arXiv preprint arXiv:1908.01686</em>, 2019.</p>
</div>
<div class="citation" id="id90" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">WKohlerNoe20</a><span class="fn-bracket">]</span></span>
<p>Hao Wu, Jonas K√∂hler, and Frank No√©. Stochastic normalizing flows. <em>arXiv preprint arXiv:2002.06707</em>, 2020.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="VAE.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Variational Autoencoder</p>
      </div>
    </a>
    <a class="right-next"
       href="molnets.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Modern Molecular NNs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-equation">15.1. Flow Equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bijectors">15.2. Bijectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bijector-chains">15.2.1. Bijector Chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">15.3. Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-bijectors">15.4. Common Bijectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">15.5. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moon-example">15.6. Moon Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-data">15.6.1. Generating Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#z-distribution">15.6.2. Z Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normalizing-flow">15.6.3. The Normalizing Flow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">15.6.4. Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-videos">15.7. Relevant Videos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-flow-for-molecular-conformation">15.7.1. Normalizing Flow for Molecular Conformation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">15.8. Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">15.9. Cited References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andrew D. White
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">‚úï</button> <img id="wh-modal-img"> </div>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>