
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub üìñ" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>15. Normalizing Flows &#8212; deep learning for molecules &amp; materials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dmol.pub/dl/flows.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="16. Predicting DFT Energies with GNNs" href="../applied/QM9.html" />
    <link rel="prev" title="14. Variational Autoencoder" href="VAE.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">deep learning for molecules & materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   3. Regression &amp; Model Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   4. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/kernel.html">
   5. Kernel Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   6. Deep Learning Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   7. Standard Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gnn.html">
   8. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   9. Input Data &amp; Equivariances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Equivariant.html">
   10. Equivariant Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xai.html">
   11. Explaining Predictions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   12. Attention Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   13. Deep Learning on Sequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VAE.html">
   14. Variational Autoencoder
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   15. Normalizing Flows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  D. Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/QM9.html">
   16. Predicting DFT Energies with GNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/MolGenerator.html">
   17. Generative RNN in Browser
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  E. Contributed Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Hyperparameter_tuning.html">
   18. Hyperparameter Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/e3nn_traj.html">
   19. Equivariant Neural Network for Predicting Trajectories
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pretraining.html">
   20. Pretraining
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  F. Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../style.html">
   21. Style Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   22. Changelog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  G. In Progress
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="molnets.html">
   23. Modern Molecular NNs
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <script async defer src="https://api.dmol.pub/latest.js"></script><noscript><img src="https://api.dmol.pub/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript> By <a href="https://twitter.com/andrewwhite01">Andrew White</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/flows.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/whitead/dmol-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/flows.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/dl/flows.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#flow-equation">
   15.1. Flow Equation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bijectors">
   15.2. Bijectors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bijector-chains">
     15.2.1. Bijector Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   15.3. Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-bijectors">
   15.4. Common Bijectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   15.5. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moon-example">
   15.6. Moon Example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-data">
     15.6.1. Generating Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#z-distribution">
     15.6.2. Z Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-normalizing-flow">
     15.6.3. The Normalizing Flow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     15.6.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   15.7. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalizing-flow-for-molecular-conformation">
     15.7.1. Normalizing Flow for Molecular Conformation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   15.8. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   15.9. Cited References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Normalizing Flows</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#flow-equation">
   15.1. Flow Equation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bijectors">
   15.2. Bijectors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bijector-chains">
     15.2.1. Bijector Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   15.3. Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-bijectors">
   15.4. Common Bijectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   15.5. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moon-example">
   15.6. Moon Example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-data">
     15.6.1. Generating Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#z-distribution">
     15.6.2. Z Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-normalizing-flow">
     15.6.3. The Normalizing Flow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     15.6.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   15.7. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalizing-flow-for-molecular-conformation">
     15.7.1. Normalizing Flow for Molecular Conformation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   15.8. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   15.9. Cited References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="normalizing-flows">
<h1><span class="section-number">15. </span>Normalizing Flows<a class="headerlink" href="#normalizing-flows" title="Permalink to this headline">#</a></h1>
<p>The VAE was our first example of a generative model that is capable of sampling from <span class="math notranslate nohighlight">\(P(x)\)</span>. A VAE can also estimate <span class="math notranslate nohighlight">\(P(x)\)</span> by going from the encoder to <span class="math notranslate nohighlight">\(z\)</span>, and then using the known distribution <span class="math notranslate nohighlight">\(P(z)\)</span>. However, VAEs typically are not great at either tasks. Their samples are often described as ‚Äúblurry‚Äù because they have mean-seeking behavior. That is, the individual samples do not correspond to a very likely example (not mode-seeking). Instead the distribution of samples is good (mean-seeking). <strong>Generative adversarial networks</strong> (GANs) are an alternative to VAEs which have high probability samples. However, both methods suffer often have training problems and also give poor estimates of <span class="math notranslate nohighlight">\(P(x)\)</span> because of both lack of normalization and assumptions of normal distributions. An alternative is a <strong>normalizing flow</strong> that has better stability training and better estimates of <span class="math notranslate nohighlight">\(P(x)\)</span>. Normalizing flows are also used as components in other networks, like it can act as <span class="math notranslate nohighlight">\(P(z)\)</span> of a latent space for a VAE instead of standard normal distributions.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You can construct interesting bijective functions between <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, but in the case of finite precision on a computer this is not possible so we can work with the slightly imprecise heuristic that functions need to have the same number of inputs and outputs to be bijective.</p>
</aside>
<p>A <strong>normalizing flow</strong> is similar to a VAE in that we try to build up <span class="math notranslate nohighlight">\(P(x)\)</span> by starting from a simple known distribution <span class="math notranslate nohighlight">\(P(z)\)</span>. We use functions, like the decoder from a VAE, to go from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(z\)</span>. However, we make sure that the functions we choose keep the probability mass normalized (<span class="math notranslate nohighlight">\(\sum P(x) = 1\)</span>) and can be used forward (to sample from x) and backward (to compute <span class="math notranslate nohighlight">\(P(x)\)</span>). We call these functions <strong>bijectors</strong> because they are bijective (surjective and injective). Recall surjective (onto) means every output has a corresponding input and injective (onto) means each output has exactly one corresponding input.</p>
<p>An example of a bijector is an element-wise cosine <span class="math notranslate nohighlight">\(y_i = \cos x_i\)</span> (assuming <span class="math notranslate nohighlight">\(x_i\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(\pi\)</span>). A non-bijective function would be <span class="math notranslate nohighlight">\(y_i = \cos x_i\)</span> on the interval from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(2\pi\)</span>, because it outputs all values from <span class="math notranslate nohighlight">\([0,1]\)</span> twice and hence is not injective. Any function which changes the number of elements is automatically not bijective (see margin note). A consequence of using only bijectors in constructing our normalizing flow is that the size of the latent space must be equal to the size of the feature space. Remember the VAE used a smaller latent space than the feature space.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="VAE.html"><span class="doc">Variational Autoencoder</span></a> and assumes the same background of probability theory. This chapter is an introduction to the key ideas, but is not fully developed yet. Some knowledge of vector calculus (Jacobians) is assumed as well. After completing it, you should be able to</p>
<ul class="simple">
<li><p>Understand the trade-offs between a VAE, GAN, and normalizing flow.</p></li>
<li><p>Identify a bijector and construct a bijector chain</p></li>
<li><p>Construct a normalizing flow using common bijectors types and train it</p></li>
<li><p>Sample from a normalizing flow and compute sample probabilities</p></li>
</ul>
</div>
<p>You can find a recent review of normalizing <a class="reference external" href="https://arxiv.org/pdf/1908.09257.pdf">flows here</a> <span id="id1">[<a class="reference internal" href="#id74" title="Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: an introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.">KPB20</a>]</span> and <a class="reference external" href="https://arxiv.org/abs/1912.02762">here</a><span id="id2">[<a class="reference internal" href="#id91" title="George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762, 2019.">PNR+19</a>]</span>. Although generating images and sound is the most popular application of normalizing flows, some of their biggest scientific impact has been on more efficient sampling from posteriors or likelihoods and other complex probability distributions <span id="id3">[<a class="reference internal" href="#id90" title="George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: fast likelihood-free inference with autoregressive flows. In The 22nd International Conference on Artificial Intelligence and Statistics, 837‚Äì848. PMLR, 2019.">PSM19</a>]</span>. You find details on how to do normalizing flows on categorical (discrete) data in Hoogeboom et al. <span id="id4">[<a class="reference internal" href="#id108" title="Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows: learning categorical distributions with normalizing flows. In Third Symposium on Advances in Approximate Bayesian Inference. 2021. URL: https://openreview.net/forum?id=fdsXhAy5Cp.">HNJ+21</a>]</span>.</p>
<section id="flow-equation">
<h2><span class="section-number">15.1. </span>Flow Equation<a class="headerlink" href="#flow-equation" title="Permalink to this headline">#</a></h2>
<p>Recall for the VAE decoder, we had an explicit formula for <span class="math notranslate nohighlight">\(p(x | z)\)</span>. This allowed us to compute <span class="math notranslate nohighlight">\(p(x) = \int\,dz p(x | z)p(z)\)</span> which is the quantity of interest. The VAE decoder is a conditional probability density function. In the normalizing flow, we do not use probability density functions. We use bijective functions. So we cannot just compute an integral to change variables. We can use the change of variable formula. Consider our normalizing flow to be defined by our bijector <span class="math notranslate nohighlight">\(x = f(z)\)</span>, its inverse <span class="math notranslate nohighlight">\(z = g(x)\)</span>, and the starting probability distribution <span class="math notranslate nohighlight">\(P_z(z)\)</span>. Then the formula for probability of <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="amsmath math notranslate nohighlight" id="equation-dcc1fb98-05ef-4c96-a8dd-94222f9c266e">
<span class="eqno">(15.1)<a class="headerlink" href="#equation-dcc1fb98-05ef-4c96-a8dd-94222f9c266e" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(x) = P_z\left(g(x)\right) \,\left| \textrm{det}\left[\mathbf{J}_g\right]\right|
\end{equation}\]</div>
<p>where the term on the right is the absolute value of the determinant of the Jacobian of <span class="math notranslate nohighlight">\(g\)</span>. <a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobians</a> are matrices that describe how infinitesimal changes in each domain dimension change each range dimension. This term corrects for the volume change of the distribution. For example, if <span class="math notranslate nohighlight">\(f(z) = 2z\)</span>, then <span class="math notranslate nohighlight">\(g(x) = x / 2\)</span>, and the Jacobian determinant is <span class="math notranslate nohighlight">\(1 / 2\)</span>. The intuition is that we are stretching out <span class="math notranslate nohighlight">\(z\)</span> by 2, so we need to account for the increase in volume to keep the probability normalized. You can read more about the change of variable formula for <a class="reference external" href="https://cranmer.github.io/stats-ds-book/distributions/change-of-variables.html">probability distributions here</a></p>
</section>
<section id="bijectors">
<h2><span class="section-number">15.2. </span>Bijectors<a class="headerlink" href="#bijectors" title="Permalink to this headline">#</a></h2>
<p>A bijector is a function that is <a class="reference external" href="https://en.wikipedia.org/wiki/Injective_function">injective</a> (1 to 1) and <a class="reference external" href="https://en.wikipedia.org/wiki/Surjective_function">surjective</a> (onto). An equivalent way to view a bijective function is if it has an inverse. For example, a sum reduction has no inverse and is thus not bijective. <span class="math notranslate nohighlight">\(\sum [1,0] = 1\)</span> and <span class="math notranslate nohighlight">\(\sum [-1, 2] = 1\)</span>. Multiplying by a matrix which has an inverse is bijective. <span class="math notranslate nohighlight">\(y = x^2\)</span> is not bijective, since <span class="math notranslate nohighlight">\(y = 4\)</span> has two solutions.</p>
<p>Remember that we must compute the determinant of the bijector Jacobian. If the Jacobian is dense (all output elements depend on all input elements), computing this quantity will be <span class="math notranslate nohighlight">\(O\left(|x|_0^3\right)\)</span> where <span class="math notranslate nohighlight">\(|x|_0\)</span> is the number of dimensions of <span class="math notranslate nohighlight">\(x\)</span> because a determinant scales by <span class="math notranslate nohighlight">\(O(n^3)\)</span>. This would make computing normalizing flows impractical in high-dimensions. However, in practice we restrict ourselves to bijectors that have easy to calculate Jacobians. For example, if the bijector is <span class="math notranslate nohighlight">\(x_i = \cos z_i\)</span> then the Jacobian will be diagonal. Such a diagonal Jacobian means that each dimension is independent of the other though.</p>
<p>One way to get faster determinants without just making each dimension independent is to get a triangular Jacobian. Then <span class="math notranslate nohighlight">\(x_0\)</span> only depends on <span class="math notranslate nohighlight">\(z_0\)</span>, <span class="math notranslate nohighlight">\(x_1\)</span> depends on <span class="math notranslate nohighlight">\(z_0, z_1\)</span>, and <span class="math notranslate nohighlight">\(x_2\)</span> depends on <span class="math notranslate nohighlight">\(z_0, z_1, z_2\)</span>, etc. This enables fitting high-dimensional correlations for some of the dimensions (like <span class="math notranslate nohighlight">\(x_{n}\)</span>). The matrix determinant of a triangular matrix is computed in linear time with respect to the number of dimensions ‚Äì because it is just the product of the matrix diagonal.</p>
<section id="bijector-chains">
<h3><span class="section-number">15.2.1. </span>Bijector Chains<a class="headerlink" href="#bijector-chains" title="Permalink to this headline">#</a></h3>
<p>Just like in deep neural networks, multiple bijectors are chained together to increase how complex of the final fit distribution <span class="math notranslate nohighlight">\(\hat{P}(x)\)</span> can be. The change of variable equation can be repeatedly applied:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b53c3cdf-c809-46a4-93fd-c28098aab4d5">
<span class="eqno">(15.2)<a class="headerlink" href="#equation-b53c3cdf-c809-46a4-93fd-c28098aab4d5" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(x) = P_z\left[g_1\left(g_0(x)\right)\right] \,\left| \textrm{det}\left[\mathbf{J}_{g_1}\right]\right|  \left|\textrm{det}\left[\mathbf{J}_{g_0}\right]\right|
\end{equation}\]</div>
<p>where we would compute <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(f_0\left(f_1(z)\right)\)</span>. One critical point is that you should also include a <strong>permute bijector</strong> that swaps the order of dimensions. Since the bijectors typically have triangular Jacobians, certain output dimensions will depend on many input dimensions and others will only depend on a single one. By applying a permutation, you allow each dimension to influence each other.</p>
</section>
</section>
<section id="training">
<h2><span class="section-number">15.3. </span>Training<a class="headerlink" href="#training" title="Permalink to this headline">#</a></h2>
<p>At this point, you may be wondering how you could possibly train a normalizing flow. The trainable parameters appear in the bijectors. They have adjustable parameters. The loss equation is quite simple: the negative log-likelihood (negative to make it minimization). Explicitly:</p>
<div class="amsmath math notranslate nohighlight" id="equation-007990eb-272b-41b4-a591-9ae4bf4338c7">
<span class="eqno">(15.3)<a class="headerlink" href="#equation-007990eb-272b-41b4-a591-9ae4bf4338c7" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathcal{l} = -\log P_z\left[g_1\left(g_0(x)\right)\right] - \sum_i \log\left| \textrm{det}\left[\mathbf{J}_{g_i}\right]\right|
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the training point and when you take the gradient of the loss, it is with respect to the parameters of the bijectors.</p>
</section>
<section id="common-bijectors">
<h2><span class="section-number">15.4. </span>Common Bijectors<a class="headerlink" href="#common-bijectors" title="Permalink to this headline">#</a></h2>
<p>The choice of bijector functions is a fast changing area. I will thus only mention a few. You can of course use any bijective function or matrix, but these become inefficient at high-dimension due to the Jacobian calculation. One class of efficient bijectors are autoregressive bijectors. These have triangular Jacobians because each output dimension can only depend on the dimensions with a lower index. There are two variants: masked autoregressive flows (MAF)<span id="id5">[<a class="reference internal" href="#id75" title="George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, 2338‚Äì2347. 2017.">PPM17</a>]</span> and inverse autoregressive flows (IAF) <span id="id6">[<a class="reference internal" href="#id76" title="Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in neural information processing systems, 4743‚Äì4751. 2016.">KSJ+16</a>]</span>. MAFs are efficient at training and computing probabilities, but are slow for sampling from <span class="math notranslate nohighlight">\(P(x)\)</span>. IAFs are slow at training and computing probabilities but efficient for sampling. Wavenets combine the advantages of both <span id="id7">[<a class="reference internal" href="#id77" title="Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: a generative flow for raw audio. arXiv preprint arXiv:1811.02155, 2018.">KLS+18</a>]</span>. I‚Äôll mention one other common bijector which is not autoregressive: real non-volume preserving (RealNVPs) <span id="id8">[<a class="reference internal" href="#id78" title="Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.">DSDB16</a>]</span>. RealNVPs are less expressive than IAFs/MAFs, meaning they have trouble replicating complex distributions, but are efficient at all three tasks: training, sampling, and computing probabilities. Another interesting variant is the Glow bijector,which is able to expand the rank of the normalizing flow, for example going from a matrix to an RGB image <span id="id9">[<a class="reference internal" href="#id79" title="Hari Prasanna Das, Pieter Abbeel, and Costas J Spanos. Dimensionality reduction flows. arXiv preprint arXiv:1908.01686, 2019.">DAS19</a>]</span>. What are the equations for all these bijectors? Most are variants of standard neural network layers but with special rules about which outputs depend on which inputs.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Remember to add permute bijectors between autoregressive bijectors to ensure the dependence between dimensions is well-mixed.</p>
</div>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">15.5. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Permalink to this headline">#</a></h2>
<p>Click the ¬†<i aria-label="Launch interactive content" class="fas fa-rocket"></i>¬† above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
<p>The hidden code below imports the tensorflow probability package and other necessary packages. Note that the tensorflow probability package (<code class="docutils literal notranslate"><span class="pre">tfp</span></code>) is further broken into distributions (<code class="docutils literal notranslate"><span class="pre">tfd</span></code>) and bijectors (<code class="docutils literal notranslate"><span class="pre">tfb</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span> <span class="k">as</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">tfb</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="moon-example">
<h2><span class="section-number">15.6. </span>Moon Example<a class="headerlink" href="#moon-example" title="Permalink to this headline">#</a></h2>
<p>We‚Äôll start with a basic 2D example to learn the two moons distribution with a normalizing flow. Two moons is a common example dataset that is hard to cluster and model as a probability distribution.</p>
<p>When doing normalizing flows you have two options to implement them. You can do all the Jacobians, inverses, and likelihood calculations analytically and implement them in a normal ML framework like Jax, PyTorch, or TensorFlow. This is actually most common. The second option is to utilize a probability library that knows how to use bijectors and distributions. The packages for that are PYMC, TensorFlow Probability (which has a non-tensorflow JAX version confusingly), and Pyro (Pytorch). We‚Äôll use TensorFlow Probability for this work.</p>
<section id="generating-data">
<h3><span class="section-number">15.6.1. </span>Generating Data<a class="headerlink" href="#generating-data" title="Permalink to this headline">#</a></h3>
<p>In the code below, I set-up my imports and sample points which will be used for training. Remember, this code has nothing to do with normalizing flows ‚Äì it‚Äôs just to generate data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">moon_n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">ndim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">moon_n</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7fa538e3d070&gt;]
</pre></div>
</div>
<img alt="../_images/flows_7_1.png" src="../_images/flows_7_1.png" />
</div>
</div>
</section>
<section id="z-distribution">
<h3><span class="section-number">15.6.2. </span>Z Distribution<a class="headerlink" href="#z-distribution" title="Permalink to this headline">#</a></h3>
<p>Our Z distribution should always be as simple as possible. I‚Äôll create a 2D Gaussian with unit variance, no covariance, and centered at 0. I‚Äôll be using the tensorflow probability package for this example. The key new concept is that we organize our tensors that were <em>sampled</em> from a probability distribution in a specific way. We, by convention, make the first axes be the <strong>sample</strong> shape, the second axes be the <strong>batch</strong> shape, and the final axes be the <strong>event</strong> shape. The sample shape is the number of times we sampled from our distribution. It is a <em>shape</em> and not a single dimension because occasionally you‚Äôll want to organize your samples into some shape. The batch shape is a result of possibly multiple distributions batched together. For example, you might have 2 Gaussians, instead of a single 2D Gaussian. Finally, the event shape is the shape of a single sample from the distribution. This is overly complicated for our example, but you should be able to read information about the distribution now by understanding this nomenclature. You can find a tutorial on these <a class="reference external" href="https://www.tensorflow.org/probability/examples/Understanding_TensorFlow_Distributions_Shapes">shapes here</a> and more tutorials on <a class="reference external" href="https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability">tensorflow probability here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zdist</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span><span class="p">)</span>
<span class="n">zdist</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions.MultivariateNormalDiag &#39;MultivariateNormalDiag&#39; batch_shape=[] event_shape=[2] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<p>With our new understanding of shapes, you can see that this distribution has no <code class="docutils literal notranslate"><span class="pre">batch_shape</span></code> because there is only one set of parameters and the <code class="docutils literal notranslate"><span class="pre">event_shape</span></code> is <code class="docutils literal notranslate"><span class="pre">[2]</span></code> because it‚Äôs a 2D Gaussian. Let‚Äôs now sample from this distribution and view it</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">zdist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_11_0.png" src="../_images/flows_11_0.png" />
</div>
</div>
<p>As expected, our starting distribution looks nothing like are target distribution. Let‚Äôs demonstrate a bijector now. We‚Äôll implement the following bijector:</p>
<div class="math notranslate nohighlight">
\[
x = \vec{z} \times (1, 0.5)^T + (0.5, 0.25)
\]</div>
<p>This is bijective because the operations are element-wise and invertible. Rather than just write this out using operations like <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> or <code class="docutils literal notranslate"><span class="pre">*</span></code>, we‚Äôll use the built-in bijectors from TensorFlow probability. The reason we do this is that they have their inverses and Jacobian determinants already defined. We first create a bijector that scales by <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0.5]</span></code> and one then one that shifts by <code class="docutils literal notranslate"><span class="pre">[0.5,0.25]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shift_bij</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">Shift</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>
<span class="n">scale_bij</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">Scale</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="c1"># make composite via function convolution</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">shift_bij</span><span class="p">(</span><span class="n">scale_bij</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To now apply the change of variable formula, we create a <strong>transformed distribution</strong>. What is important about this choice is that we can compute likelihoods, probabilities, and sample from it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">td</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">zdist</span><span class="p">,</span> <span class="n">bijector</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
<span class="n">td</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tfp.distributions.TransformedDistribution &#39;chain_of_shift_of_scaleMultivariateNormalDiag&#39; batch_shape=[] event_shape=[2] dtype=float32&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_16_0.png" src="../_images/flows_16_0.png" />
</div>
</div>
<p>We show above the sampling from this new distribution. We can also plot it‚Äôs probability, which is impossible for a VAE-like model!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make points for grid</span>
<span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># compute P(x)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zgrid</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="c1"># plot and set axes limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_18_0.png" src="../_images/flows_18_0.png" />
</div>
</div>
</section>
<section id="the-normalizing-flow">
<h3><span class="section-number">15.6.3. </span>The Normalizing Flow<a class="headerlink" href="#the-normalizing-flow" title="Permalink to this headline">#</a></h3>
<p>Now we will build bijectors that are expressive enough to capture the moon distribution. I will use 3 sets of a MAF and permutation for 6 total bijectors. MAF‚Äôs have dense neural network layers in them, so I will also set the usual parameters for a neural network: dimension of hidden layer and activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">my_bijects</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># loop over desired bijectors and put into list</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="c1"># Syntax to make a MAF</span>
    <span class="n">anet</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">AutoregressiveNetwork</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="n">ndim</span><span class="p">,</span> <span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span>
    <span class="p">)</span>
    <span class="n">ab</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">MaskedAutoregressiveFlow</span><span class="p">(</span><span class="n">anet</span><span class="p">)</span>
    <span class="c1"># Add bijector to list</span>
    <span class="n">my_bijects</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ab</span><span class="p">)</span>
    <span class="c1"># Now permuate (!important!)</span>
    <span class="n">permute</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">Permute</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">my_bijects</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">permute</span><span class="p">)</span>
<span class="c1"># put all bijectors into one &quot;chain bijector&quot;</span>
<span class="c1"># that looks like one</span>
<span class="n">big_bijector</span> <span class="o">=</span> <span class="n">tfb</span><span class="o">.</span><span class="n">Chain</span><span class="p">(</span><span class="n">my_bijects</span><span class="p">)</span>
<span class="c1"># make transformed dist</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">zdist</span><span class="p">,</span> <span class="n">bijector</span><span class="o">=</span><span class="n">big_bijector</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we have not actually trained but we can still view our distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zgrid</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_22_0.png" src="../_images/flows_22_0.png" />
</div>
</div>
<p>You can already see that the distribution looks more complex than a Gaussian.</p>
</section>
<section id="id10">
<h3><span class="section-number">15.6.4. </span>Training<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h3>
<p>To train, we‚Äôll use TensorFlow Keras, which just handles computing derivatives and the optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># declare the feature dimension</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># create a &quot;placeholder&quot; function</span>
<span class="c1"># that will be model output</span>
<span class="n">log_prob</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># use input (feature) and output (log prob)</span>
<span class="c1"># to make model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>


<span class="c1"># define a loss</span>
<span class="k">def</span> <span class="nf">neg_loglik</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>
    <span class="c1"># losses always take in label, prediction</span>
    <span class="c1"># in keras. We do not have labels,</span>
    <span class="c1"># but we still need to accept the arg</span>
    <span class="c1"># to comply with Keras format</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">log_prob</span>


<span class="c1"># now we prepare model for training</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="n">neg_loglik</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One detail is that we have to create fake labels (zeros) because Keras expects there to always be training labels. Thus our loss we defined above (negative log-likelihood) takes in the labels but does nothing with them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">moon_n</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_27_0.png" src="../_images/flows_27_0.png" />
</div>
</div>
<p>Training looks reasonable. Let‚Äôs now see our distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="p">(</span>
    <span class="n">z1</span><span class="p">,</span>
    <span class="n">z2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">zpoints</span><span class="p">,</span> <span class="n">zpoints</span><span class="p">)</span>
<span class="n">zgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">z1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zgrid</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_29_0.png" src="../_images/flows_29_0.png" />
</div>
</div>
<p>Wow! We now can compute the probability of any point in this distribution. You can see there are some oddities that could be fixed with further training. One issue that cannot be overcome is the connection between the two curves ‚Äì it is not possible to get fully disconnected densities. This is because of our requirement that the bijectors are invertible and volume preserving ‚Äì you can only squeeze volume so far but cannot completely disconnect. Some work has been done on addressing this issue by adding sampling to the flow and this gives more expressive normalizing flows <span id="id11">[<a class="reference internal" href="#id89" title="Hao Wu, Jonas K√∂hler, and Frank No√©. Stochastic normalizing flows. arXiv preprint arXiv:2002.06707, 2020.">WKohlerNoe20</a>]</span>.</p>
<p>Finally, we‚Äôll sample from our model just to show that indeed it is generative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zsamples</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">moon_n</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zsamples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/flows_31_0.png" src="../_images/flows_31_0.png" />
</div>
</div>
</section>
</section>
<section id="relevant-videos">
<h2><span class="section-number">15.7. </span>Relevant Videos<a class="headerlink" href="#relevant-videos" title="Permalink to this headline">#</a></h2>
<section id="normalizing-flow-for-molecular-conformation">
<h3><span class="section-number">15.7.1. </span>Normalizing Flow for Molecular Conformation<a class="headerlink" href="#normalizing-flow-for-molecular-conformation" title="Permalink to this headline">#</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/XhAP2VNPVhg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">15.8. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>A normalizing flow builds up a probability distribution of <span class="math notranslate nohighlight">\(x\)</span> by starting from a known distribution on <span class="math notranslate nohighlight">\(z\)</span>. Bijective functions are used to go from <span class="math notranslate nohighlight">\(z\)</span> to <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>Bijectors are functions that keep the probability mass normalized and are used to go forward and backward (because they have well-defined inverses).</p></li>
<li><p>To find the probability distribution of <span class="math notranslate nohighlight">\(x\)</span> we use the change of variable formula, which requires a function inverse and Jacobian.</p></li>
<li><p>The bijector function has trainable parameters, which can be trained using a negative log-likelihood function.</p></li>
<li><p>Multiple bijectors can be chained together, but typically must include a permute bijector to swap the order of dimensions.</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">15.9. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id12">
<dl class="citation">
<dt class="label" id="id74"><span class="brackets"><a class="fn-backref" href="#id1">KPB20</a></span></dt>
<dd><p>Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: an introduction and review of current methods. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2020.</p>
</dd>
<dt class="label" id="id91"><span class="brackets"><a class="fn-backref" href="#id2">PNR+19</a></span></dt>
<dd><p>George Papamakarios, Eric Nalisnick, Danilo¬†Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. <em>arXiv preprint arXiv:1912.02762</em>, 2019.</p>
</dd>
<dt class="label" id="id90"><span class="brackets"><a class="fn-backref" href="#id3">PSM19</a></span></dt>
<dd><p>George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: fast likelihood-free inference with autoregressive flows. In <em>The 22nd International Conference on Artificial Intelligence and Statistics</em>, 837‚Äì848. PMLR, 2019.</p>
</dd>
<dt class="label" id="id108"><span class="brackets"><a class="fn-backref" href="#id4">HNJ+21</a></span></dt>
<dd><p>Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows: learning categorical distributions with normalizing flows. In <em>Third Symposium on Advances in Approximate Bayesian Inference</em>. 2021. URL: <a class="reference external" href="https://openreview.net/forum?id=fdsXhAy5Cp">https://openreview.net/forum?id=fdsXhAy5Cp</a>.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id5">PPM17</a></span></dt>
<dd><p>George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In <em>Advances in Neural Information Processing Systems</em>, 2338‚Äì2347. 2017.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id6">KSJ+16</a></span></dt>
<dd><p>Durk¬†P Kingma, Tim Salimans, Rafal Jozefowicz, Xi¬†Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In <em>Advances in neural information processing systems</em>, 4743‚Äì4751. 2016.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id7">KLS+18</a></span></dt>
<dd><p>Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: a generative flow for raw audio. <em>arXiv preprint arXiv:1811.02155</em>, 2018.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id8">DSDB16</a></span></dt>
<dd><p>Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. <em>arXiv preprint arXiv:1605.08803</em>, 2016.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id9">DAS19</a></span></dt>
<dd><p>Hari¬†Prasanna Das, Pieter Abbeel, and Costas¬†J Spanos. Dimensionality reduction flows. <em>arXiv preprint arXiv:1908.01686</em>, 2019.</p>
</dd>
<dt class="label" id="id89"><span class="brackets"><a class="fn-backref" href="#id11">WKohlerNoe20</a></span></dt>
<dd><p>Hao Wu, Jonas K√∂hler, and Frank No√©. Stochastic normalizing flows. <em>arXiv preprint arXiv:2002.06707</em>, 2020.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="VAE.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">14. </span>Variational Autoencoder</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../applied/QM9.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Predicting DFT Energies with GNNs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Andrew D. White<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">‚úï</button> <img id="wh-modal-img"> </div>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>