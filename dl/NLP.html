
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub üìñ" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>13. Deep Learning on Sequences &#8212; deep learning for molecules &amp; materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cb1cce99" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css?v=ffeaf963" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl/NLP';</script>
    <script src="../_static/custom.js?v=3f5092eb"></script>
    <link rel="canonical" href="https://dmol.pub/dl/NLP.html" />
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14. Variational Autoencoder" href="VAE.html" />
    <link rel="prev" title="12. Attention Layers" href="attention.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="deep learning for molecules & materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="deep learning for molecules & materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">A. Math Review</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/tensors-and-shapes.html">1. Tensors and Shapes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">B. Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ml/introduction.html">2. Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/regression.html">3. Regression &amp; Model Assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/classification.html">4. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/kernel.html">5. Kernel Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C. Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">6. Deep Learning Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">7. Standard Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">8. Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">9. Input Data &amp; Equivariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="Equivariant.html">10. Equivariant Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="xai.html">11. Explaining Predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention.html">12. Attention Layers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Deep Learning on Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="VAE.html">14. Variational Autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="flows.html">15. Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="molnets.html">16. Modern Molecular NNs</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">D. Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/QM9.html">17. Predicting DFT Energies with GNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied/MolGenerator.html">18. Generative RNN in Browser</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">E. Contributed Chapters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/e3nn_traj.html">19. Equivariant Neural Network for Predicting Trajectories</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">20. Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">F. Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../style.html">21. Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">22. Changelog</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/NLP.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/whitead/dmol-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/NLP.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dl/NLP.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning on Sequences</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-molecules-into-text">13.1. Converting Molecules into Text</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selfies">13.1.1. SELFIES</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#demo">13.1.1.1. Demo</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stereochemistry">13.1.2. Stereochemistry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ideas">13.1.3. Other Ideas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-chemical-bond">13.1.4. What is a chemical bond?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">13.2. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks">13.3. Recurrent Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-rnns">13.3.1. Generative RNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-padding">13.4. Masking &amp; Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-solubility-example">13.5. RNN Solubility Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">13.6. Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">13.6.1. Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-latent-space-for-design">13.7. Using the Latent Space for Design</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-materials-as-text">13.8. Representing Materials as Text</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">13.9. Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">13.10. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">13.11. Cited References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-on-sequences">
<h1><span class="section-number">13. </span>Deep Learning on Sequences<a class="headerlink" href="#deep-learning-on-sequences" title="Link to this heading">#</a></h1>
<p>Deep learning on sequences is part of a broader long-term effort in machine learning on sequences. <em>Sequences</em> is a broad term that includes text, integer sequences, DNA, and other ordered data. Deep learning on sequences often intersects with another field called natural language processing (NLP). NLP is a much broader field than deep learning, but there is quite a bit of overlap with sequence modeling.</p>
<p>We‚Äôll focus on the application of deep learning on sequences and NLP to molecules and materials. NLP in chemistry would at first appear to be a rich area, especially with the large amount of historic chemistry data existing only in plain text. However, the most work in this area has been on representations of molecules <em>as text</em> via the SMILES<span id="id1">[<a class="reference internal" href="../ml/introduction.html#id117" title="David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31‚Äì36, 1988.">Wei88</a>]</span> (and recently SELFIES <span id="id2">[<a class="reference internal" href="#id134" title="Mario Krenn, Florian H√§se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): a 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, nov 2020. URL: https://doi.org/10.1088/2632-2153/aba947, doi:10.1088/2632-2153/aba947.">KHN+20</a>]</span>) encoding. There is nothing <em>natural</em> about SMILES and SELFIES though, so we should be careful to discriminate between work on natural language (like identifying names of compounds in a research article) from predicting the solubility of a compound from its SMILES. I hope there will be more NLP in the area, but publishers prevent bulk access/ML on publications. Few corpuses (collections of natural language documents) exist for NLP on chemistry articles.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> and <a class="reference internal" href="attention.html"><span class="doc">Attention Layers</span></a>. After completing this chapter, you should be able to</p>
<ul class="simple">
<li><p>Define natural language processing and sequence modeling</p></li>
<li><p>Recognize and be able to encode molecules into SMILES or other string encodings</p></li>
<li><p>Understand RNNs and know some layer types</p></li>
<li><p>Construct RNNs in seq2seq or seq2vec configurations</p></li>
<li><p>Know the transformer architecture</p></li>
<li><p>Understand how design can be made easier in a latent space</p></li>
</ul>
</div>
<p>One advantage of working with molecules as text relative to graph neural networks (GNNs) is that existing ML frameworks have many more features for working with text, due to the strong connection between NLP and sequence modeling. Another reason is that it is easier to train generative models, because generating valid text is easier than generating valid graphs. You‚Äôll thus see generative/unsupervised learning of chemical space more often done with sequence models, whereas GNNs are typically better for supervised learning tasks and can incorporate spatial features (e.g., <span id="id3">[<a class="reference internal" href="gnn.html#id109" title="Ziyue Yang, Maghesree Chakraborty, and Andrew D White. Predicting chemical shifts with graph neural networks. bioRxiv, 2020.">YCW20</a>, <a class="reference internal" href="gnn.html#id106" title="Johannes Klicpera, Janek Gro√ü, and Stephan G√ºnnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations. 2020.">KGrossGunnemann20</a>]</span>). Outside of deep learning, graphical representations are in viewed as more robust than text encodings when used in methods like genetic algorithms and chemical space exploration <span id="id4">[<a class="reference internal" href="#id156" title="Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096‚Äì1108, 2019.">BFSV19</a>]</span>. NLP with sequence models can also be used to understand natural language descriptions of materials and molecules, which is essential for <em>materials</em> that are defined with more than just the molecular structure.</p>
<p>In sequence modeling, unsupervised learning is very common. We can predict the probability of the next token (word or character) in a sequence, like guessing the next word in a sentence. This does not require labels, because we only need examples of the sequences. It is also called pre-training, because it precedes training on sequences with labels. For chemistry this could be predicting the next atom in a SMILES string. For materials, this might be predicting the next word in a synthesis procedure. These pre-trained have statistical model of a language and can be fine-tuned (trained a second time) for a more specific task, like predicting if a molecule will bind to a protein.</p>
<p>Another common task in sequence modeling (inluding NLP) is to convert sequences into continuous vectors. This doesn‚Äôt always involve deep learning. Models that can embed sequences into a vector space are often called seq2vec or x2vec, where x might be molecule or synthesis procedure.</p>
<p>Finally, we often see <em>translation</em> tasks where we go from one sequence language to another. A sequence to sequence model (seq2seq) is similar to pre-training because it actually predicts probabilities for the output sequence.</p>
<section id="converting-molecules-into-text">
<h2><span class="section-number">13.1. </span>Converting Molecules into Text<a class="headerlink" href="#converting-molecules-into-text" title="Link to this heading">#</a></h2>
<p>Before we can begin to use neural networks, we need to convert molecules into text. Simplified molecular-input line-entry system (SMILES) is a de facto standard for converting molecules into a string. SMILES enables molecular structures to be correctly saved in spreadsheets, databases, and input to models that work on sequences like text. Here‚Äôs an example SMILES string: <code class="docutils literal notranslate"><span class="pre">CC(NC)CC1=CC=C(OCO2)C2=C1</span></code>. SMILES was crucial to the field of cheminformatics and is widely used today beyond deep learning. Some of the first deep learning work was with SMILES strings because of the ability to apply NLP models to SMILES strings.</p>
<p>Let us imagine SMILES as a function whose domain is molecular graphs (or some equivalent complete description of a molecule) and the image is a string. This can be thought of as an <strong>encoder</strong> that converts a molecular graph into a string. The SMILES encoder function is not surjective ‚Äì there are many strings that cannot be reached from decoding graphs. The SMILES encoder function is injective ‚Äì each graph has a different SMILES string. The inverse of this function, the SMILES <strong>decoder</strong>, cannot have the domain of all strings because some strings do not decode to valid molecular graphs. This is because of the syntax rules of SMILES. Thus, we can regard the domain to be restricted to <em>valid</em> SMILES string. In that case, the decoder is surjective ‚Äì all graphs are reachable via a SMILES string. The decoder is not injective ‚Äì multiple graphs can be reached by SMILES string.</p>
<p>This last point, the non-injectivity of a SMILES decoder, is a problem identified in database storage and retrieval of compounds. Since multiple SMILES strings map to the same molecular graph, it can happen that multiple entries in a database are actually the same molecule. One way around this is <strong>canonicalization</strong> which is a modification to the encoder to make a unique SMILES string. It can fail though <span id="id5">[<a class="reference internal" href="#id198" title="Noel M O‚ÄôBoyle. Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. Journal of cheminformatics, 4(1):1‚Äì14, 2012.">OBoyle12</a>]</span>. If we restrict ourselves to valid, canonical SMILES, then the SMILES decoder function is injective and surjective ‚Äì bijective.</p>
<aside class="margin sidebar">
<p class="sidebar-title">idempotent</p>
<p>One way to assess correctness of the canonicalization process is by testing the idempotent property of the SMILES encoder/decoder pair. That is, if we decode/encode a canonical SMILES string we should get back the same string.</p>
</aside>
<p>The difficulty of canonicalization and thus perceived weakness of SMILES in creating unique strings led (in part) to the creation of InChi strings. InChI is an alternative that is inherently canonical. InChI strings are typically longer and involve more tokens, which seems to affect their use in deep learning. InChI as a representation is often worse with the same amount of data vs SMILES.</p>
<p>If you‚Äôve read the previous chapters on equivariances (<a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a> and <a class="reference internal" href="Equivariant.html"><span class="doc">Equivariant Neural Networks</span></a>), a natural question is if SMILES is permutation invariant. That is, if you change the order of atoms in the molecular graph that has no effect on chemistry, is the SMILES string identical? Yes, if you use the canonical SMILES. So in a supervised setting, using canonical SMILES gives an atom ordering permutation invariant neural network because the representation <em>will not</em> be permuted after canonicalization. Be careful; you should not trust that SMILES you find in a datset are canonical .</p>
<section id="selfies">
<h3><span class="section-number">13.1.1. </span>SELFIES<a class="headerlink" href="#selfies" title="Link to this heading">#</a></h3>
<p>Recent work from Krenn et al. developed an alternative approach to SMILES called SELF-referencIng Embedded Strings (SELFIES)<span id="id6">[<a class="reference internal" href="#id134" title="Mario Krenn, Florian H√§se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): a 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, nov 2020. URL: https://doi.org/10.1088/2632-2153/aba947, doi:10.1088/2632-2153/aba947.">KHN+20</a>]</span>. Every string is a valid molecule. Note that the characters in SELFIES are not all ASCII characters, so it‚Äôs not like every sentence encodes a molecule (would be cool though). SELFIES is an excellent choice for generative models because any SELFIES string automatically decodes to a valid molecule. SELFIES, as of 2021, is not directly canonicalized though and thus is not permutation invariant by itself. However, if you add canonical SMILES as an intermediate step, then SELFIES are canonical. It seems that models which output a molecule (generative or supervised) benefit from using SELFIES instead of SMILES because the model does not need to learn how to make valid strings ‚Äì all strings are already valid SELFIES <span id="id7">[<a class="reference internal" href="#id135" title="Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Decimer: towards deep learning for chemical image recognition. Journal of Cheminformatics, 12(1):1‚Äì9, 2020.">RZS20</a>]</span>. This benefit is less clear in supervised learning and no difference has been observed empirically<span id="id8">[<a class="reference internal" href="#id138" title="Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.">CGR20</a>]</span>. Here‚Äôs a blog post giving an <a class="reference external" href="https://aspuru.substack.com/p/molecular-graph-representations-and">overview of SELFIES and its applications</a>.</p>
<section id="demo">
<h4><span class="section-number">13.1.1.1. </span>Demo<a class="headerlink" href="#demo" title="Link to this heading">#</a></h4>
<p>You can get a sense for SMILES and SELFIES in this <a class="reference external" href="https://whitead.github.io/molecule-dream/">demo page</a> that uses a RNN (discussed below) to generate SMILES and SELFIES strings.</p>
</section>
</section>
<section id="stereochemistry">
<h3><span class="section-number">13.1.2. </span>Stereochemistry<a class="headerlink" href="#stereochemistry" title="Link to this heading">#</a></h3>
<p>SMILES and SELFIES can treat stereoisomers, but there are a few complications. <code class="docutils literal notranslate"><span class="pre">rdkit</span></code>, the dominant Python package, <a class="reference external" href="https://github.com/rdkit/rdkit/issues/3220">cannot treat non-tetrahedral chiral centers with SMILES</a> as of 2022. For example, even though SMILES according to its specification can correctly distinguish cisplatin and transplatin, the implementation of SMILES in <code class="docutils literal notranslate"><span class="pre">rdkit</span></code> cannot. Other examples of chirality that are present in the SMILES specification but not implementations are planar and axial chirality. SELFIES relies on SMILES (most often the <code class="docutils literal notranslate"><span class="pre">rdkit</span></code> implementation) and thus is also susceptible to this problem. This is an issue for any organometallic compounds. In organic chemistry though, most chirality is tetrahedral and correctly treated by <code class="docutils literal notranslate"><span class="pre">rdkit</span></code>.</p>
</section>
<section id="other-ideas">
<h3><span class="section-number">13.1.3. </span>Other Ideas<a class="headerlink" href="#other-ideas" title="Link to this heading">#</a></h3>
<p>Recent work by Kim et al. <span id="id9">[<a class="reference internal" href="#id257" title="Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. arXiv preprint arXiv:2207.02505, 2022.">KNM+22</a>]</span> has shown that we may actually be able to directly insert the graph as a sequence into a sequence neural network without needing to make a decision like using SMILES or SELFIES. They basically add a special character/embedding for noting if a piece of a graph is a node or edge.</p>
</section>
<section id="what-is-a-chemical-bond">
<h3><span class="section-number">13.1.4. </span>What is a chemical bond?<a class="headerlink" href="#what-is-a-chemical-bond" title="Link to this heading">#</a></h3>
<p>More broadly, the idea of a chemical bond is a concept created by chemists <span id="id10">[<a class="reference internal" href="#id219" title="Philip Ball. Beyond the bond. Nature, 469(7328):26‚Äì28, 2011.">Bal11</a>]</span>. You cannot measure the existence of a chemical bond in the lab and it is not some quantum mechanical operator with an observable. There are certain molecules which cannot be represented by classic single,double,triple,aromatic bonded representations, like ferrocene or diborane. This bleeds over to text encoding of a molecule where the bonding topology doesn‚Äôt map neatly to bond order. The specific issue this can cause is that multiple unique molecules may appear to have the same encoding (non-injective). In situations like this, it is probably better to just work with the exact 3D coordinates and then bond order or type is less important than distance between atoms.</p>
</section>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">13.2. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Link to this heading">#</a></h2>
<p>Click the ¬†<i aria-label="Launch interactive content" class="fas fa-rocket"></i>¬† above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
</section>
<section id="recurrent-neural-networks">
<h2><span class="section-number">13.3. </span>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Link to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>String is a synonym for sequence here. Character and symbol are synonyms for token (single element of the string).</p>
</aside>
<p>Recurrent neural networks (RNN) have been by far the most popular approach to working with molecular strings. RNNs have a critical property that they can have different length input sequences, making it appropriate for SMILES or SELFIES which both have variable length. RNNs have recurrent layers that consume an input sequence element-by-element. Consider an input sequence <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> which is composed of a series of vectors (recall that characters or words can be represented with one-hot or embedding vectors) <span class="math notranslate nohighlight">\(\mathbf{X} = \left[\vec{x}_0, \vec{x}_1,\ldots,\vec{x}_L\right]\)</span>. The RNN layer function is binary and takes as input the <span class="math notranslate nohighlight">\(i\)</span>th element of the input sequence and the output from the <span class="math notranslate nohighlight">\(i - 1\)</span> layer function. You can write it as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d9bed0a7-8916-4568-961a-afaf5dc515da">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-d9bed0a7-8916-4568-961a-afaf5dc515da" title="Permalink to this equation">#</a></span>\[\begin{equation}
f(f\ldots f(\vec{x}_0,\vec{0}), \vec{x}_1), \vec{x}_2)\ldots \vec{x}_L)
\end{equation}\]</div>
<p>Commonly we would like to actually see and look at the these intermediate outputs from the layer function <span class="math notranslate nohighlight">\(f_4(\vec{x}_4, f_3(\ldots)) = \vec{h}_4\)</span>. These <span class="math notranslate nohighlight">\(\vec{h}\)</span>s are called the hidden state because of the connection between RNNs and Markov State Models. We can <strong>unroll</strong> our picture of an RNN to be:</p>
<figure class="align-default" id="rnn">
<a class="reference internal image-reference" href="../_images/rnn.jpg"><img alt="Unrolled picture of RNN" src="../_images/rnn.jpg" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">Unrolled picture of RNN.</span><a class="headerlink" href="#rnn" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>where the initial hidden state is assumed to be <span class="math notranslate nohighlight">\(\vec{0}\)</span>, but could be trained. The output at the end is shown as <span class="math notranslate nohighlight">\(\vec{y}\)</span>. <em>Notice there are no subscripts on <span class="math notranslate nohighlight">\(f\)</span> because we use the same function and weights at each step</em>. This re-use of weights makes the choice of parameter number independent of input lengths, which is also necessary to make the RNN accommodate arbitrary length input sequences. It should be noted that the length of <span class="math notranslate nohighlight">\(\vec{y}\)</span> may be a function of the input length, so that the <span class="math notranslate nohighlight">\(\vec{h}_i\)</span> may be increasing in length at each step to enable an output <span class="math notranslate nohighlight">\(\vec{y}\)</span>. Some diagrams of RNNs will show that by indicating a growing output sequence as an additional output from <span class="math notranslate nohighlight">\(f(\vec{x}_i, h_{i-1})\)</span>.</p>
<p>Interestingly, the form of <span class="math notranslate nohighlight">\(f(\vec{x}, \vec{h})\)</span> is quite flexible based on the discussion above. There have been hundreds of ideas for the function <span class="math notranslate nohighlight">\(f\)</span> and it is problem dependent. The two most common are long short-term memory (LSTM) units and gated recurrent unit (GRU). You can spend quite a bit of time trying to reason about <a class="reference external" href="http://d2l.ai/chapter_recurrent-modern/gru.html">these functions</a>, understanding how <a class="reference external" href="http://d2l.ai/chapter_recurrent-neural-networks/bptt.html">gradients propagate nicely through them</a>, and there is an analogy about how they are inspired by human memory. Ultimately, they are used because they perform well and are widely-implemented so we do not need to spend much time on these details. The main thing to know is that GRUs are simpler and faster, but LSTMs seem to be better at more difficult sequences. Note that <span class="math notranslate nohighlight">\(\vec{h}\)</span> is typically 1-3 different quantities in modern implementations. Another details is the word <strong>units</strong>. Units are like the hidden state dimension, but because the hidden state could be multiple quantities (e.g., LSTM) we do not call it dimension.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Actually, they are not used so much anymore because transformers seem to be a direct replacement for RNNs.</p>
</aside>
<p>The RNN layer allows us to input an arbitrary length sequence and outputs a label which could depend on the length of the input sequence. You can imagine that this could be used for regression or classification. <span class="math notranslate nohighlight">\(\hat{y}\)</span> would be a scalar. Or you could take the output from an RNN layer into an MLP to get a class.</p>
<section id="generative-rnns">
<h3><span class="section-number">13.3.1. </span>Generative RNNs<a class="headerlink" href="#generative-rnns" title="Link to this heading">#</a></h3>
<p>An interesting use case for an RNN is in unsupervised generative models, where we try to predict new examples. This means that we‚Äôre trying to learn <span class="math notranslate nohighlight">\(P(\mathbf{X})\)</span> <span id="id11">[<a class="reference internal" href="#id136" title="Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120‚Äì131, 2018.">SKTW18</a>]</span>. With a generative RNN, we predict the sequence one symbol at a time by conditioning on a growing sequence. This is called <strong>autoregressive</strong> generation.</p>
<div class="amsmath math notranslate nohighlight" id="equation-79273c58-269c-40d1-a98d-9f83f0575988">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-79273c58-269c-40d1-a98d-9f83f0575988" title="Permalink to this equation">#</a></span>\[\begin{equation}
P(\mathbf{X}) = \prod P(\vec{x}_L | \vec{x}_{L - 1}, \vec{x}_{L - 2}, \ldots,\vec{x}_0)\ldots P(\vec{x}_1 | \vec{x}_0) P(\vec{x}_0))
\end{equation}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is also called <strong>self-supervised</strong> instead of unsupervised learning. The distinction is that we‚Äôre creating labels by chopping up our training data ‚Äì so it is supervised. But it‚Äôs not quite supervised because labels do not need to be supplied.</p>
</aside>
<p>The RNN is trained to take as input a sequence and output the probability for the next character. Our network is trained to be this conditional probability: <span class="math notranslate nohighlight">\(P(\vec{x}_i | \vec{x}_{L - i}, \vec{x}_{L - i}, \ldots, \vec{x}_0)\)</span>. What about the <span class="math notranslate nohighlight">\(P(\vec{x}_0)\)</span> term? Typically we just <em>pick</em> what the first character should be. Or, we could create an artificial ‚Äústart‚Äù character that marks the beginning of a sequence (typically <code class="docutils literal notranslate"><span class="pre">0</span></code>) and always choose that.</p>
<p>We can train the RNN to agree with <span class="math notranslate nohighlight">\(P(\vec{x}_i | \vec{x}_{L - i}, \vec{x}_{L - i}, \ldots, \vec{x}_0)\)</span> by taking an arbitrary sequence <span class="math notranslate nohighlight">\(\vec{x}\)</span> and choosing a split point <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> and training on the proceeding sequence elements. This is just multi-class classification. The number of classes is the number of available characters and our model should output a probability vector across the classes. Recall the loss for this cross-entropy.</p>
<p>When doing this process with SMILES an obvious way to judge success would be if the generated sequences are valid SMILES strings. This at first seems reasonable and was used as a benchmark for years in this topic. However, this is a low-bar: we can find valid SMILES in much more efficient ways. You can download 77 million SMILES <span id="id12">[<a class="reference internal" href="#id138" title="Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.">CGR20</a>]</span> and you can find vendors that will give you a multi-million entry database of purchasable molecules. You can also just use SELFIES and then an untrained RNN will generate only valid strings, since SELFIES is bijective. A more interesting metric is to assess if your generated molecules are in the same region of chemical space as the training data<span id="id13">[<a class="reference internal" href="#id136" title="Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120‚Äì131, 2018.">SKTW18</a>]</span>. I believe though that generative RNNs are relatively poor compared with other generative models in 2021. They are still strong though when composed with other architectures, like VAEs <span id="id14">[<a class="reference internal" href="#id137" title="Rafael G√≥mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos√© Miguel Hern√°ndez-Lobato, Benjam√≠n S√°nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al√°n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268‚Äì276, 2018.">GomezBWD+18</a>]</span> or encoder/decoder <span id="id15">[<a class="reference internal" href="#id135" title="Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Decimer: towards deep learning for chemical image recognition. Journal of Cheminformatics, 12(1):1‚Äì9, 2020.">RZS20</a>]</span>.</p>
<p>You can see a worked out example in <a class="reference internal" href="../applied/MolGenerator.html"><span class="doc">Generative RNN in Browser</span></a>.</p>
</section>
</section>
<section id="masking-padding">
<h2><span class="section-number">13.4. </span>Masking &amp; Padding<a class="headerlink" href="#masking-padding" title="Link to this heading">#</a></h2>
<p>As in our <a class="reference internal" href="gnn.html"><span class="doc">Graph Neural Networks</span></a> chapter, we run into issues with variable length inputs. The easiest and most compute efficient way to treat this is to pad (and/or trim) all strings to be the same length, making it easy to batch examples. A memory efficient way is to not batch and either batch gradients as a separate step or trim your sequences into subsequences and save the RNN hidden-state between them. Due to the way that NVIDIA has written RNN kernels, padding should always be done on the right (sequences all begin at index 0). The character used for padding is typically 0. Don‚Äôt forget, we will always first convert our string characters to integers corresponding to indices of our vocabulary (see <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a>). Thus, remember to make sure that the index 0 should be reserved for padding.</p>
<p>Masking is used for two things. Masking is used to ensure that the padded values are not accidentally considered in training. This is framework dependent and you can read about <a class="reference external" href="https://keras.io/guides/understanding_masking_and_padding/">Keras here</a>, which is what we‚Äôll use. The second use for masking is to do element-by-element training like the generative RNN. We train each time with a shorter mask, enabling it to see more of the sequence. This prevents you from needing to slice-up the training examples into many shorter sequences. This idea of a right-mask that prevents the model for using characters farther in the sequence is sometimes called <strong>causal masking</strong> because we‚Äôre preventing characters from the ‚Äúfuture‚Äù affecting the model.</p>
</section>
<section id="rnn-solubility-example">
<h2><span class="section-number">13.5. </span>RNN Solubility Example<a class="headerlink" href="#rnn-solubility-example" title="Link to this heading">#</a></h2>
<p>Let‚Äôs revisit our solubility example from before. We‚Äôll use a GRU to <em>encode</em> the SMILES string into a vector and then apply a dense layer to get a scalar value for solubility. Let‚Äôs revisit the solubility AqSolDB<span id="id16">[<a class="reference internal" href="../ml/regression.html#id26" title="Murat Cihan Sorkun, Abhishek Khetan, and S√ºleyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. Sci. Data, 6(1):143, 2019. doi:10.1038/s41597-019-0151-1.">SKE19</a>]</span> dataset from <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a>. Recall it has about 10,000 unique compounds with measured solubility in water (label) and their SMILES strings. Many of the steps below are explained in the <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> chapter that introduces Keras and the principles of building a deep model.</p>
<p>I‚Äôve hidden the cell below which sets-up our imports and shown a few rows of the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dmol</span>

<span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv&quot;</span>
<span class="p">)</span>
<span class="n">features_start_at</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">soldata</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;MolWt&quot;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soldata</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Name</th>
      <th>InChI</th>
      <th>InChIKey</th>
      <th>SMILES</th>
      <th>Solubility</th>
      <th>SD</th>
      <th>Ocurrences</th>
      <th>Group</th>
      <th>MolWt</th>
      <th>...</th>
      <th>NumRotatableBonds</th>
      <th>NumValenceElectrons</th>
      <th>NumAromaticRings</th>
      <th>NumSaturatedRings</th>
      <th>NumAliphaticRings</th>
      <th>RingCount</th>
      <th>TPSA</th>
      <th>LabuteASA</th>
      <th>BalabanJ</th>
      <th>BertzCT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>A-3</td>
      <td>N,N,N-trimethyloctadecan-1-aminium bromide</td>
      <td>InChI=1S/C21H46N.BrH/c1-5-6-7-8-9-10-11-12-13-...</td>
      <td>SZEMGTQCPRNXEG-UHFFFAOYSA-M</td>
      <td>[Br-].CCCCCCCCCCCCCCCCCC[N+](C)(C)C</td>
      <td>-3.616127</td>
      <td>0.0</td>
      <td>1</td>
      <td>G1</td>
      <td>392.510</td>
      <td>...</td>
      <td>17.0</td>
      <td>142.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>158.520601</td>
      <td>0.000000e+00</td>
      <td>210.377334</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A-4</td>
      <td>Benzo[cd]indol-2(1H)-one</td>
      <td>InChI=1S/C11H7NO/c13-11-8-5-1-3-7-4-2-6-9(12-1...</td>
      <td>GPYLCFQEKPUWLD-UHFFFAOYSA-N</td>
      <td>O=C1Nc2cccc3cccc1c23</td>
      <td>-3.254767</td>
      <td>0.0</td>
      <td>1</td>
      <td>G1</td>
      <td>169.183</td>
      <td>...</td>
      <td>0.0</td>
      <td>62.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>29.10</td>
      <td>75.183563</td>
      <td>2.582996e+00</td>
      <td>511.229248</td>
    </tr>
    <tr>
      <th>2</th>
      <td>A-5</td>
      <td>4-chlorobenzaldehyde</td>
      <td>InChI=1S/C7H5ClO/c8-7-3-1-6(5-9)2-4-7/h1-5H</td>
      <td>AVPYQKSLYISFPO-UHFFFAOYSA-N</td>
      <td>Clc1ccc(C=O)cc1</td>
      <td>-2.177078</td>
      <td>0.0</td>
      <td>1</td>
      <td>G1</td>
      <td>140.569</td>
      <td>...</td>
      <td>1.0</td>
      <td>46.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>17.07</td>
      <td>58.261134</td>
      <td>3.009782e+00</td>
      <td>202.661065</td>
    </tr>
    <tr>
      <th>3</th>
      <td>A-8</td>
      <td>zinc bis[2-hydroxy-3,5-bis(1-phenylethyl)benzo...</td>
      <td>InChI=1S/2C23H22O3.Zn/c2*1-15(17-9-5-3-6-10-17...</td>
      <td>XTUPUYCJWKHGSW-UHFFFAOYSA-L</td>
      <td>[Zn++].CC(c1ccccc1)c2cc(C(C)c3ccccc3)c(O)c(c2)...</td>
      <td>-3.924409</td>
      <td>0.0</td>
      <td>1</td>
      <td>G1</td>
      <td>756.226</td>
      <td>...</td>
      <td>10.0</td>
      <td>264.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6.0</td>
      <td>120.72</td>
      <td>323.755434</td>
      <td>2.322963e-07</td>
      <td>1964.648666</td>
    </tr>
    <tr>
      <th>4</th>
      <td>A-9</td>
      <td>4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth...</td>
      <td>InChI=1S/C25H30N2O4/c1-5-20(26(10-22-14-28-22)...</td>
      <td>FAUAZXVRLVIARB-UHFFFAOYSA-N</td>
      <td>C1OC1CN(CC2CO2)c3ccc(Cc4ccc(cc4)N(CC5CO5)CC6CO...</td>
      <td>-4.662065</td>
      <td>0.0</td>
      <td>1</td>
      <td>G1</td>
      <td>422.525</td>
      <td>...</td>
      <td>12.0</td>
      <td>164.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>56.60</td>
      <td>183.183268</td>
      <td>1.084427e+00</td>
      <td>769.899934</td>
    </tr>
  </tbody>
</table>
<p>5 rows √ó 26 columns</p>
</div></div></div>
</div>
<p>We‚Äôll extract our labels and convert SMILES into padded characters. We make use of a <strong>tokenizer</strong>, which is essentially a look-up table for how to go from the characters in a SMILES string to integers. To make our model run faster, I will filter out very long SMILES strings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># filter out long smiles</span>
<span class="n">smask</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">96</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed </span><span class="si">{</span><span class="n">soldata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">smask</span><span class="p">)</span><span class="si">}</span><span class="s2"> long SMILES strings&quot;</span><span class="p">)</span>
<span class="n">filtered_soldata</span> <span class="o">=</span> <span class="n">soldata</span><span class="p">[</span><span class="n">smask</span><span class="p">]</span>

<span class="c1"># build a character-level tokenizer</span>
<span class="c1"># reserve 0 for padding</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filtered_soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">))</span>
<span class="n">char_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">))}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">smiles_list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[[</span><span class="n">char_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">smiles_list</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Removed 285 long SMILES strings
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="c1"># convert SMILES to integer sequences and pad</span>
<span class="n">seqs</span> <span class="o">=</span> <span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">filtered_soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">)</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">seqs</span><span class="p">)</span>
<span class="n">padded_seqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">seqs</span><span class="p">),</span> <span class="n">max_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seqs</span><span class="p">):</span>
    <span class="n">padded_seqs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span> <span class="o">=</span> <span class="n">s</span>

<span class="c1"># build datasets</span>
<span class="n">seq_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">padded_seqs</span><span class="p">)</span>
<span class="n">sol_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">filtered_soldata</span><span class="o">.</span><span class="n">Solubility</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">TensorDataset</span><span class="p">(</span><span class="n">seq_tensor</span><span class="p">[:</span><span class="n">split</span><span class="p">],</span> <span class="n">sol_tensor</span><span class="p">[:</span><span class="n">split</span><span class="p">]),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span>
<span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">TensorDataset</span><span class="p">(</span><span class="n">seq_tensor</span><span class="p">[</span><span class="n">split</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">split</span><span class="p">],</span> <span class="n">sol_tensor</span><span class="p">[</span><span class="n">split</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">split</span><span class="p">]),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">TensorDataset</span><span class="p">(</span><span class="n">seq_tensor</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">split</span> <span class="p">:],</span> <span class="n">sol_tensor</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">split</span> <span class="p">:]),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We‚Äôre now ready to build our model. We will just use an embedding then RNN and some dense layers to get to a final predicted solubility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SolubilityRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># padding_idx=0 zeros out the embedding for padding tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">SolubilityRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SolubilityRNN(
  (embedding): Embedding(128, 16, padding_idx=0)
  (gru): GRU(16, 32, batch_first=True)
  (fc1): Linear(in_features=32, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>Now we‚Äôll compile our model and train it. This is a regression problem, so we use mean squared error for our loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
            <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">val_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span>

    <span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
    <span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/58cfd63073123e198b2cb20f50bdb1290baf16b6696632153df2486e6bffbffb.png" src="../_images/58cfd63073123e198b2cb20f50bdb1290baf16b6696632153df2486e6bffbffb.png" />
</div>
</div>
<p>As usual, we could keep training and I encourage you to explore adding regularization or modifying the architecture. Let‚Äôs now see how the test data looks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate on test data</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
        <span class="n">yhat</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="n">test_y</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot test data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;correlation = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span><span class="w"> </span><span class="n">yhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;loss = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Testing Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/271b1cdbdbc5ee576e330b5bc9fb4e1f5410d3bc3a64dcfed8547232e652eb36.png" src="../_images/271b1cdbdbc5ee576e330b5bc9fb4e1f5410d3bc3a64dcfed8547232e652eb36.png" />
</div>
</div>
<p>Linear regression from <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a> still wins, but this demonstrates the use of an RNN for this task.</p>
</section>
<section id="transformers">
<h2><span class="section-number">13.6. </span>Transformers<a class="headerlink" href="#transformers" title="Link to this heading">#</a></h2>
<p>Transformers have been well-established now as the current state of the art for language modeling tasks. The transformer architecture is actually just multi-headed attention blocks repeated in multiple layers. The paper describing the architecture was quite a breakthrough. At the time, the best models used convolutions, recurrence, attention and encoder/decoder. The paper title was ‚Äúattention is all you need‚Äù and that is basically the conclusion <span id="id17">[<a class="reference internal" href="attention.html#id55" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 5998‚Äì6008. 2017.">VSP+17</a>]</span>. They found that multi-head attention (including self-attention) was what mattered and this led to <strong>transformers</strong>. Transformers are simple and <em>scalable</em> because each layer is nearly the same operation. This has led to simple ‚Äúscaling-up the language model‚Äù resulting in things like GPT-3, which has billions of parameters and cost millions of dollars to train. GPT-3 is also surprisingly good and versatile. The single model is able to answer questions, describe computer code, translate languages, and infer recipe instructions for cookies. I highly recommend reading the paper, it‚Äôs quite interesting<span id="id18">[<a class="reference internal" href="#id139" title="Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and others. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.">BMR+20</a>]</span>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Considering the whole sequence simultaneously is also possible with bi-directional RNNs that read a sequence simultaneously from both ends ‚Äì meaning context before and after a missing character can be used for training.</p>
</aside>
<p>There are two principles from the transformer that interest us. One is of course that it is a simple and effective replacement for RNNs. The second is that the transformer considers the whole sequence simultaneously. This has a few consequences. The first is that it is again input size dependent. However, we can pad and mask to get around that. The second consequence is that the self-supervised/unsupervised training can be more interesting than just predict the next character in the string. Instead, we can randomly delete characters and ask the transformer to infer the missing character. This is how transformers are typically ‚Äúpre-trained‚Äù ‚Äì by feeding a bunch of masked sequences to teach the transformer the language. Then, if desired, the transformer can be refined with labels on your specific task. Transformers and their pre-training training procedure have led to pre-trained chemistry specific models that can be downloaded and used immediately on chemistry data, like ChemBERTa <span id="id19">[<a class="reference internal" href="#id138" title="Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.">CGR20</a>]</span>. These pre-trained models have been trained on 77 million molecules and so should already have some ‚Äúintuition‚Äù about molecular structures and they indeed do well on supervised learning tasks.</p>
<section id="architecture">
<h3><span class="section-number">13.6.1. </span>Architecture<a class="headerlink" href="#architecture" title="Link to this heading">#</a></h3>
<p>The transformer is fundamentally made-up of layers of multi-head attention blocks as discussed in <a class="reference internal" href="attention.html"><span class="doc">Attention Layers</span></a>. You can get a detailed overview of the <a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">transformer architecture here</a>.  The overall architecture is an encoder/decoder like seen in <a class="reference internal" href="VAE.html"><span class="doc">Variational Autoencoder</span></a>. Like the variational autoencoder, the decoder portion can be discarded and only the encoder is used for supervised tasks. Thus, you might <strong>pre-train</strong> the encoder/decoder with self-supervised training (strings with withheld characters) on a large dataset without labels and then use only the encoder for a regression tasks with a smaller dataset.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Some have recently argued that convolutions might be as effective as transformers with some tuning. If substantiated, this could upend NLP because convolutions are simpler to understand, parallelize, and interpret. <span id="id20">[<a class="reference internal" href="#id161" title="Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are pre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322, 2021.">TDG+21</a>]</span></p>
</aside>
<p>What exactly is going in and out of the encoder/decoder? The transformer is an example of a sequence to sequence (seq2seq) model and the most obvious interpretation is translating between two languages like English to French. The encoder takes in English and the decoder produces French. Or maybe SMILES to IUPAC name. However, that requires ‚Äúlabels‚Äù (the paired sequence). To do self-supervised training pre-training, we need the input to the encoder to be a sequence missing some values and the decoder output to be the same sequence with probabilities for each position values filled in. This is called <strong>masked</strong> self-supervised training. If you pre-train in this way, you can do two tasks with your pre-trained encoder/decoder. You can use the encoder alone as a way to embed a string into real numbers and then a downstream task like predicting a molecule‚Äôs enthalpy of formation from its SMILES string. The other way to use a model trained this way is for autoregressive generation. The input might be a few characters or a <em>prompt</em> <span id="id21">[<a class="reference internal" href="#id141" title="Laria Reynolds and Kyle McDonell. Prompt programming for large language models: beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350, 2021.">RM21</a>]</span> specifically crafted like a question. This is similar the generative RNN, although it allows more flexibility.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Self-supervised training is not just for transformers! It has been successfully applied to graph neural networks as well <span id="id22">[<a class="reference internal" href="#id145" title="Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molclr: molecular contrastive learning of representations via graph neural networks. arXiv preprint arXiv:2102.10056, 2021.">WWCF21</a>]</span>.</p>
</aside>
<p>There are many details to transformers and ‚Äúhand-tuned‚Äù hyperparameters. Examples in modern transformers are layer normalizations (similar to batch normalization), embeddings, dropout, weight decay, learning rate decay, and position information encoding <span id="id23">[<a class="reference internal" href="#id140" title="Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.">LOG+19</a>]</span>. Position information is quite an interesting topic ‚Äì you need to include the location of a token (character) in its embedding. Was it the first character or last character?  This is key because when you compute the attention between tokens, the relative location is probably important. Some recent promising work proposed a kind of phase/amplitude split, where the position is the phase and the amplitude is the embedding called rotary positional encodings<span id="id24">[<a class="reference internal" href="#id160" title="Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.">SLP+21</a>]</span>.</p>
<p>If you would like to see how to implement a real transformer with most of these details, take a look at this <a class="reference external" href="https://keras.io/examples/generative/text_generation_with_miniature_gpt/">Keras tutorial</a>. Because transformers are so tightly coupled with pre-training, there has been a great deal of effort in pre-training models. Aside from <a class="reference external" href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a>, a general model pre-trained on an enormous corpus of billions of sequences from multiple languages, there are many language specific pre-trained models. <a class="reference external" href="https://huggingface.co/">Hugging Face</a> is a company and API that hosts pre-trained transformers for specific language models like Chinese language, XML, SMILES, or question and answer format. These can be quickly downloaded and utilized, enabling rapid use of state-of-the art language models.</p>
</section>
</section>
<section id="using-the-latent-space-for-design">
<h2><span class="section-number">13.7. </span>Using the Latent Space for Design<a class="headerlink" href="#using-the-latent-space-for-design" title="Link to this heading">#</a></h2>
<p>One of the most interesting applications of these encoder/decoder seq2seq models in chemistry is their use for doing optimal design of a molecule. We pre-train an encoder/decoder pair with masking. The encoder brings our molecule to a continuous representation (seq2vec). Then we can do regression in this vector space for whatever property we would like (e.g., solubility). Then we can optimize this regressed model, finding an input vector that is a minimum or maximum, and finally convert that input vector into a molecule using the decoder <span id="id25">[<a class="reference internal" href="#id137" title="Rafael G√≥mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos√© Miguel Hern√°ndez-Lobato, Benjam√≠n S√°nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al√°n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268‚Äì276, 2018.">GomezBWD+18</a>]</span>. The vector space output by the encoder is called the <strong>latent space</strong> like we saw in <a class="reference internal" href="VAE.html"><span class="doc">Variational Autoencoder</span></a>. Of course, this works for RNN seq2seq models, transformers, or convolutions.</p>
</section>
<section id="representing-materials-as-text">
<h2><span class="section-number">13.8. </span>Representing Materials as Text<a class="headerlink" href="#representing-materials-as-text" title="Link to this heading">#</a></h2>
<p>Materials are an interesting problem for deep learning because they are not defined by a single molecule. There can be information like the symmetry group or components/phases for a composite material. This creates a challenge for modeling, especially for real materials that have complexities like annealing temperature, additives, and age. From a philosophical point of view, a material is defined by how it was constructed. Practically that means a material is defined by the text describing its synthesis <span id="id26">[<a class="reference internal" href="#id148" title="Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning for molecular and materials science. Nature, 559(7715):547‚Äì555, 2018.">BDC+18</a>]</span>. This is an idea taken to its extreme in Tshitoyan et al. <span id="id27">[<a class="reference internal" href="#id142" title="Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95‚Äì98, 2019.">TDW+19</a>]</span> who found success in representing thermoelectrics via the text describing their synthesis <span id="id28">[<a class="reference internal" href="#id149" title="Matthew C Swain and Jacqueline M Cole. Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. Journal of chemical information and modeling, 56(10):1894‚Äì1904, 2016.">SC16</a>]</span>. This work is amazing to me because they had to manually collect papers (publishers do not allow ML/bulk download on articles) and annotate the synthesis methods. Their seq2vec model is relatively old (2 years!) and yet there has not been much progress in this area. I think this is a promising direction but challenging due to the data access limitations. For example, recent progress by Friedrich et al. <span id="id29">[<a class="reference internal" href="#id143" title="Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Maruscyk, and Lukas Lange. The sofc-exp corpus and neural approaches to information extraction in the materials science domain. arXiv preprint arXiv:2006.03039, 2020.">FAT+20</a>]</span> built a pre-trained transformer for solid oxide fuel cells materials but their corpus was limited to open access articles (45) over a 7 year period. This is one critical line of research that is limited due to copyright issues. Text can be copyrighted, not data, but maybe someday a court can be convinced that they are interchangeable.</p>
</section>
<section id="applications">
<h2><span class="section-number">13.9. </span>Applications<a class="headerlink" href="#applications" title="Link to this heading">#</a></h2>
<p>As discussed above, molecular design has been one of the most popular areas for sequence models in chemistry <span id="id30">[<a class="reference internal" href="#id136" title="Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120‚Äì131, 2018.">SKTW18</a>, <a class="reference internal" href="#id137" title="Rafael G√≥mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos√© Miguel Hern√°ndez-Lobato, Benjam√≠n S√°nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al√°n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268‚Äì276, 2018.">GomezBWD+18</a>, <a class="reference internal" href="#id150" title="Daniel Merk, Lukas Friedrich, Francesca Grisoni, and Gisbert Schneider. De novo design of bioactive small molecules by artificial intelligence. Molecular informatics, 37(1-2):1700153, 2018.">MFGS18</a>]</span>. Transformers have been found to be excellent at predicting chemical reactions. Schwaller et al. <span id="id31">[<a class="reference internal" href="#id152" title="Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chemical Science, 11(12):3316‚Äì3325, 2020.">SPZ+20</a>]</span> have shown how to do retrosynthetic  pathway analysis with transformers. The transformers take as input just the reactants and reagents and can predict the products. The models can be calibrated to include uncertainty estimates <span id="id32">[<a class="reference internal" href="#id153" title="Philippe Schwaller, Teodoro Laino, Th√©ophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572‚Äì1583, 2019.">SLG+19</a>]</span> and predict synthetic yield <span id="id33">[<a class="reference internal" href="#id151" title="Philippe Schwaller, Alain C Vaucher, Teodoro Laino, and Jean-Louis Reymond. Prediction of chemical reaction yields using deep learning. ChemRxiv Preprint, 2020. URL: https://doi.org/10.26434/chemrxiv.12758474.v2.">SVLR20</a>]</span>. Beyond taking molecules as input, Vaucher et al. trained a seq2seq transformer that can translate the unstructured methods section of a scientific paper into a set of structured synthetic steps <span id="id34">[<a class="reference internal" href="#id154" title="Alain C Vaucher, Federico Zipoli, Joppe Geluykens, Vishnu H Nair, Philippe Schwaller, and Teodoro Laino. Automated extraction of chemical synthesis actions from experimental procedures. Nature communications, 11(1):1‚Äì11, 2020.">VZG+20</a>]</span>. Finally, Schwaller et al. <span id="id35">[<a class="reference internal" href="#id155" title="Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. Nature Machine Intelligence, pages 1‚Äì9, 2021.">SPV+21</a>]</span> trained a transformer to classify reactions into organic reaction classes leading to a <a class="reference external" href="https://rxn4chemistry.github.io/rxnfp/tmaps/tmap_ft_10k.html">fascinating map of chemical reactions</a>.</p>
</section>
<section id="summary">
<h2><span class="section-number">13.10. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Text is a natural representation of both molecules and materials</p></li>
<li><p>SMILES and SELFIES are ways to convert molecules into strings</p></li>
<li><p>Recurrent neural networks (RNNs) are an input-length independent method of converting strings into vectors for regression or classification</p></li>
<li><p>RNNs can be trained in seq2seq (encoder/decoder) setting by having it predict the next character in a sequence. This yields a model that can autoregressively generate new sequences/molecules</p></li>
<li><p>Withholding or masking sequences for training is called self-supervised training and is a pre-training step for seq2seq models to enable them to learn the properties of a language like English or SMILES</p></li>
<li><p>Transformers are currently the best seq2seq models</p></li>
<li><p>The latent space of seq2seq models can be used for molecular design</p></li>
<li><p>Materials can be represented as text which is a complete representation for many materials</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">13.11. </span>Cited References<a class="headerlink" href="#cited-references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id36">
<div role="list" class="citation-list">
<div class="citation" id="id144" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Wei88</a><span class="fn-bracket">]</span></span>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>Journal of chemical information and computer sciences</em>, 28(1):31‚Äì36, 1988.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">SKE19</a><span class="fn-bracket">]</span></span>
<p>Murat¬†Cihan Sorkun, Abhishek Khetan, and S√ºleyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. <em>Sci. Data</em>, 6(1):143, 2019. <a class="reference external" href="https://doi.org/10.1038/s41597-019-0151-1">doi:10.1038/s41597-019-0151-1</a>.</p>
</div>
<div class="citation" id="id110" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">YCW20</a><span class="fn-bracket">]</span></span>
<p>Ziyue Yang, Maghesree Chakraborty, and Andrew¬†D White. Predicting chemical shifts with graph neural networks. <em>bioRxiv</em>, 2020.</p>
</div>
<div class="citation" id="id107" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">KGrossGunnemann20</a><span class="fn-bracket">]</span></span>
<p>Johannes Klicpera, Janek Gro√ü, and Stephan G√ºnnemann. Directional message passing for molecular graphs. In <em>International Conference on Learning Representations</em>. 2020.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">VSP+17</a><span class="fn-bracket">]</span></span>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Advances in neural information processing systems</em>, 5998‚Äì6008. 2017.</p>
</div>
<div class="citation" id="id134" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KHN+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Mario Krenn, Florian H√§se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): a 100% robust molecular string representation. <em>Machine Learning: Science and Technology</em>, 1(4):045024, nov 2020. URL: <a class="reference external" href="https://doi.org/10.1088/2632-2153/aba947">https://doi.org/10.1088/2632-2153/aba947</a>, <a class="reference external" href="https://doi.org/10.1088/2632-2153/aba947">doi:10.1088/2632-2153/aba947</a>.</p>
</div>
<div class="citation" id="id156" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">BFSV19</a><span class="fn-bracket">]</span></span>
<p>Nathan Brown, Marco Fiscato, Marwin¬†HS Segler, and Alain¬†C Vaucher. Guacamol: benchmarking models for de novo molecular design. <em>Journal of chemical information and modeling</em>, 59(3):1096‚Äì1108, 2019.</p>
</div>
<div class="citation" id="id198" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">OBoyle12</a><span class="fn-bracket">]</span></span>
<p>Noel¬†M O‚ÄôBoyle. Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. <em>Journal of cheminformatics</em>, 4(1):1‚Äì14, 2012.</p>
</div>
<div class="citation" id="id135" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RZS20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id15">2</a>)</span>
<p>Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Decimer: towards deep learning for chemical image recognition. <em>Journal of Cheminformatics</em>, 12(1):1‚Äì9, 2020.</p>
</div>
<div class="citation" id="id138" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CGR20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id12">2</a>,<a role="doc-backlink" href="#id19">3</a>)</span>
<p>Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. <em>arXiv preprint arXiv:2010.09885</em>, 2020.</p>
</div>
<div class="citation" id="id257" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">KNM+22</a><span class="fn-bracket">]</span></span>
<p>Jinwoo Kim, Tien¬†Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. <em>arXiv preprint arXiv:2207.02505</em>, 2022.</p>
</div>
<div class="citation" id="id219" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Bal11</a><span class="fn-bracket">]</span></span>
<p>Philip Ball. Beyond the bond. <em>Nature</em>, 469(7328):26‚Äì28, 2011.</p>
</div>
<div class="citation" id="id136" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SKTW18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id13">2</a>,<a role="doc-backlink" href="#id30">3</a>)</span>
<p>Marwin¬†HS Segler, Thierry Kogej, Christian Tyrchan, and Mark¬†P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. <em>ACS central science</em>, 4(1):120‚Äì131, 2018.</p>
</div>
<div class="citation" id="id137" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GomezBWD+18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id14">1</a>,<a role="doc-backlink" href="#id25">2</a>,<a role="doc-backlink" href="#id30">3</a>)</span>
<p>Rafael G√≥mez-Bombarelli, Jennifer¬†N Wei, David Duvenaud, Jos√©¬†Miguel Hern√°ndez-Lobato, Benjam√≠n S√°nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy¬†D Hirzel, Ryan¬†P Adams, and Al√°n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. <em>ACS central science</em>, 4(2):268‚Äì276, 2018.</p>
</div>
<div class="citation" id="id139" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">BMR+20</a><span class="fn-bracket">]</span></span>
<p>Tom¬†B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and others. Language models are few-shot learners. <em>arXiv preprint arXiv:2005.14165</em>, 2020.</p>
</div>
<div class="citation" id="id161" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">TDG+21</a><span class="fn-bracket">]</span></span>
<p>Yi¬†Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are pre-trained convolutions better than pre-trained transformers? <em>arXiv preprint arXiv:2105.03322</em>, 2021.</p>
</div>
<div class="citation" id="id141" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">RM21</a><span class="fn-bracket">]</span></span>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: beyond the few-shot paradigm. <em>arXiv preprint arXiv:2102.07350</em>, 2021.</p>
</div>
<div class="citation" id="id145" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">WWCF21</a><span class="fn-bracket">]</span></span>
<p>Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir¬†Barati Farimani. Molclr: molecular contrastive learning of representations via graph neural networks. <em>arXiv preprint arXiv:2102.10056</em>, 2021.</p>
</div>
<div class="citation" id="id140" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">LOG+19</a><span class="fn-bracket">]</span></span>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. <em>arXiv preprint arXiv:1907.11692</em>, 2019.</p>
</div>
<div class="citation" id="id160" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">SLP+21</a><span class="fn-bracket">]</span></span>
<p>Jianlin Su, Yu¬†Lu, Shengfeng Pan, Bo¬†Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. <em>arXiv preprint arXiv:2104.09864</em>, 2021.</p>
</div>
<div class="citation" id="id148" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">BDC+18</a><span class="fn-bracket">]</span></span>
<p>Keith¬†T Butler, Daniel¬†W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning for molecular and materials science. <em>Nature</em>, 559(7715):547‚Äì555, 2018.</p>
</div>
<div class="citation" id="id142" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">TDW+19</a><span class="fn-bracket">]</span></span>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin¬†A Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. <em>Nature</em>, 571(7763):95‚Äì98, 2019.</p>
</div>
<div class="citation" id="id149" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">SC16</a><span class="fn-bracket">]</span></span>
<p>Matthew¬†C Swain and Jacqueline¬†M Cole. Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. <em>Journal of chemical information and modeling</em>, 56(10):1894‚Äì1904, 2016.</p>
</div>
<div class="citation" id="id143" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">FAT+20</a><span class="fn-bracket">]</span></span>
<p>Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Maruscyk, and Lukas Lange. The sofc-exp corpus and neural approaches to information extraction in the materials science domain. <em>arXiv preprint arXiv:2006.03039</em>, 2020.</p>
</div>
<div class="citation" id="id150" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">MFGS18</a><span class="fn-bracket">]</span></span>
<p>Daniel Merk, Lukas Friedrich, Francesca Grisoni, and Gisbert Schneider. De novo design of bioactive small molecules by artificial intelligence. <em>Molecular informatics</em>, 37(1-2):1700153, 2018.</p>
</div>
<div class="citation" id="id152" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">SPZ+20</a><span class="fn-bracket">]</span></span>
<p>Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu¬†H Nair, Rico¬†Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. <em>Chemical Science</em>, 11(12):3316‚Äì3325, 2020.</p>
</div>
<div class="citation" id="id153" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">SLG+19</a><span class="fn-bracket">]</span></span>
<p>Philippe Schwaller, Teodoro Laino, Th√©ophile Gaudin, Peter Bolgar, Christopher¬†A Hunter, Costas Bekas, and Alpha¬†A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. <em>ACS central science</em>, 5(9):1572‚Äì1583, 2019.</p>
</div>
<div class="citation" id="id151" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id33">SVLR20</a><span class="fn-bracket">]</span></span>
<p>Philippe Schwaller, Alain¬†C Vaucher, Teodoro Laino, and Jean-Louis Reymond. Prediction of chemical reaction yields using deep learning. <em>ChemRxiv Preprint</em>, 2020. URL: <a class="reference external" href="https://doi.org/10.26434/chemrxiv.12758474.v2">https://doi.org/10.26434/chemrxiv.12758474.v2</a>.</p>
</div>
<div class="citation" id="id154" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id34">VZG+20</a><span class="fn-bracket">]</span></span>
<p>Alain¬†C Vaucher, Federico Zipoli, Joppe Geluykens, Vishnu¬†H Nair, Philippe Schwaller, and Teodoro Laino. Automated extraction of chemical synthesis actions from experimental procedures. <em>Nature communications</em>, 11(1):1‚Äì11, 2020.</p>
</div>
<div class="citation" id="id155" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">SPV+21</a><span class="fn-bracket">]</span></span>
<p>Philippe Schwaller, Daniel Probst, Alain¬†C Vaucher, Vishnu¬†H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. <em>Nature Machine Intelligence</em>, pages 1‚Äì9, 2021.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="attention.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Attention Layers</p>
      </div>
    </a>
    <a class="right-next"
       href="VAE.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Variational Autoencoder</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-molecules-into-text">13.1. Converting Molecules into Text</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selfies">13.1.1. SELFIES</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#demo">13.1.1.1. Demo</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stereochemistry">13.1.2. Stereochemistry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ideas">13.1.3. Other Ideas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-chemical-bond">13.1.4. What is a chemical bond?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">13.2. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks">13.3. Recurrent Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-rnns">13.3.1. Generative RNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masking-padding">13.4. Masking &amp; Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-solubility-example">13.5. RNN Solubility Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">13.6. Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">13.6.1. Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-latent-space-for-design">13.7. Using the Latent Space for Design</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-materials-as-text">13.8. Representing Materials as Text</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">13.9. Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">13.10. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">13.11. Cited References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andrew D. White
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">‚úï</button> <img id="wh-modal-img"> </div>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>