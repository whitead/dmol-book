
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub ðŸ“–" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>20. Pretraining &#8212; deep learning for molecules &amp; materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cb1cce99" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css?v=ffeaf963" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl/pretraining';</script>
    <script src="../_static/custom.js?v=3f5092eb"></script>
    <link rel="canonical" href="https://dmol.pub/dl/pretraining.html" />
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="21. Style Guide" href="../style.html" />
    <link rel="prev" title="19. Equivariant Neural Network for Predicting Trajectories" href="../applied/e3nn_traj.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="deep learning for molecules & materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="deep learning for molecules & materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">A. Math Review</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/tensors-and-shapes.html">1. Tensors and Shapes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">B. Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ml/introduction.html">2. Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/regression.html">3. Regression &amp; Model Assessment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/classification.html">4. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ml/kernel.html">5. Kernel Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C. Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">6. Deep Learning Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">7. Standard Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">8. Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">9. Input Data &amp; Equivariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="Equivariant.html">10. Equivariant Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="xai.html">11. Explaining Predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention.html">12. Attention Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="NLP.html">13. Deep Learning on Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="VAE.html">14. Variational Autoencoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="flows.html">15. Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="molnets.html">16. Modern Molecular NNs</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">D. Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/QM9.html">17. Predicting DFT Energies with GNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied/MolGenerator.html">18. Generative RNN in Browser</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">E. Contributed Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied/e3nn_traj.html">19. Equivariant Neural Network for Predicting Trajectories</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">20. Pretraining</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">F. Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../style.html">21. Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">22. Changelog</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/pretraining.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/whitead/dmol-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/pretraining.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dl/pretraining.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Pretraining</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-pretraining-work">20.1. How does pretraining work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-pretraining-a-model-work">20.2. Why does pretraining a model work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-vs-fine-tuning">20.3. Transfer learning vs fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">20.3.1. Transfer Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">20.3.2. Fine-Tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-for-graph-models">20.4. Pretraining for graph models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">20.5. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">20.6. Cited References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="pretraining">
<h1><span class="section-number">20. </span>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">#</a></h1>
<div class="admonition-authors admonition">
<p class="admonition-title">Authors:</p>
<p><a class="reference external" href="https://github.com/hgandhi2411">Heta Gandhi</a> &amp; <a class="reference external" href="https://github.com/SamCox822">Sam Cox</a></p>
</div>
<p>Up until this point, we have been building deep learning models from scratch and mostly training on labelled data to complete a task. A lot of times, especially in chemistry, labelled data is not readily accessible or abundant. In this scenerio, it is helpful to use a pretrained model and leverage the pretrained weights and architecture to learn a new task. In this chapter, we will look into pretraining, how it works, and some applications.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> and <a class="reference internal" href="gnn.html"><span class="doc">Graph Neural Networks</span></a>. After completing this chapter, you should be able to</p>
<ul class="simple">
<li><p>Understand why pretraining is useful, and in which situations it is appropriate</p></li>
<li><p>Understand transfer learning and fine-tuning</p></li>
<li><p>Be able to use a pretrained model for a simple downstream task</p></li>
</ul>
</div>
<section id="how-does-pretraining-work">
<h2><span class="section-number">20.1. </span>How does pretraining work?<a class="headerlink" href="#how-does-pretraining-work" title="Link to this heading">#</a></h2>
<p>Pretraining is a training process in which the weights of a model can be trained on a large dataset, for use as a starting place for training on smaller, similar datasets.</p>
<p>Supervised deep learning models are generally trained on labeled data to achieve a single task. However, for most practical problems, especially in chemistry, labeled examples are limited, imbalanced, or expensive to obtain, whereas unlabeled data is abundant. When labeled data is scarce, supervised learning techniques lead to poor generalization <span id="id1">[<a class="reference internal" href="#id228" title="Huanru Henry Mao. A survey on self-supervised pre-training for sequential transfer learning in neural networks. arXiv preprint arXiv:2007.00800, 2020.">Mao20</a>]</span>. Instead, in low data regimes, self-supervised learning (SSL) methods (an unsupervised learning approach) are often employed. In SSL, the model is trained on labels that are automatically generated from the data itself. SSL has been largely successful in large language models and computer vision, as well as in chemistry. SSL is the approach used to pre-train models, which can be fine-tuned for downstream tasks, or can be used for transfer learning. The figure below from <span id="id2">[<a class="reference internal" href="#id226" title="Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 201â€“208. JMLR Workshop and Conference Proceedings, 2010.">ECBV10</a>]</span> shows how pretraining can affect test error.</p>
<figure class="align-default" id="tept">
<img alt="Test error comparison. Comparing test loss error on MNIST data, with 400 different iterations each. On the left, red and blue correspond to test error for one layer with and without pretraining, respectively. The right image has four layers instead of one." src="../_images/test_error_pretraining.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.1 </span><span class="caption-text">Test error comparison. Comparing test loss error on MNIST data, with 400 different iterations each. On the left, red and blue correspond to test error for one layer with and without pretraining, respectively. The right image has four layers instead of one.</span><a class="headerlink" href="#tept" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="why-does-pretraining-a-model-work">
<h2><span class="section-number">20.2. </span>Why does pretraining a model work?<a class="headerlink" href="#why-does-pretraining-a-model-work" title="Link to this heading">#</a></h2>
<p>There are many theoretical reasons for why pretraining works. Pretraining can be seen as a sort of regularization technique, because it initializes parameters and restricts learning to a subset of the parameter space <span id="id3">[<a class="reference internal" href="#id228" title="Huanru Henry Mao. A survey on self-supervised pre-training for sequential transfer learning in neural networks. arXiv preprint arXiv:2007.00800, 2020.">Mao20</a>, <a class="reference internal" href="#id226" title="Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 201â€“208. JMLR Workshop and Conference Proceedings, 2010.">ECBV10</a>]</span>. More specifically, the parameters are initialized so that they are restricted to a better local basin of attractionâ€¦a region that captures the structure of the input distribution <span id="id4">[<a class="reference internal" href="#id223" title="Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812â€“5823, 2020.">YCS+20</a>]</span>. Practically, the parameter space is more constrained as the magnitude of the weights increase during training because the function becomes more nonlinear, and the loss function becomes more topologically complex <span id="id5">[<a class="reference internal" href="#id223" title="Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812â€“5823, 2020.">YCS+20</a>]</span>.</p>
<p>In plainer words, the model collects information about which aspects of the inputs are important, setting the weights accordingly. Then, the model can perform implicit metalearning (helping with hyperparameter choice), and it has been shown that the fine-tuned modelsâ€™ weights are often not far from the pretrained values <span id="id6">[<a class="reference internal" href="#id228" title="Huanru Henry Mao. A survey on self-supervised pre-training for sequential transfer learning in neural networks. arXiv preprint arXiv:2007.00800, 2020.">Mao20</a>]</span>. Thus, pretraining can help your model drive the parameters toward the values you actually want for your downstream task.</p>
</section>
<section id="transfer-learning-vs-fine-tuning">
<h2><span class="section-number">20.3. </span>Transfer learning vs fine-tuning<a class="headerlink" href="#transfer-learning-vs-fine-tuning" title="Link to this heading">#</a></h2>
<section id="transfer-learning">
<h3><span class="section-number">20.3.1. </span>Transfer Learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h3>
<p>Transfer learning works by taking a pretrained model and freezing the layers and parameters that were already trained. Then you can either add layer(s) on top, or you can modify only the last layer and train it to your new task. In transfer learning, the feature extraction layers from the pretraining process are kept frozen. It is necessary that your data has some connection with the original data.</p>
<p>There are largely two types of transfer learning, and you can find a more formal definition in <span id="id7">[<a class="reference internal" href="#id228" title="Huanru Henry Mao. A survey on self-supervised pre-training for sequential transfer learning in neural networks. arXiv preprint arXiv:2007.00800, 2020.">Mao20</a>]</span>. The first is transductive transfer learning, where you have the same tasks, but only have labels in the source (pretraining) dataset. For example, imagine training a model to predict the space group of theoretical inorganic crystal structures. Transductive transfer learning could be using this model to predict the space group of self-assembled biochemical structures. Youâ€™re using a different dataset, where the only labels are in the inorganic crystal data.</p>
<p>The second type of transfer learning is called inductive transfer learning, where you want to learn a new task, and you have labels for both your source and your target dataset. For example, imagine you train a model to predict solubility of small organic molecules. You could use inductive transfer learning and use this model to predict the pKa of another organic molecule (labeled) dataset. Notice that in both cases, the input type is the same for the source and the target problem. Also, this shouldnâ€™t be too difficult for the model, since you would imagine there would be some relationship between the solubility and the pKa of organic molecules.</p>
</section>
<section id="fine-tuning">
<h3><span class="section-number">20.3.2. </span>Fine-Tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h3>
<p>Fine-tuning is a bit different in that instead of freezing the layers and parameters, you retrain either the entire model or parts of the model. So instead of freezing the pre-trained parameters, you use them as a starting point. This can be especially helpful for low-data regimes. However, it is easy to quickly overfit when fine-tuning a pretrained model, especially on a relatively small dataset, so it is important to tune your hyperparameters, such as the learning rate.</p>
<p>For example, SMILES-BERT <span id="id8">[<a class="reference internal" href="#id230" title="Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, 429â€“436. 2019.">WGW+19</a>]</span> is a model pre-trained on SMILES strings via a recovery task. The unlabeled data is SMILES strings, with randomly masked or corrupted tokens. The model is trained to correctly recover the original SMILES string. By learning this task, the model learns to identify important components of the input, which can be applied via fine-tuning to a molecular property prediction downstream task. In this case, the original dataset is unlabeled, and the labels are generated automatically from the data, which is SMILES strings. Then, the target task dataset is SMILES strings with a molecular property label.</p>
<p>For more information on the comparison between transfer learning and fine-tuning, you can check out <a class="reference external" href="https://www.youtube.com/watch?v=3nbin3bT8ec&amp;amp;t=199s">this youtube video</a>. Also, the figure below gives a layout of fine-tuning and transfer learning. What is important to note is that in transfer learning, we retrain the last layer or add layers on the end, whereas in fine-tuning we can retrain the feature extraction layers also.</p>
<figure class="align-default" id="tlft">
<img alt="Comparison of fine-tuning and transfer learning with a general model architecture. Starting with the top middle block (original model), follow the flow chart for different situations." src="../_images/TL_FT.gif" />
<figcaption>
<p><span class="caption-number">Fig. 20.2 </span><span class="caption-text">Comparison of fine-tuning and transfer learning with a general model architecture. Starting with the top middle block (original model), follow the flow chart for different situations.</span><a class="headerlink" href="#tlft" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="pretraining-for-graph-models">
<h2><span class="section-number">20.4. </span>Pretraining for graph models<a class="headerlink" href="#pretraining-for-graph-models" title="Link to this heading">#</a></h2>
<p>GNNs typically require a large amount of labeled data and are not typically generalizable. Particularly in chemistry, there is a significant amount of unlabeled graph data available. Because of this, SSL has become very popular in GNNs, and it can be broadly split into two categories based on the method: contrastive learning and predictive learning. Predictive models are trained to generate labels based on the input, whereas contrastive models learn to generate diverse and informative information about the input and perform contrastive learning (compare representations) <span id="id9">[<a class="reference internal" href="#id222" title="Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870â€“15882, 2021.">ZLW+21</a>]</span>. You can see a comparison of the two methods and example architectures in the figure below <span id="id10">[<a class="reference internal" href="#id227" title="Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: a unified review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.">XXZ+22</a>]</span>.</p>
<p>Contrastive learning is focused on learning to maximize the agreement of features among differently augmented views of the data <span id="id11">[<a class="reference internal" href="#id223" title="Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812â€“5823, 2020.">YCS+20</a>]</span>. The goal of a contrastive learning approach is for the model to learn representations invariant to the perturbations or augmentations by maximizing the agreement between the base graph and its augmented versions. In other words, if two graphs are similar, the representation should be similar. Likewise, if two graphs are dissimilar, the model learns that the representations should be dissimilar. There have been many approaches to this, including subgraph or motif-based learning, where the model learns to break apart frequent subgraph patterns, such as functional groups <span id="id12">[<a class="reference internal" href="#id222" title="Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870â€“15882, 2021.">ZLW+21</a>]</span>. Another approach by <span id="id13">[<a class="reference internal" href="#id223" title="Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812â€“5823, 2020.">YCS+20</a>]</span> combined 4 different data augmentation techniques, similar to how masking is done for large language models, though <span id="id14">[<a class="reference internal" href="#id224" title="Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 3585â€“3594. 2021.">SXW+21</a>]</span> found that those random augmentations often changed the global properties of the molecular graph, proposing instead to augment by replacing substructures with bioisosteres.</p>
<p>Another way to think about contrastive learning is that the model looks at one or more encoders and learns that similar graphs should output similar representations, while less similar graphs should have less agreeable representations. Contrastive learning frameworks construct multiple views of each input graph, then an encoder outputs a representation for each view <span id="id15">[<a class="reference internal" href="#id227" title="Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: a unified review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.">XXZ+22</a>]</span>. During training, the encoder is trained so that the agreement between representations of the same graph is maximized. In this case, representations from the same instance (same graph) should agree, while representations from separate instances should disagree. The agreement is often measured with Mutual Information, which is a measure of shared information across representations. A thorough discussion of agreement metrics is given in <span id="id16">[<a class="reference internal" href="#id227" title="Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: a unified review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.">XXZ+22</a>]</span>.</p>
<p>Predictive models, in contrast, train with self-generated labels. This category of model is sometimes called generative models, as graph reconstruction is a popular approach. In graph reconstruction, the graph is distorted in some way (node removed, edge removed, node replaced with another type, etc.), and the model learns to reconstruct the original graph as its output. However, it is not correct to think of predictive models as simply generative models, because graph reconstruction, with an encoder and decoder, is not the only type of predictive model for graphs. Another popular example is property prediction. In property prediction examples, remember that we are still training on unlabeled data, so the property needs to be something implicit in the data, such as the connectivity of two nodes {cite} <code class="docutils literal notranslate"><span class="pre">xie2022self</span></code>. There wonâ€™t be a decoder in this case, because we donâ€™t want a graph as the output.</p>
<figure class="align-default" id="ptgnn">
<img alt="Comparison of contrastive and predictive models in the context of self-supervised learning for GNNs. On the left, contrastive models require data pairs and discriminate between positive and negative examples, and an example architecture is provided. On the right, predictive models have data(self)-generated labels and predict outputs based on input properties. An example architecture is provided." src="../_images/ssl_graphs.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.3 </span><span class="caption-text">Comparison of contrastive and predictive models in the context of self-supervised learning for GNNs. On the left, contrastive models require data pairs and discriminate between positive and negative examples, and an example architecture is provided. On the right, predictive models have data(self)-generated labels and predict outputs based on input properties. An example architecture is provided.</span><a class="headerlink" href="#ptgnn" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">20.5. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Link to this heading">#</a></h2>
<p>Click the Â <i aria-label="Launch interactive content" class="fas fa-rocket"></i>Â  above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/main/package/setup.py">this book here</a></p>
</div>
<p>Letâ€™s look at a simple example of using a pre-trained model to do transfer learning. We will load a pre-trained model from the huggingface library and use it to predict aqueous solubility of molecules. <a class="reference external" href="https://huggingface.co/">HuggingFace</a> is an open source platform that enables users to build, train and deploy their deep learning models. We load the ChemBERTa model which was originally trained on SMILES strings from the ZINC-250k dataset. Using the learned representations from ChemBERTa, we can predict aqueous solubility on a smaller dataset.<span id="id17">[<a class="reference internal" href="../ml/regression.html#id26" title="Murat Cihan Sorkun, Abhishek Khetan, and SÃ¼leyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. Sci. Data, 6(1):143, 2019. doi:10.1038/s41597-019-0151-1.">SKE19</a>]</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span><span class="o">,</span><span class="w"> </span><span class="nn">sklearn</span><span class="o">,</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.13.12/x64/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<p>We begin by creating our train and test datasets. The BBB dataset that we use is slightly imbalanced, so we use stratification to make sure both classes are present in train and test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv&quot;</span>
<span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">N</span><span class="p">)]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span> <span class="p">:]</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train</span><span class="p">[[</span><span class="s2">&quot;SMILES&quot;</span><span class="p">,</span> <span class="s2">&quot;Solubility&quot;</span><span class="p">]]</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Solubility&quot;</span><span class="p">:</span> <span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="s2">&quot;SMILES&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">})</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test</span><span class="p">[[</span><span class="s2">&quot;SMILES&quot;</span><span class="p">,</span> <span class="s2">&quot;Solubility&quot;</span><span class="p">]]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Solubility&quot;</span><span class="p">:</span> <span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="s2">&quot;SMILES&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we initialize a regression model from the <code class="docutils literal notranslate"><span class="pre">ChemBERTa_zinc250k_v2_40k</span></code> pre-trained model using HuggingFace <code class="docutils literal notranslate"><span class="pre">transformers</span></code>. We load the tokenizer and a sequence classification model with <code class="docutils literal notranslate"><span class="pre">num_labels=1</span></code> (which makes it a regression model). We also create a PyTorch Dataset class for our data. Then we train the model using the solubility dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;seyonec/ChemBERTa_zinc250k_v2_40k&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">SolubilityDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>


<span class="n">train_ds</span> <span class="o">=</span> <span class="n">SolubilityDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">train_dataset</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">SolubilityDataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   0%|          | 0/101 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   1%|          | 1/101 [00:00&lt;00:00, 13189.64it/s, Materializing param=roberta.embeddings.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   1%|          | 1/101 [00:00&lt;00:00, 1716.16it/s, Materializing param=roberta.embeddings.LayerNorm.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   2%|â–         | 2/101 [00:00&lt;00:00, 1777.25it/s, Materializing param=roberta.embeddings.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   2%|â–         | 2/101 [00:00&lt;00:00, 1219.10it/s, Materializing param=roberta.embeddings.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   3%|â–Ž         | 3/101 [00:00&lt;00:00, 1321.32it/s, Materializing param=roberta.embeddings.position_embeddings.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   3%|â–Ž         | 3/101 [00:00&lt;00:00, 1067.80it/s, Materializing param=roberta.embeddings.position_embeddings.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   4%|â–         | 4/101 [00:00&lt;00:00, 1184.75it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   4%|â–         | 4/101 [00:00&lt;00:00, 1037.10it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   5%|â–         | 5/101 [00:00&lt;00:00, 1134.58it/s, Materializing param=roberta.embeddings.word_embeddings.weight]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   5%|â–         | 5/101 [00:00&lt;00:00, 1030.90it/s, Materializing param=roberta.embeddings.word_embeddings.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   6%|â–Œ         | 6/101 [00:00&lt;00:00, 1117.39it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   6%|â–Œ         | 6/101 [00:00&lt;00:00, 1029.82it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   7%|â–‹         | 7/101 [00:00&lt;00:00, 1099.10it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   7%|â–‹         | 7/101 [00:00&lt;00:00, 1022.68it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   8%|â–Š         | 8/101 [00:00&lt;00:00, 1072.71it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   8%|â–Š         | 8/101 [00:00&lt;00:00, 1010.37it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   9%|â–‰         | 9/101 [00:00&lt;00:00, 1061.49it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:   9%|â–‰         | 9/101 [00:00&lt;00:00, 1004.25it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  10%|â–‰         | 10/101 [00:00&lt;00:00, 1043.67it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]     
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  10%|â–‰         | 10/101 [00:00&lt;00:00, 998.26it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  11%|â–ˆ         | 11/101 [00:00&lt;00:00, 1035.37it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  11%|â–ˆ         | 11/101 [00:00&lt;00:00, 991.67it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  12%|â–ˆâ–        | 12/101 [00:00&lt;00:00, 1027.60it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  12%|â–ˆâ–        | 12/101 [00:00&lt;00:00, 990.25it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  13%|â–ˆâ–Ž        | 13/101 [00:00&lt;00:00, 1021.68it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  13%|â–ˆâ–Ž        | 13/101 [00:00&lt;00:00, 984.29it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  14%|â–ˆâ–        | 14/101 [00:00&lt;00:00, 1017.73it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  14%|â–ˆâ–        | 14/101 [00:00&lt;00:00, 987.48it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  15%|â–ˆâ–        | 15/101 [00:00&lt;00:00, 1015.41it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  15%|â–ˆâ–        | 15/101 [00:00&lt;00:00, 984.79it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  16%|â–ˆâ–Œ        | 16/101 [00:00&lt;00:00, 1008.96it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]   
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  16%|â–ˆâ–Œ        | 16/101 [00:00&lt;00:00, 982.09it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  17%|â–ˆâ–‹        | 17/101 [00:00&lt;00:00, 1008.23it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  17%|â–ˆâ–‹        | 17/101 [00:00&lt;00:00, 964.89it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  18%|â–ˆâ–Š        | 18/101 [00:00&lt;00:00, 993.82it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  18%|â–ˆâ–Š        | 18/101 [00:00&lt;00:00, 968.80it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  19%|â–ˆâ–‰        | 19/101 [00:00&lt;00:00, 992.82it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  19%|â–ˆâ–‰        | 19/101 [00:00&lt;00:00, 971.24it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  20%|â–ˆâ–‰        | 20/101 [00:00&lt;00:00, 994.01it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  20%|â–ˆâ–‰        | 20/101 [00:00&lt;00:00, 969.85it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  21%|â–ˆâ–ˆ        | 21/101 [00:00&lt;00:00, 992.26it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  21%|â–ˆâ–ˆ        | 21/101 [00:00&lt;00:00, 970.42it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  22%|â–ˆâ–ˆâ–       | 22/101 [00:00&lt;00:00, 992.54it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  22%|â–ˆâ–ˆâ–       | 22/101 [00:00&lt;00:00, 972.51it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 23/101 [00:00&lt;00:00, 991.19it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 23/101 [00:00&lt;00:00, 972.73it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  24%|â–ˆâ–ˆâ–       | 24/101 [00:00&lt;00:00, 991.11it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  24%|â–ˆâ–ˆâ–       | 24/101 [00:00&lt;00:00, 972.71it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  25%|â–ˆâ–ˆâ–       | 25/101 [00:00&lt;00:00, 990.11it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  25%|â–ˆâ–ˆâ–       | 25/101 [00:00&lt;00:00, 970.96it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 26/101 [00:00&lt;00:00, 989.21it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 26/101 [00:00&lt;00:00, 972.36it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  27%|â–ˆâ–ˆâ–‹       | 27/101 [00:00&lt;00:00, 990.23it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  27%|â–ˆâ–ˆâ–‹       | 27/101 [00:00&lt;00:00, 970.99it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  28%|â–ˆâ–ˆâ–Š       | 28/101 [00:00&lt;00:00, 987.99it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  28%|â–ˆâ–ˆâ–Š       | 28/101 [00:00&lt;00:00, 973.73it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  29%|â–ˆâ–ˆâ–Š       | 29/101 [00:00&lt;00:00, 989.11it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  29%|â–ˆâ–ˆâ–Š       | 29/101 [00:00&lt;00:00, 945.02it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  30%|â–ˆâ–ˆâ–‰       | 30/101 [00:00&lt;00:00, 961.17it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  30%|â–ˆâ–ˆâ–‰       | 30/101 [00:00&lt;00:00, 947.47it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 31/101 [00:00&lt;00:00, 961.71it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 31/101 [00:00&lt;00:00, 947.75it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/101 [00:00&lt;00:00, 963.18it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/101 [00:00&lt;00:00, 950.75it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/101 [00:00&lt;00:00, 964.02it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/101 [00:00&lt;00:00, 951.05it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 34/101 [00:00&lt;00:00, 963.71it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 34/101 [00:00&lt;00:00, 950.87it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 35/101 [00:00&lt;00:00, 963.34it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 35/101 [00:00&lt;00:00, 951.43it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/101 [00:00&lt;00:00, 963.26it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/101 [00:00&lt;00:00, 952.52it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/101 [00:00&lt;00:00, 964.07it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/101 [00:00&lt;00:00, 952.88it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/101 [00:00&lt;00:00, 963.63it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/101 [00:00&lt;00:00, 952.56it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 39/101 [00:00&lt;00:00, 964.74it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 39/101 [00:00&lt;00:00, 951.57it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 40/101 [00:00&lt;00:00, 961.69it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 40/101 [00:00&lt;00:00, 952.19it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/101 [00:00&lt;00:00, 963.06it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/101 [00:00&lt;00:00, 953.98it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/101 [00:00&lt;00:00, 963.73it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/101 [00:00&lt;00:00, 954.34it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/101 [00:00&lt;00:00, 965.66it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/101 [00:00&lt;00:00, 956.32it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 44/101 [00:00&lt;00:00, 967.53it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 44/101 [00:00&lt;00:00, 958.06it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/101 [00:00&lt;00:00, 968.89it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/101 [00:00&lt;00:00, 960.17it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/101 [00:00&lt;00:00, 970.71it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/101 [00:00&lt;00:00, 962.18it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/101 [00:00&lt;00:00, 972.52it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/101 [00:00&lt;00:00, 963.17it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/101 [00:00&lt;00:00, 971.99it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/101 [00:00&lt;00:00, 962.82it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/101 [00:00&lt;00:00, 972.38it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/101 [00:00&lt;00:00, 785.26it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 50/101 [00:00&lt;00:00, 780.73it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 50/101 [00:00&lt;00:00, 772.09it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/101 [00:00&lt;00:00, 771.70it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/101 [00:00&lt;00:00, 764.21it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/101 [00:00&lt;00:00, 773.27it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/101 [00:00&lt;00:00, 767.99it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/101 [00:00&lt;00:00, 776.26it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/101 [00:00&lt;00:00, 771.54it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 54/101 [00:00&lt;00:00, 779.37it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 54/101 [00:00&lt;00:00, 773.43it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/101 [00:00&lt;00:00, 780.72it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/101 [00:00&lt;00:00, 775.37it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/101 [00:00&lt;00:00, 783.71it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/101 [00:00&lt;00:00, 778.83it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/101 [00:00&lt;00:00, 786.99it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/101 [00:00&lt;00:00, 781.44it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 58/101 [00:00&lt;00:00, 789.00it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 58/101 [00:00&lt;00:00, 784.17it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/101 [00:00&lt;00:00, 792.01it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/101 [00:00&lt;00:00, 786.82it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 60/101 [00:00&lt;00:00, 793.84it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 60/101 [00:00&lt;00:00, 789.24it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/101 [00:00&lt;00:00, 796.56it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/101 [00:00&lt;00:00, 791.75it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/101 [00:00&lt;00:00, 798.83it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/101 [00:00&lt;00:00, 794.19it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/101 [00:00&lt;00:00, 801.14it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/101 [00:00&lt;00:00, 796.82it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 64/101 [00:00&lt;00:00, 803.76it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 64/101 [00:00&lt;00:00, 799.50it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 65/101 [00:00&lt;00:00, 805.96it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 65/101 [00:00&lt;00:00, 802.01it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/101 [00:00&lt;00:00, 809.00it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/101 [00:00&lt;00:00, 804.94it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/101 [00:00&lt;00:00, 811.40it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/101 [00:00&lt;00:00, 807.00it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 68/101 [00:00&lt;00:00, 813.27it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 68/101 [00:00&lt;00:00, 809.08it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/101 [00:00&lt;00:00, 815.41it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/101 [00:00&lt;00:00, 811.04it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 70/101 [00:00&lt;00:00, 817.39it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 70/101 [00:00&lt;00:00, 813.47it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/101 [00:00&lt;00:00, 819.24it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/101 [00:00&lt;00:00, 814.88it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/101 [00:00&lt;00:00, 821.05it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/101 [00:00&lt;00:00, 817.19it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/101 [00:00&lt;00:00, 822.85it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/101 [00:00&lt;00:00, 818.74it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 74/101 [00:00&lt;00:00, 824.62it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 74/101 [00:00&lt;00:00, 820.47it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 75/101 [00:00&lt;00:00, 826.03it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 75/101 [00:00&lt;00:00, 821.79it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/101 [00:00&lt;00:00, 827.70it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/101 [00:00&lt;00:00, 823.76it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/101 [00:00&lt;00:00, 829.13it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/101 [00:00&lt;00:00, 825.27it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 78/101 [00:00&lt;00:00, 830.84it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 78/101 [00:00&lt;00:00, 827.03it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 79/101 [00:00&lt;00:00, 832.70it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 79/101 [00:00&lt;00:00, 828.69it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 80/101 [00:00&lt;00:00, 834.17it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 80/101 [00:00&lt;00:00, 830.24it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/101 [00:00&lt;00:00, 835.48it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/101 [00:00&lt;00:00, 831.88it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 82/101 [00:00&lt;00:00, 837.25it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 82/101 [00:00&lt;00:00, 832.21it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/101 [00:00&lt;00:00, 838.22it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/101 [00:00&lt;00:00, 834.48it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 84/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 84/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 84/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 85/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 85/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 88/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 88/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 89/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 89/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 90/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 90/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 92/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 92/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 94/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 94/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 95/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 95/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 98/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]    
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 98/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 99/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 99/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 100/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]     
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 100/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00&lt;00:00, 839.72it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00&lt;00:00, 805.24it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./chemberta_solubility&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">use_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_ds</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: &#39;3.488&#39;, &#39;grad_norm&#39;: &#39;33.5&#39;, &#39;learning_rate&#39;: &#39;4.02e-05&#39;, &#39;epoch&#39;: &#39;1&#39;}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: &#39;1.861&#39;, &#39;grad_norm&#39;: &#39;22.08&#39;, &#39;learning_rate&#39;: &#39;3.02e-05&#39;, &#39;epoch&#39;: &#39;2&#39;}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: &#39;1.296&#39;, &#39;grad_norm&#39;: &#39;20.82&#39;, &#39;learning_rate&#39;: &#39;2.02e-05&#39;, &#39;epoch&#39;: &#39;3&#39;}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: &#39;0.9756&#39;, &#39;grad_norm&#39;: &#39;42.75&#39;, &#39;learning_rate&#39;: &#39;1.02e-05&#39;, &#39;epoch&#39;: &#39;4&#39;}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: &#39;0.7283&#39;, &#39;grad_norm&#39;: &#39;27.41&#39;, &#39;learning_rate&#39;: &#39;2e-07&#39;, &#39;epoch&#39;: &#39;5&#39;}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing model shards:   0%|          | 0/1 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;00:00,  4.48it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&lt;00:00,  4.46it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train_runtime&#39;: &#39;859.1&#39;, &#39;train_samples_per_second&#39;: &#39;4.644&#39;, &#39;train_steps_per_second&#39;: &#39;0.291&#39;, &#39;train_loss&#39;: &#39;1.67&#39;, &#39;epoch&#39;: &#39;5&#39;}
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TrainOutput(global_step=250, training_loss=1.669712142944336, metrics={&#39;train_runtime&#39;: 859.0806, &#39;train_samples_per_second&#39;: 4.644, &#39;train_steps_per_second&#39;: 0.291, &#39;train_loss&#39;: 1.669712142944336, &#39;epoch&#39;: 5.0})
</pre></div>
</div>
</div>
</div>
<p>Now we evaluate the trained model on our test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">eval_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;eval_loss&#39;: &#39;2.007&#39;, &#39;eval_runtime&#39;: &#39;12.58&#39;, &#39;eval_samples_per_second&#39;: &#39;15.9&#39;, &#39;eval_steps_per_second&#39;: &#39;1.034&#39;, &#39;epoch&#39;: &#39;5&#39;}
{&#39;eval_loss&#39;: 2.0070083141326904, &#39;eval_runtime&#39;: 12.5753, &#39;eval_samples_per_second&#39;: 15.904, &#39;eval_steps_per_second&#39;: 1.034, &#39;epoch&#39;: 5.0}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="o">-</span><span class="mi">10</span><span class="p">,</span>
    <span class="mf">0.0</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;Correlation coefficient: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">],</span><span class="w"> </span><span class="n">predictions</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actual Solubility&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Predicted Solubility&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/222fd70d08595306366dd482ebfe8e3d905b8f5b2449abd728c4702691a221a1.png" src="../_images/222fd70d08595306366dd482ebfe8e3d905b8f5b2449abd728c4702691a221a1.png" />
</div>
</div>
<p>The model performs quite well on our test set. We have fine-tuned the pretrained model for a task that it was not trained for. This shows that even though the original model was trained on the ZINC dataset, the input representations can be used to make predictions on another dataset, with a different task. Using pre-trained models saves time and effort spent in training the model. To further improve performance on this silubility prediction task, you can change some other parameters like the learning rate or add additional layers before the output layer.</p>
</section>
<section id="cited-references">
<h2><span class="section-number">20.6. </span>Cited References<a class="headerlink" href="#cited-references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id18">
<div role="list" class="citation-list">
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">SKE19</a><span class="fn-bracket">]</span></span>
<p>MuratÂ Cihan Sorkun, Abhishek Khetan, and SÃ¼leyman Er. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. <em>Sci. Data</em>, 6(1):143, 2019. <a class="reference external" href="https://doi.org/10.1038/s41597-019-0151-1">doi:10.1038/s41597-019-0151-1</a>.</p>
</div>
<div class="citation" id="id228" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mao20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>,<a role="doc-backlink" href="#id6">3</a>,<a role="doc-backlink" href="#id7">4</a>)</span>
<p>HuanruÂ Henry Mao. A survey on self-supervised pre-training for sequential transfer learning in neural networks. <em>arXiv preprint arXiv:2007.00800</em>, 2020.</p>
</div>
<div class="citation" id="id226" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ECBV10<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em>, 201â€“208. JMLR Workshop and Conference Proceedings, 2010.</p>
</div>
<div class="citation" id="id223" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>YCS+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>,<a role="doc-backlink" href="#id11">3</a>,<a role="doc-backlink" href="#id13">4</a>)</span>
<p>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. <em>Advances in Neural Information Processing Systems</em>, 33:5812â€“5823, 2020.</p>
</div>
<div class="citation" id="id230" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">WGW+19</a><span class="fn-bracket">]</span></span>
<p>Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In <em>Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics</em>, 429â€“436. 2019.</p>
</div>
<div class="citation" id="id222" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZLW+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Zaixi Zhang, QiÂ Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. <em>Advances in Neural Information Processing Systems</em>, 34:15870â€“15882, 2021.</p>
</div>
<div class="citation" id="id227" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XXZ+22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id10">1</a>,<a role="doc-backlink" href="#id15">2</a>,<a role="doc-backlink" href="#id16">3</a>)</span>
<p>Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: a unified review. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2022.</p>
</div>
<div class="citation" id="id224" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">SXW+21</a><span class="fn-bracket">]</span></span>
<p>Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In <em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>, 3585â€“3594. 2021.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../applied/e3nn_traj.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Equivariant Neural Network for Predicting Trajectories</p>
      </div>
    </a>
    <a class="right-next"
       href="../style.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Style Guide</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-pretraining-work">20.1. How does pretraining work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-does-pretraining-a-model-work">20.2. Why does pretraining a model work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-vs-fine-tuning">20.3. Transfer learning vs fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">20.3.1. Transfer Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">20.3.2. Fine-Tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-for-graph-models">20.4. Pretraining for graph models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-this-notebook">20.5. Running This Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cited-references">20.6. Cited References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andrew D. White
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">âœ•</button> <img id="wh-modal-img"> </div>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>