@article{martins2012bbb,
  author  = {Martins, Ines Filipa and Teixeira, Ana L and Pinheiro, Luis and Falcao, Andre O},
  doi     = {10.1021/ci300124c},
  journal = {Journal of Chemical Information and Modeling},
  number  = {6},
  pages   = {1686--1697},
  title   = {{A Bayesian Approach to in Silico Blood-Brain Barrier Penetration Modeling}},
  url     = {https://doi.org/10.1021/ci300124c},
  volume  = {52},
  year    = {2012}
}

@article{ramakrishnan2014quantum,
  title     = {Quantum chemistry structures and properties of 134 kilo molecules},
  author    = {Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and Von Lilienfeld, O Anatole},
  year      = 2014,
  journal   = {Scientific data},
  publisher = {Nature Publishing Group},
  volume    = 1,
  number    = 1,
  pages     = {1--7}
}
@article{krenn2020self,
  title     = {Self-Referencing Embedded Strings (SELFIES): A 100\% robust molecular string representation},
  author    = {Krenn, Mario and H{\"a}se, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alan},
  year      = 2020,
  journal   = {Machine Learning: Science and Technology},
  publisher = {IOP Publishing},
  volume    = 1,
  number    = 4,
  pages     = {045024}
}
@article{sterling2015zinc,
  title     = {ZINC 15--ligand discovery for everyone},
  author    = {Sterling, Teague and Irwin, John J},
  year      = 2015,
  journal   = {Journal of chemical information and modeling},
  publisher = {ACS Publications},
  volume    = 55,
  number    = 11,
  pages     = {2324--2337}
}
@book{alpaydin2020introduction,
  title     = {Introduction to machine learning},
  author    = {Alpaydin, Ethem},
  year      = 2020,
  publisher = {MIT press}
}
@article{balachandran2019machine,
  title     = {Machine learning guided design of functional materials with targeted properties},
  author    = {Balachandran, Prasanna V},
  year      = 2019,
  journal   = {Computational Materials Science},
  publisher = {Elsevier},
  volume    = 164,
  pages     = {82--90}
}
@article{barber2019predictive,
  title   = {Predictive inference with the jackknife+},
  author  = {Barber, Rina Foygel and Candes, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
  year    = 2019,
  journal = {arXiv preprint arXiv:1905.02928}
}
@article{barrett2018classifying,
  title     = {Classifying antimicrobial and multifunctional peptides with Bayesian network models},
  author    = {Barrett, Rainier and Jiang, Shaoyi and White, Andrew D},
  year      = 2018,
  journal   = {Peptide Science},
  publisher = {Wiley Online Library},
  volume    = 110,
  number    = 4,
  pages     = {e24079}
}
@book{bishop2006pattern,
  title     = {Pattern recognition and machine learning},
  author    = {Bishop, Christopher M},
  year      = 2006,
  publisher = {springer}
}
@article{chawla2002smote,
  title   = {SMOTE: synthetic minority over-sampling technique},
  author  = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  year    = 2002,
  journal = {Journal of artificial intelligence research},
  volume  = 16,
  pages   = {321--357}
}
@article{fung2021benchmarking,
  title     = {Benchmarking graph neural networks for materials chemistry},
  author    = {Victor Fung and Jiaxin Zhang and Eric Juarez and Bobby G. Sumpter},
  year      = 2021,
  month     = jun,
  journal   = {npj Computational Materials},
  publisher = {Springer Science and Business Media {LLC}},
  volume    = 7,
  number    = 1,
  doi       = {10.1038/s41524-021-00554-0},
  url       = {https://doi.org/10.1038/s41524-021-00554-0}
}
@article{gomez2020machine,
  title     = {Machine Learning and Big-Data in Computational Chemistry},
  author    = {G{\'o}mez-Bombarelli, Rafael and Aspuru-Guzik, Al{\'a}n},
  year      = 2020,
  journal   = {Handbook of Materials Modeling: Methods: Theory and Modeling},
  publisher = {Springer},
  pages     = {1939--1962}
}
@article{moriwaki2018mordred,
  title     = {Mordred: a molecular descriptor calculator},
  author    = {Moriwaki, Hirotomo and Tian, Yu-Shi and Kawashita, Norihito and Takagi, Tatsuya},
  year      = 2018,
  journal   = {Journal of cheminformatics},
  publisher = {Springer},
  volume    = 10,
  number    = 1,
  pages     = 4
}
@article{nandy2018strategies,
  title     = {Strategies and software for machine learning accelerated discovery in transition metal chemistry},
  author    = {Nandy, Aditya and Duan, Chenru and Janet, Jon Paul and Gugler, Stefan and Kulik, Heather J},
  year      = 2018,
  journal   = {Industrial \& Engineering Chemistry Research},
  publisher = {ACS Publications},
  volume    = 57,
  number    = 42,
  pages     = {13973--13986}
}
@article{rupp2012fast,
  title     = {Fast and accurate modeling of molecular atomization energies with machine learning},
  author    = {Rupp, Matthias and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert and Von Lilienfeld, O Anatole},
  year      = 2012,
  journal   = {Physical review letters},
  publisher = {APS},
  volume    = 108,
  number    = 5,
  pages     = {058301}
}
@article{scherer2020kernel,
  title     = {Kernel-based machine learning for efficient simulations of molecular liquids},
  author    = {Scherer, Christoph and Scheid, René and Andrienko, Denis and Bereau, Tristan},
  year      = 2020,
  journal   = {Journal of Chemical Theory and Computation},
  publisher = {ACS Publications},
  volume    = 16,
  number    = 5,
  pages     = {3194--3204}
}
@article{Sorkun2019,
  title    = {{AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds}},
  author   = {Sorkun, Murat Cihan and Khetan, Abhishek and Er, S{\"{u}}leyman},
  year     = 2019,
  journal  = {Sci. Data},
  volume   = 6,
  number   = 1,
  pages    = 143,
  doi      = {10.1038/s41597-019-0151-1},
  issn     = {2052-4463},
  abstract = {Water is a ubiquitous solvent in chemistry and life. It is therefore no surprise that the aqueous solubility of compounds has a key role in various domains, including but not limited to drug discovery, paint, coating, and battery materials design. Measurement and prediction of aqueous solubility is a complex and prevailing challenge in chemistry. For the latter, different data-driven prediction models have recently been developed to augment the physics-based modeling approaches. To construct accurate data-driven estimation models, it is essential that the underlying experimental calibration data used by these models is of high fidelity and quality. Existing solubility datasets show variance in the chemical space of compounds covered, measurement methods, experimental conditions, but also in the non-standard representations, size, and accessibility of data. To address this problem, we generated a new database of compounds, AqSolDB, by merging a total of nine different aqueous solubility datasets, curating the merged data, standardizing and validating the compound representation formats, marking with reliability labels, and providing 2D descriptors of compounds as a Supplementary Resource.}
}
@article{sun2019machine,
  title     = {Machine learning--assisted molecular design and efficiency prediction for high-performance organic photovoltaic materials},
  author    = {Sun, Wenbo and Zheng, Yujie and Yang, Ke and Zhang, Qi and Shah, Akeel A and Wu, Zhou and Sun, Yuyang and Feng, Liang and Chen, Dongyang and Xiao, Zeyun and others},
  year      = 2019,
  journal   = {Science advances},
  publisher = {American Association for the Advancement of Science},
  volume    = 5,
  number    = 11,
  pages     = {eaay4275}
}
@article{sutton2020identifying,
  title     = {Identifying domains of applicability of machine learning models for materials science},
  author    = {Sutton, Christopher and Boley, Mario and Ghiringhelli, Luca M and Rupp, Matthias and Vreeken, Jilles and Scheffler, Matthias},
  year      = 2020,
  journal   = {Nature Communications},
  publisher = {Nature Publishing Group},
  volume    = 11,
  number    = 1,
  pages     = {1--9}
}
@article{wu2018moleculenet,
  title     = {MoleculeNet: a benchmark for molecular machine learning},
  author    = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  year      = 2018,
  journal   = {Chemical science},
  publisher = {Royal Society of Chemistry},
  volume    = 9,
  number    = 2,
  pages     = {513--530}
}
@article{xie2019graph,
  title     = {Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials},
  author    = {Xie, Tian and France-Lanord, Arthur and Wang, Yanming and Shao-Horn, Yang and Grossman, Jeffrey C},
  year      = 2019,
  journal   = {Nature communications},
  publisher = {Nature Publishing Group},
  volume    = 10,
  number    = 1,
  pages     = {1--9}
}
@inproceedings{byrd2019effect,
  title        = {What is the effect of importance weighting in deep learning?},
  author       = {Byrd, Jonathon and Lipton, Zachary},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning},
  pages        = {872--881},
  organization = {PMLR}
}
@article{song2021inferring,
  title     = {Inferring protein sequence-function relationships with large-scale positive-unlabeled learning},
  author    = {Song, Hyebin and Bremer, Bennett J and Hinds, Emily C and Raskutti, Garvesh and Romero, Philip A},
  year      = 2021,
  journal   = {Cell Systems},
  publisher = {Elsevier},
  volume    = 12,
  number    = 1,
  pages     = {92--101}
}
@article{pham2005selection,
  title     = {Selection of K in K-means clustering},
  author    = {Pham, Duc Truong and Dimov, Stefan S and Nguyen, Chi D},
  year      = 2005,
  journal   = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
  publisher = {SAGE Publications Sage UK: London, England},
  volume    = 219,
  number    = 1,
  pages     = {103--119}
}
@article{raschka2018model,
  title   = {Model evaluation, model selection, and algorithm selection in machine learning},
  author  = {Raschka, Sebastian},
  year    = 2018,
  journal = {arXiv preprint arXiv:1811.12808}
}
@article{he2009learning,
  title     = {Learning from imbalanced data},
  author    = {He, Haibo and Garcia, Edwardo A},
  year      = 2009,
  journal   = {IEEE Transactions on knowledge and data engineering},
  publisher = {Ieee},
  volume    = 21,
  number    = 9,
  pages     = {1263--1284}
}
@article{youbi2021simple,
  title   = {Simple data balancing achieves competitive worst-group-accuracy},
  author  = {Youbi Idrissi, Badr and Arjovsky, Martin and Pezeshki, Mohammad and Lopez-Paz, David},
  year    = 2021,
  journal = {arXiv e-prints},
  pages   = {arXiv--2110}
}
@article{45826,
  title   = {Neural Architecture Search with Reinforcement Learning},
  author  = {Barret Zoph and Quoc V. Le},
  year    = 2017,
  journal = {arXiv preprint arXiv:1611.01578},
  url     = {https://arxiv.org/abs/1611.01578}
}
@article{LeNet,
  title   = {Gradient-based learning applied to document recognition},
  author  = {Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
  year    = 1998,
  journal = {Proceedings of the IEEE},
  volume  = 86,
  number  = 11,
  pages   = {2278--2324}
}
@article{hyperband,
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  author  = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
  year    = 2018,
  journal = {Journal of Machine Learning Research},
  volume  = 18,
  number  = 185,
  pages   = {1--52},
  url     = {http://jmlr.org/papers/v18/16-558.html}
}
@inproceedings{zhao2020exploring,
  title     = {Exploring self-attention for image recognition},
  author    = {Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen},
  year      = 2020,
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {10076--10085}
}
@inproceedings{glorot2010understanding,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  year      = 2010,
  booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages     = {249--256}
}
@article{noconv,
  title     = {Deep, big, simple neural nets for handwritten digit recognition},
  author    = {Cire{\c{s}}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  year      = 2010,
  journal   = {Neural computation},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  volume    = 22,
  number    = 12,
  pages     = {3207--3220}
}
@article{solubility,
  title     = {Bioinformatics approaches for improved recombinant protein production in Escherichia coli: protein solubility prediction},
  author    = {Chang, Catherine Ching Han and Song, Jiangning and Tey, Beng Ti and Ramanan, Ramakrishnan Nagasundara},
  year      = 2014,
  journal   = {Briefings in bioinformatics},
  publisher = {Oxford University Press},
  volume    = 15,
  number    = 6,
  pages     = {953--962}
}
@article{deepsol,
  title    = {{DeepSol: a deep learning framework for sequence-based protein solubility prediction}},
  author   = {Khurana, Sameer and Rawi, Reda and Kunji, Khalid and Chuang, Gwo-Yu and Bensmail, Halima and Mall, Raghvendra},
  year     = 2018,
  month    = {03},
  journal  = {Bioinformatics},
  volume   = 34,
  number   = 15,
  pages    = {2605--2613},
  doi      = {10.1093/bioinformatics/bty166},
  issn     = {1367-4803},
  url      = {https://doi.org/10.1093/bioinformatics/bty166},
  abstract = {{Protein solubility plays a vital role in pharmaceutical research and production yield. For a given protein, the extent of its solubility can represent the quality of its function, and is ultimately defined by its sequence. Thus, it is imperative to develop novel, highly accurate in silico sequence-based protein solubility predictors. In this work we propose, DeepSol, a novel Deep Learning-based protein solubility predictor. The backbone of our framework is a convolutional neural network that exploits k-mer structure and additional sequence and structural features extracted from the protein sequence.DeepSol outperformed all known sequence-based state-of-the-art solubility prediction methods and attained an accuracy of 0.77 and Matthew’s correlation coefficient of 0.55. The superior prediction accuracy of DeepSol allows to screen for sequences with enhanced production capacity and can more reliably predict solubility of novel proteins.DeepSol’s best performing models and results are publicly deposited at https://doi.org/10.5281/zenodo.1162886 (Khurana and Mall, 2018).Supplementary data are available at Bioinformatics online.}}
}
@article{neal2018modern,
  title    = {A modern take on the bias-variance tradeoff in neural networks},
  author   = {Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  year     = 2018,
  journal  = {arXiv preprint arXiv:1810.08591},
  abstract = {{Protein solubility plays a vital role in pharmaceutical research and production yield. For a given protein, the extent of its solubility can represent the quality of its function, and is ultimately defined by its sequence. Thus, it is imperative to develop novel, highly accurate in silico sequence-based protein solubility predictors. In this work we propose, DeepSol, a novel Deep Learning-based protein solubility predictor. The backbone of our framework is a convolutional neural network that exploits k-mer structure and additional sequence and structural features extracted from the protein sequence.DeepSol outperformed all known sequence-based state-of-the-art solubility prediction methods and attained an accuracy of 0.77 and Matthew's correlation coefficient of 0.55. The superior prediction accuracy of DeepSol allows to screen for sequences with enhanced production capacity and can more reliably predict solubility of novel proteins.DeepSol's best performing models and results are publicly deposited at https://doi.org/10.5281/zenodo.1162886 (Khurana and Mall, 2018).Supplementary data are available at Bioinformatics online.}}
}
@inproceedings{gal2016dropout,
  title     = {Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author    = {Gal, Yarin and Ghahramani, Zoubin},
  year      = 2016,
  booktitle = {international conference on machine learning},
  pages     = {1050--1059}
}
@article{kipf2016semi,
  title   = {Semi-supervised classification with graph convolutional networks},
  author  = {Kipf, Thomas N and Welling, Max},
  year    = 2016,
  journal = {arXiv preprint arXiv:1609.02907}
}
@article{battaglia2018relational,
  title   = {Relational inductive biases, deep learning, and graph networks},
  author  = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  year    = 2018,
  journal = {arXiv preprint arXiv:1806.01261}
}
@article{li2015gated,
  title   = {Gated graph sequence neural networks},
  author  = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  year    = 2015,
  journal = {arXiv preprint arXiv:1511.05493}
}
@article{chung2014empirical,
  title   = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author  = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year    = 2014,
  journal = {arXiv preprint arXiv:1412.3555}
}
@article{gilmer2017neural,
  title   = {Neural message passing for quantum chemistry},
  author  = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  year    = 2017,
  journal = {arXiv preprint arXiv:1704.01212}
}
@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year      = 2017,
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008}
}
@article{luong2015effective,
  title   = {Effective approaches to attention-based neural machine translation},
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  year    = 2015,
  journal = {arXiv preprint arXiv:1508.04025}
}
@article{zhang2018gaan,
  title   = {Gaan: Gated attention networks for learning on large and spatiotemporal graphs},
  author  = {Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
  year    = 2018,
  journal = {arXiv preprint arXiv:1803.07294}
}
@article{treisman1980feature,
  title     = {A feature-integration theory of attention},
  author    = {Treisman, Anne M and Gelade, Garry},
  year      = 1980,
  journal   = {Cognitive psychology},
  publisher = {Elsevier},
  volume    = 12,
  number    = 1,
  pages     = {97--136}
}
@article{BALUJA1997329,
  title    = {Expectation-based selective attention for visual monitoring and control of a robot vehicle},
  author   = {Shumeet Baluja and Dean A. Pomerleau},
  year     = 1997,
  journal  = {Robotics and Autonomous Systems},
  volume   = 22,
  number   = 3,
  pages    = {329--344},
  doi      = {https://doi.org/10.1016/S0921-8890(97)00046-8},
  issn     = {0921-8890},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889097000468},
  note     = {Robot Learning: The New Wave},
  keywords = {Expectation-based selective attention, Autonomous navigation, Temporal coherence, Saliency map, Artificial neural networks},
  abstract = {Reliable vision-based control of an autonomous vehicle requires the ability to focus attention on the important features in an input scene. Previous work with an autonomous lane following system, ALVINN (Pomerleau, 1993), has yielded good results in uncluttered conditions. This paper presents an artificial neural network based learning approach for handling difficult scenes which will confuse the ALVINN system. This work presents a mechanism for achieving task-specific focus of attention by exploiting temporal coherence. A saliency map, which is based upon a computed expectation of the contents of the inputs in the next time step, indicates which regions of the input retina are important for performing the task. The saliency map can be used to accentuate the features which are important for the task, and de-emphasize those which are not.}
}
@article{thomas2018tensor,
  title   = {Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author  = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  year    = 2018,
  journal = {arXiv preprint arXiv:1802.08219}
}
@inproceedings{weiler20183d,
  title     = {3d steerable cnns: Learning rotationally equivariant features in volumetric data},
  author    = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
  year      = 2018,
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {10381--10392}
}
@article{Bart,
  title     = {On representing chemical environments},
  author    = {Bart\'ok, Albert P. and Kondor, Risi and Cs\'anyi, G\'abor},
  year      = 2013,
  month     = {May},
  journal   = {Phys. Rev. B},
  publisher = {American Physical Society},
  volume    = 87,
  pages     = 184115,
  doi       = {10.1103/PhysRevB.87.184115},
  url       = {https://link.aps.org/doi/10.1103/PhysRevB.87.184115},
  issue     = 18,
  numpages  = 16
}
@article{behler2011atom,
  title     = {Atom-centered symmetry functions for constructing high-dimensional neural network potentials},
  author    = {Behler, J{\"o}rg},
  year      = 2011,
  journal   = {The Journal of chemical physics},
  publisher = {American Institute of Physics},
  volume    = 134,
  number    = 7,
  pages     = {074106}
}
@inproceedings{ravanbakhsh2017equivariance,
  title     = {Equivariance Through Parameter-Sharing},
  author    = {Ravanbakhsh, Siamak and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  year      = 2017,
  booktitle = {International Conference on Machine Learning},
  pages     = {2892--2901}
}
@article{mesquita2020rethinking,
  title   = {Rethinking pooling in graph neural networks},
  author  = {Mesquita, Diego and Souza, Amauri and Kaski, Samuel},
  year    = 2020,
  journal = {Advances in Neural Information Processing Systems},
  volume  = 33
}
@article{luzhnica2019graph,
  title   = {On graph classification networks, datasets and baselines},
  author  = {Luzhnica, Enxhell and Day, Ben and Li{\`o}, Pietro},
  year    = 2019,
  journal = {arXiv preprint arXiv:1905.04682}
}
@inproceedings{xu2018powerful,
  title     = {How Powerful are Graph Neural Networks?},
  author    = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year      = 2018,
  booktitle = {International Conference on Learning Representations}
}
@inproceedings{hamilton2017inductive,
  title     = {Inductive representation learning on large graphs},
  author    = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  year      = 2017,
  booktitle = {Advances in neural information processing systems},
  pages     = {1024--1034}
}
@article{shchur2018pitfalls,
  title   = {Pitfalls of graph neural network evaluation},
  author  = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  year    = 2018,
  journal = {arXiv preprint arXiv:1811.05868}
}
@inproceedings{errica2019fair,
  title     = {A Fair Comparison of Graph Neural Networks for Graph Classification},
  author    = {Errica, Federico and Podda, Marco and Bacciu, Davide and Micheli, Alessio},
  year      = 2019,
  booktitle = {International Conference on Learning Representations}
}
@article{dwivedi2020benchmarking,
  title   = {Benchmarking graph neural networks},
  author  = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  year    = 2020,
  journal = {arXiv preprint arXiv:2003.00982}
}
@article{bronstein2017geometric,
  title     = {Geometric deep learning: going beyond euclidean data},
  author    = {Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year      = 2017,
  journal   = {IEEE Signal Processing Magazine},
  publisher = {IEEE},
  volume    = 34,
  number    = 4,
  pages     = {18--42}
}
@article{wu2020comprehensive,
  title     = {A comprehensive survey on graph neural networks},
  author    = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  year      = 2020,
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  publisher = {IEEE}
}
@article{kobyzev2020normalizing,
  title     = {Normalizing flows: An introduction and review of current methods},
  author    = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
  year      = 2020,
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE}
}
@inproceedings{papamakarios2017masked,
  title     = {Masked autoregressive flow for density estimation},
  author    = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year      = 2017,
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2338--2347}
}
@inproceedings{kingma2016improved,
  title     = {Improved variational inference with inverse autoregressive flow},
  author    = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year      = 2016,
  booktitle = {Advances in neural information processing systems},
  pages     = {4743--4751}
}
@article{kim2018flowavenet,
  title   = {FloWaveNet: A generative flow for raw audio},
  author  = {Kim, Sungwon and Lee, Sang-gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  year    = 2018,
  journal = {arXiv preprint arXiv:1811.02155}
}
@article{dinh2016density,
  title   = {Density estimation using real nvp},
  author  = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  year    = 2016,
  journal = {arXiv preprint arXiv:1605.08803}
}
@article{das2019dimensionality,
  title   = {Dimensionality reduction flows},
  author  = {Das, Hari Prasanna and Abbeel, Pieter and Spanos, Costas J},
  year    = 2019,
  journal = {arXiv preprint arXiv:1908.01686}
}
@article{foote2000relation,
  title     = {A relation between the principal axes of inertia and ligand binding},
  author    = {Foote, Jefferson and Raman, Anandi},
  year      = 2000,
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {National Acad Sciences},
  volume    = 97,
  number    = 3,
  pages     = {978--983}
}
@article{esteves2020theoretical,
  title   = {Theoretical Aspects of Group Equivariant Neural Networks},
  author  = {Esteves, Carlos},
  year    = 2020,
  journal = {arXiv preprint arXiv:2004.05154}
}
@inproceedings{mathieu2019disentangling,
  title     = {Disentangling disentanglement in variational autoencoders},
  author    = {Mathieu, Emile and Rainforth, Tom and Siddharth, N and Teh, Yee Whye},
  year      = 2019,
  booktitle = {International Conference on Machine Learning},
  publisher = {PMLR},
  pages     = {4402--4412}
}
@inproceedings{klicpera2019directional,
  title     = {Directional Message Passing for Molecular Graphs},
  author    = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  year      = 2020,
  booktitle = {International Conference on Learning Representations}
}
@article{jing2020learning,
  title   = {Learning from Protein Structure with Geometric Vector Perceptrons},
  author  = {Jing, Bowen and Eismann, Stephan and Suriana, Patricia and Townshend, Raphael JL and Dror, Ron},
  year    = 2020,
  journal = {arXiv preprint arXiv:2009.01411}
}
@article{li2020graph,
  title     = {Graph neural network based coarse-grained mapping prediction},
  author    = {Li, Zhiheng and Wellawatte, Geemi P and Chakraborty, Maghesree and Gandhi, Heta A and Xu, Chenliang and White, Andrew D},
  year      = 2020,
  journal   = {Chemical Science},
  publisher = {Royal Society of Chemistry},
  volume    = 11,
  number    = 35,
  pages     = {9524--9531}
}
@article{yang2020predicting,
  title     = {Predicting Chemical Shifts with Graph Neural Networks},
  author    = {Yang, Ziyue and Chakraborty, Maghesree and White, Andrew D},
  year      = 2020,
  journal   = {bioRxiv},
  publisher = {Cold Spring Harbor Laboratory}
}
@article{maziarka2020molecule,
  title   = {Molecule Attention Transformer},
  author  = {Maziarka, {\L}ukasz and Danel, Tomasz and Mucha, S{\l}awomir and Rataj, Krzysztof and Tabor, Jacek and Jastrz{\k{e}}bski, Stanis{\l}aw},
  year    = 2020,
  journal = {arXiv preprint arXiv:2002.08264}
}
@article{miller2020relevance,
  title   = {Relevance of rotationally equivariant convolutions for predicting molecular properties},
  author  = {Miller, Benjamin Kurt and Geiger, Mario and Smidt, Tess E and No{\'e}, Frank},
  year    = 2020,
  journal = {arXiv preprint arXiv:2008.08461}
}
@article{wu2020stochastic,
  title   = {Stochastic Normalizing Flows},
  author  = {Wu, Hao and K{\"o}hler, Jonas and No{\'e}, Frank},
  year    = 2020,
  journal = {arXiv preprint arXiv:2002.06707}
}
@inproceedings{papamakarios2019sequential,
  title        = {Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows},
  author       = {Papamakarios, George and Sterratt, David and Murray, Iain},
  year         = 2019,
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages        = {837--848},
  organization = {PMLR}
}
@article{papamakarios2019normalizing,
  title   = {Normalizing flows for probabilistic modeling and inference},
  author  = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year    = 2019,
  journal = {arXiv preprint arXiv:1912.02762}
}
@inproceedings{zaheer2017deep,
  title     = {Deep sets},
  author    = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year      = 2017,
  booktitle = {Advances in neural information processing systems},
  pages     = {3391--3401}
}
@inproceedings{kondor2018generalization,
  title     = {On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups},
  author    = {Kondor, Risi and Trivedi, Shubhendu},
  year      = 2018,
  booktitle = {International Conference on Machine Learning},
  pages     = {2747--2755}
}
@article{cohen2019general,
  title   = {A general theory of equivariant cnns on homogeneous spaces},
  author  = {Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
  year    = 2019,
  journal = {Advances in neural information processing systems},
  volume  = 32,
  pages   = {9145--9156}
}
@article{winter2021auto,
  title   = {Auto-Encoding Molecular Conformations},
  author  = {Winter, Robin and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  year    = 2021,
  journal = {arXiv preprint arXiv:2101.01618}
}
@article{finzi2020generalizing,
  title   = {Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data},
  author  = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  year    = 2020,
  journal = {arXiv preprint arXiv:2002.12880}
}
@article{romero2020attentive,
  title   = {Attentive Group Equivariant Convolutional Networks},
  author  = {Romero, David W and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark},
  year    = 2020,
  journal = {arXiv},
  pages   = {arXiv--2002}
}
@inproceedings{cohen2016group,
  title     = {Group equivariant convolutional networks},
  author    = {Cohen, Taco and Welling, Max},
  year      = 2016,
  booktitle = {International conference on machine learning},
  pages     = {2990--2999}
}
@article{wang2020equivariant,
  title   = {Equivariant Maps for Hierarchical Structures},
  author  = {Wang, Renhao and Albooyeh, Marjan and Ravanbakhsh, Siamak},
  year    = 2020,
  journal = {arXiv preprint arXiv:2006.03627}
}
@article{batzner2021se3equivariant,
  title   = {SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials},
  author  = {Simon Batzner and Tess E. Smidt and Lixin Sun and Jonathan P. Mailoa and Mordechai Kornbluth and Nicola Molinari and Boris Kozinsky},
  year    = 2021,
  journal = {arXiv preprint arXiv:2101.03164}
}
@article{klicpera2020directional,
  title   = {Directional message passing for molecular graphs},
  author  = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  year    = 2020,
  journal = {arXiv preprint arXiv:2003.03123}
}
@book{serre1977linear,
  title     = {Linear representations of finite groups},
  author    = {Serre, Jean-Pierre},
  year      = 1977,
  publisher = {Springer},
  volume    = 42
}
@book{zee2016,
  title     = {Group theory in a nutshell for physicists},
  author    = {Zee, Anthony},
  year      = 2016,
  publisher = {Princeton University Press}
}
@article{musil2021physicsinspired,
  title   = {Physics-inspired structural representations for molecules and materials},
  author  = {Felix Musil and Andrea Grisafi and Albert P. Bartók and Christoph Ortner and Gábor Csányi and Michele Ceriotti},
  year    = 2021,
  journal = {arXiv preprint arXiv:2101.04673}
}
@article{chew2020fast,
  title     = {Fast predictions of liquid-phase acid-catalyzed reaction rates using molecular dynamics simulations and convolutional neural networks},
  author    = {Chew, Alex K and Jiang, Shengli and Zhang, Weiqi and Zavala, Victor M and Van Lehn, Reid C},
  year      = 2020,
  journal   = {Chemical Science},
  publisher = {Royal Society of Chemistry},
  volume    = 11,
  number    = 46,
  pages     = {12464--12476}
}
@article{lang2020wigner,
  title   = {A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels},
  author  = {Lang, Leon and Weiler, Maurice},
  year    = 2020,
  journal = {arXiv preprint arXiv:2010.10952}
}
@article{kondor2018clebsch,
  title   = {Clebsch-gordan nets: a fully fourier space spherical convolutional neural network},
  author  = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  year    = 2018,
  journal = {arXiv preprint arXiv:1806.09231}
}
@inproceedings{hoogeboom2021argmax,
  title     = {Argmax Flows: Learning Categorical Distributions with Normalizing Flows},
  author    = {Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
  year      = 2021,
  booktitle = {Third Symposium on Advances in Approximate Bayesian Inference},
  url       = {https://openreview.net/forum?id=fdsXhAy5Cp}
}
@article{Xie2018Crystal,
  title     = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
  author    = {Xie, Tian and Grossman, Jeffrey C.},
  year      = 2018,
  month     = {Apr},
  journal   = {Phys. Rev. Lett.},
  publisher = {American Physical Society},
  volume    = 120,
  pages     = 145301,
  doi       = {10.1103/PhysRevLett.120.145301},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301},
  issue     = 14,
  numpages  = 6
}
@article{Krenn_2020,
  title     = {Self-referencing embedded strings ({SELFIES}): A 100{\%} robust molecular string representation},
  author    = {Mario Krenn and Florian Häse and AkshatKumar Nigam and Pascal Friederich and Alan Aspuru-Guzik},
  year      = 2020,
  month     = {nov},
  journal   = {Machine Learning: Science and Technology},
  publisher = {{IOP} Publishing},
  volume    = 1,
  number    = 4,
  pages     = {045024},
  doi       = {10.1088/2632-2153/aba947},
  url       = {https://doi.org/10.1088/2632-2153/aba947},
  abstract  = {The discovery of novel materials and functional molecules can help to solve some of society’s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering–generally denoted as inverse design–was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model’s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.}
}
@article{rajan2020decimer,
  title     = {DECIMER: towards deep learning for chemical image recognition},
  author    = {Rajan, Kohulan and Zielesny, Achim and Steinbeck, Christoph},
  year      = 2020,
  journal   = {Journal of Cheminformatics},
  publisher = {Springer},
  volume    = 12,
  number    = 1,
  pages     = {1--9}
}
@article{segler2018generating,
  title     = {Generating focused molecule libraries for drug discovery with recurrent neural networks},
  author    = {Segler, Marwin HS and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P},
  year      = 2018,
  journal   = {ACS central science},
  publisher = {ACS Publications},
  volume    = 4,
  number    = 1,
  pages     = {120--131}
}
@article{gomez2018automatic,
  title     = {Automatic chemical design using a data-driven continuous representation of molecules},
  author    = {G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n},
  year      = 2018,
  journal   = {ACS central science},
  publisher = {ACS Publications},
  volume    = 4,
  number    = 2,
  pages     = {268--276}
}
@article{chithrananda2020chemberta,
  title   = {ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
  author  = {Chithrananda, Seyone and Grand, Gabe and Ramsundar, Bharath},
  year    = 2020,
  journal = {arXiv preprint arXiv:2010.09885}
}
@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year    = 2020,
  journal = {arXiv preprint arXiv:2005.14165}
}
@article{liu2019roberta,
  title   = {Roberta: A robustly optimized bert pretraining approach},
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year    = 2019,
  journal = {arXiv preprint arXiv:1907.11692}
}
@article{reynolds2021prompt,
  title   = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
  author  = {Laria Reynolds and Kyle McDonell},
  year    = 2021,
  journal = {arXiv preprint arXiv:2102.07350}
}
@article{tshitoyan2019unsupervised,
  title     = {Unsupervised word embeddings capture latent knowledge from materials science literature},
  author    = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A and Ceder, Gerbrand and Jain, Anubhav},
  year      = 2019,
  journal   = {Nature},
  publisher = {Nature Publishing Group},
  volume    = 571,
  number    = 7763,
  pages     = {95--98}
}
@article{friedrich2020sofc,
  title   = {The SOFC-Exp corpus and neural approaches to information extraction in the materials science domain},
  author  = {Friedrich, Annemarie and Adel, Heike and Tomazic, Federico and Hingerl, Johannes and Benteau, Renou and Maruscyk, Anika and Lange, Lukas},
  year    = 2020,
  journal = {arXiv preprint arXiv:2006.03039}
}
@article{weininger1988smiles,
  title     = {SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author    = {Weininger, David},
  year      = 1988,
  journal   = {Journal of chemical information and computer sciences},
  publisher = {ACS Publications},
  volume    = 28,
  number    = 1,
  pages     = {31--36}
}
@article{wang2021molclr,
  title   = {MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks},
  author  = {Yuyang Wang and Jianren Wang and Zhonglin Cao and Amir Barati Farimani},
  year    = 2021,
  journal = {arXiv preprint arXiv:2102.10056}
}
@misc{geiger2022e3nn,
  doi       = {10.48550/ARXIV.2207.09453},
  url       = {https://arxiv.org/abs/2207.09453},
  author    = {Geiger, Mario and Smidt, Tess},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {e3nn: Euclidean Neural Networks},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{heller2015inchi,
  title     = {InChI, the IUPAC international chemical identifier},
  author    = {Heller, Stephen R and McNaught, Alan and Pletnev, Igor and Stein, Stephen and Tchekhovskoi, Dmitrii},
  year      = 2015,
  journal   = {Journal of cheminformatics},
  publisher = {BioMed Central},
  volume    = 7,
  number    = 1,
  pages     = {1--34}
}
@article{butler2018machine,
  title     = {Machine learning for molecular and materials science},
  author    = {Butler, Keith T and Davies, Daniel W and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  year      = 2018,
  journal   = {Nature},
  publisher = {Nature Publishing Group},
  volume    = 559,
  number    = 7715,
  pages     = {547--555}
}
@article{swain2016chemdataextractor,
  title     = {ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature},
  author    = {Swain, Matthew C and Cole, Jacqueline M},
  year      = 2016,
  journal   = {Journal of chemical information and modeling},
  publisher = {ACS Publications},
  volume    = 56,
  number    = 10,
  pages     = {1894--1904}
}
@article{merk2018novo,
  title     = {De novo design of bioactive small molecules by artificial intelligence},
  author    = {Merk, Daniel and Friedrich, Lukas and Grisoni, Francesca and Schneider, Gisbert},
  year      = 2018,
  journal   = {Molecular informatics},
  publisher = {Wiley Online Library},
  volume    = 37,
  number    = {1-2},
  pages     = 1700153
}
@article{schwaller2020prediction,
  title   = {Prediction of chemical reaction yields using deep learning},
  author  = {Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis},
  year    = 2020,
  journal = {ChemRxiv Preprint},
  url     = {https://doi.org/10.26434/chemrxiv.12758474.v2}
}
@article{schwaller2020predicting,
  title     = {Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy},
  author    = {Schwaller, Philippe and Petraglia, Riccardo and Zullo, Valerio and Nair, Vishnu H and Haeuselmann, Rico Andreas and Pisoni, Riccardo and Bekas, Costas and Iuliano, Anna and Laino, Teodoro},
  year      = 2020,
  journal   = {Chemical Science},
  publisher = {Royal Society of Chemistry},
  volume    = 11,
  number    = 12,
  pages     = {3316--3325}
}
@article{schwaller2019molecular,
  title     = {Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction},
  author    = {Schwaller, Philippe and Laino, Teodoro and Gaudin, Th{\'e}ophile and Bolgar, Peter and Hunter, Christopher A and Bekas, Costas and Lee, Alpha A},
  year      = 2019,
  journal   = {ACS central science},
  publisher = {ACS Publications},
  volume    = 5,
  number    = 9,
  pages     = {1572--1583}
}
@article{vaucher2020automated,
  title     = {Automated extraction of chemical synthesis actions from experimental procedures},
  author    = {Vaucher, Alain C and Zipoli, Federico and Geluykens, Joppe and Nair, Vishnu H and Schwaller, Philippe and Laino, Teodoro},
  year      = 2020,
  journal   = {Nature communications},
  publisher = {Nature Publishing Group},
  volume    = 11,
  number    = 1,
  pages     = {1--11}
}
@article{schwaller2021mapping,
  title     = {Mapping the space of chemical reactions using attention-based neural networks},
  author    = {Schwaller, Philippe and Probst, Daniel and Vaucher, Alain C and Nair, Vishnu H and Kreutter, David and Laino, Teodoro and Reymond, Jean-Louis},
  year      = 2021,
  journal   = {Nature Machine Intelligence},
  publisher = {Nature Publishing Group},
  pages     = {1--9}
}
@article{brown2019guacamol,
  title     = {GuacaMol: benchmarking models for de novo molecular design},
  author    = {Brown, Nathan and Fiscato, Marco and Segler, Marwin HS and Vaucher, Alain C},
  year      = 2019,
  journal   = {Journal of chemical information and modeling},
  publisher = {ACS Publications},
  volume    = 59,
  number    = 3,
  pages     = {1096--1108}
}
@article{shmilovich2020discovery,
  title     = {Discovery of self-assembling $\pi$-conjugated peptides by active learning-directed coarse-grained molecular simulation},
  author    = {Shmilovich, Kirill and Mansbach, Rachael A and Sidky, Hythem and Dunne, Olivia E and Panda, Sayak Subhra and Tovar, John D and Ferguson, Andrew L},
  year      = 2020,
  journal   = {The Journal of Physical Chemistry B},
  publisher = {ACS Publications},
  volume    = 124,
  number    = 19,
  pages     = {3873--3891}
}
@article{wang2019coarse,
  title     = {Coarse-graining auto-encoders for molecular dynamics},
  author    = {Wang, Wujie and G{\'o}mez-Bombarelli, Rafael},
  year      = 2019,
  journal   = {npj Computational Materials},
  publisher = {Nature Publishing Group},
  volume    = 5,
  number    = 1,
  pages     = {1--9}
}
@article{ribeiro2018reweighted,
  title     = {Reweighted autoencoded variational Bayes for enhanced sampling (RAVE)},
  author    = {Ribeiro, Jo{\~a}o Marcelo Lamim and Bravo, Pablo and Wang, Yihang and Tiwary, Pratyush},
  year      = 2018,
  journal   = {The Journal of chemical physics},
  publisher = {AIP Publishing LLC},
  volume    = 149,
  number    = 7,
  pages     = {072301}
}
@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  year    = 2021,
  journal = {arXiv preprint arXiv:2104.09864}
}
@article{tay2021pretrained,
  title   = {Are Pre-trained Convolutions Better than Pre-trained Transformers?},
  author  = {Yi Tay and Mostafa Dehghani and Jai Gupta and Dara Bahri and Vamsi Aribandi and Zhen Qin and Donald Metzler},
  year    = 2021,
  journal = {arXiv preprint arXiv:2105.03322}
}
@inproceedings{cohen2018spherical,
  title     = {Spherical CNNs},
  author    = {Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  year      = 2018,
  booktitle = {International Conference on Learning Representations}
}
@article{finzi2021emlp,
  title   = {A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups},
  author  = {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
  year    = 2021,
  journal = {Arxiv}
}
@article{doshi2017towards,
  title   = {Towards a rigorous science of interpretable machine learning},
  author  = {Doshi-Velez, Finale and Kim, Been},
  year    = 2017,
  journal = {arXiv preprint arXiv:1702.08608}
}
@article{lee2004trust,
  title     = {Trust in automation: Designing for appropriate reliance},
  author    = {Lee, John D and See, Katrina A},
  year      = 2004,
  journal   = {Human factors},
  publisher = {SAGE Publications Sage UK: London, England},
  volume    = 46,
  number    = 1,
  pages     = {50--80}
}
@inproceedings{caruana2015intelligible,
  title        = {{Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission}},
  author       = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  year         = 2015,
  booktitle    = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages        = {1721--1730},
  organization = {ACM}
}
@article{Murdoch2019,
  title   = {{Interpretable machine learning: definitions, methods, and applications}},
  author  = {Murdoch, James W and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
  year    = 2019,
  journal = {eprint arXiv},
  pages   = {1--11},
  url     = {http://arxiv.org/abs/1901.04592},
  arxivid = {1901.04592}
}
@article{montavon2018methods,
  title     = {{Methods for interpreting and understanding deep neural networks}},
  author    = {Montavon, Grégoire and Samek, Wojciech and M{\"{u}}ller, Klaus-Robert},
  year      = 2018,
  journal   = {Digital Signal Processing},
  publisher = {Elsevier},
  volume    = 73,
  pages     = {1--15}
}
@inproceedings{ribeiro2016should,
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year      = 2016,
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages     = {1135--1144}
}
@article{ribeiro2016model,
  title   = {Model-agnostic interpretability of machine learning},
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year    = 2016,
  journal = {arXiv preprint arXiv:1606.05386}
}
@inproceedings{koh2017understanding,
  title        = {Understanding black-box predictions via influence functions},
  author       = {Koh, Pang Wei and Liang, Percy},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning},
  pages        = {1885--1894},
  organization = {PMLR}
}
@article{wachter2017counterfactual,
  title     = {Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author    = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year      = 2017,
  journal   = {Harv. JL \& Tech.},
  publisher = {HeinOnline},
  volume    = 31,
  pages     = 841
}
@article{goodman2017european,
  title   = {{European Union regulations on algorithmic decision-making and a “right to explanation”}},
  author  = {Goodman, Bryce and Flaxman, Seth},
  year    = 2017,
  journal = {AI Magazine},
  volume  = 38,
  number  = 3,
  pages   = {50--57}
}
@misc{Development2019,
  title     = {{Recommendation of the Council on Artificial Intelligence}},
  author    = {Development, Organisation for Economic Co-operation and},
  year      = 2019,
  publisher = {Development, Organisation for Economic Co-operation and},
  pages     = {0449},
  url       = {https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449}
}
@article{satorras2021en,
  title   = {E(n) Equivariant Normalizing Flows for Molecule Generation in 3D},
  author  = {Victor Garcia Satorras and Emiel Hoogeboom and Fabian B. Fuchs and Ingmar Posner and Max Welling},
  year    = 2021,
  journal = {arXiv preprint arXiv:2105.09016}
}
@article{9369420,
  title   = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  author  = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  year    = 2021,
  journal = {Proceedings of the IEEE},
  volume  = 109,
  number  = 3,
  pages   = {247--278},
  doi     = {10.1109/JPROC.2021.3060483}
}
@book{molnar2019,
  title     = {Interpretable Machine Learning},
  author    = {Christoph Molnar},
  year      = 2019,
  publisher = {Lulu.com},
  note      = {\url{https://christophm.github.io/interpretable-ml-book/}},
  subtitle  = {A Guide for Making Black Box Models Explainable}
}
@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year    = 2014,
  journal = {arXiv preprint arXiv:1409.0473}
}
@inproceedings{pmlr-v70-balduzzi17b,
  title     = {The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author    = {David Balduzzi and Marcus Frean and Lennox Leary and J. P. Lewis and Kurt Wan-Duo Ma and Brian McWilliams},
  year      = 2017,
  month     = {06--11 Aug},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = 70,
  pages     = {342--350},
  url       = {http://proceedings.mlr.press/v70/balduzzi17b.html},
  editor    = {Precup, Doina and Teh, Yee Whye},
  pdf       = {http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf},
  abstract  = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.}
}
@article{jang2016categorical,
  title   = {Categorical reparameterization with gumbel-softmax},
  author  = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year    = 2016,
  journal = {arXiv preprint arXiv:1611.01144}
}
@inproceedings{sundararajan2017axiomatic,
  title        = {Axiomatic attribution for deep networks},
  author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning},
  pages        = {3319--3328},
  organization = {PMLR}
}
@article{smilkov2017smoothgrad,
  title   = {Smoothgrad: removing noise by adding noise},
  author  = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year    = 2017,
  journal = {arXiv preprint arXiv:1706.03825}
}
@inbook{Montavon2019,
  title     = {Layer-Wise Relevance Propagation: An Overview},
  author    = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year      = 2019,
  booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {193--209},
  isbn      = {978-3-030-28954-6},
  url       = {https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10},
  editor    = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert}
}
@article{sturmfels2020visualizing,
  title   = {Visualizing the Impact of Feature Attribution Baselines},
  author  = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  year    = 2020,
  journal = {Distill},
  doi     = {10.23915/distill.00022},
  note    = {https://distill.pub/2020/attribution-baselines}
}
@article{vstrumbelj2014explaining,
  title     = {Explaining prediction models and individual predictions with feature contributions},
  author    = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
  year      = 2014,
  journal   = {Knowledge and information systems},
  publisher = {Springer},
  volume    = 41,
  number    = 3,
  pages     = {647--665}
}
@article{barrett2020investigating,
  title   = {Investigating Active Learning and Meta-Learning for Iterative Peptide Design},
  author  = {Barrett, Rainier and White, Andrew D.},
  year    = 2021,
  journal = {Journal of Chemical Information and Modeling},
  volume  = 61,
  number  = 1,
  pages   = {95--105},
  doi     = {10.1021/acs.jcim.0c00946},
  url     = {https://doi.org/10.1021/acs.jcim.0c00946}
}
@article{numeroso2020explaining,
  title   = {Explaining Deep Graph Networks with Molecular Counterfactuals},
  author  = {Numeroso, Danilo and Bacciu, Davide},
  year    = 2020,
  journal = {arXiv preprint arXiv:2011.05134}
}
@article{agarwal2021towards,
  title   = {Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations},
  author  = {Agarwal, Chirag and Zitnik, Marinka and Lakkaraju, Himabindu},
  year    = 2021,
  journal = {arXiv preprint arXiv:2106.09078}
}
@article{yuan2020explainability,
  title   = {Explainability in graph neural networks: A taxonomic survey},
  author  = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
  year    = 2020,
  journal = {arXiv preprint arXiv:2012.15445}
}
@article{wellawatte_seshadri_white_2021,
  title     = {Model agnostic generation of counterfactual explanations for molecules},
  author    = {Wellawatte, Geemi P and Seshadri, Aditi and White, Andrew D},
  year      = 2022,
  journal   = {Chem. Sci.},
  publisher = {The Royal Society of Chemistry},
  pages     = {-},
  doi       = {10.1039/D1SC05259D},
  url       = {http://dx.doi.org/10.1039/D1SC05259D}
}
@article{miller2019explanation,
  title     = {Explanation in artificial intelligence: Insights from the social sciences},
  author    = {Miller, Tim},
  year      = 2019,
  journal   = {Artificial intelligence},
  publisher = {Elsevier},
  volume    = 267,
  pages     = {1--38}
}
@article{madsen2021post,
  title   = {Post-hoc Interpretability for Neural NLP: A Survey},
  author  = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  year    = 2021,
  journal = {arXiv preprint arXiv:2108.04840}
}
@article{sanchez-lengeling2021a,
  title   = {A Gentle Introduction to Graph Neural Networks},
  author  = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alex},
  year    = 2021,
  journal = {Distill},
  doi     = {10.23915/distill.00033},
  note    = {https://distill.pub/2021/gnn-intro}
}
@inproceedings{santurkar2018does,
  title     = {How does batch normalization help optimization?},
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and M{\k{a}}dry, Aleksander},
  year      = 2018,
  booktitle = {Proceedings of the 32nd international conference on neural information processing systems},
  pages     = {2488--2498}
}
@inproceedings{lu2017expressive,
  title     = {The expressive power of neural networks: A view from the width},
  author    = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year      = 2017,
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6232--6240}
}
@article{powell1977restart,
  title     = {Restart procedures for the conjugate gradient method},
  author    = {Powell, Michael James David},
  year      = 1977,
  journal   = {Mathematical programming},
  publisher = {Springer},
  volume    = 12,
  number    = 1,
  pages     = {241--254}
}
@article{grattarola2021understanding,
  title   = {Understanding Pooling in Graph Neural Networks},
  author  = {Grattarola, Daniele and Zambon, Daniele and Bianchi, Filippo Maria and Alippi, Cesare},
  year    = 2021,
  journal = {arXiv preprint arXiv:2110.05292}
}
@article{o2012towards,
  title     = {Towards a Universal SMILES representation-A standard method to generate canonical SMILES based on the InChI},
  author    = {O’Boyle, Noel M},
  year      = 2012,
  journal   = {Journal of cheminformatics},
  publisher = {Springer},
  volume    = 4,
  number    = 1,
  pages     = {1--14}
}
@article{daigavane2021understanding,
  title   = {Understanding Convolutions on Graphs},
  author  = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  year    = 2021,
  journal = {Distill},
  doi     = {10.23915/distill.00032},
  note    = {https://distill.pub/2021/understanding-gnns}
}
@article{kingma2013auto,
  title   = {Auto-encoding variational bayes},
  author  = {Kingma, Diederik P and Welling, Max},
  year    = 2013,
  journal = {arXiv preprint arXiv:1312.6114}
}
@article{smith2017don,
  title   = {Don't decay the learning rate, increase the batch size},
  author  = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  year    = 2017,
  journal = {arXiv preprint arXiv:1711.00489}
}
@inproceedings{golovin2017google,
  title     = {Google vizier: A service for black-box optimization},
  author    = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, David},
  year      = 2017,
  booktitle = {Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages     = {1487--1495}
}
@inproceedings{domhan2015speeding,
  title     = {Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author    = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  year      = 2015,
  booktitle = {Twenty-fourth international joint conference on artificial intelligence}
}
@inproceedings{jamieson2016non,
  title        = {Non-stochastic best arm identification and hyperparameter optimization},
  author       = {Jamieson, Kevin and Talwalkar, Ameet},
  year         = 2016,
  booktitle    = {Artificial intelligence and statistics},
  pages        = {240--248},
  organization = {PMLR}
}
@book{reed1999neural,
  title     = {Neural smithing: supervised learning in feedforward artificial neural networks},
  author    = {Reed, Russell and MarksII, Robert J},
  year      = 1999,
  publisher = {Mit Press}
}
@article{nigam_stoned,
  title     = {Beyond generative models: superfast traversal{,} optimization{,} novelty{,} exploration and discovery (STONED)     algorithm for molecules using SELFIES},
  author    = {Nigam, AkshatKumar and Pollice, Robert and Krenn, Mario and Gomes, Gabriel dos Passos and Aspuru-Guzik, Al\'{a}n},
  year      = 2021,
  journal   = {Chem. Sci.},
  publisher = {The Royal Society of Chemistry},
  volume    = 12,
  pages     = {7079--7090},
  doi       = {10.1039/D1SC00231G},
  url       = {http://dx.doi.org/10.1039/D1SC00231G},
  issue     = 20
}
@article{ansari2021iterative,
  title   = {Iterative Symbolic Regression for Learning Transport Equations},
  author  = {Ansari, Mehrad and Gandhi, Heta A and Foster, David G and White, Andrew D},
  year    = 2021,
  journal = {arXiv preprint arXiv:2108.03293}
}
@incollection{billard2000regression,
  title     = {Regression analysis for interval-valued data},
  author    = {Billard, Lynne and Diday, Edwin},
  year      = 2000,
  booktitle = {Data analysis, classification, and related methods},
  publisher = {Springer},
  pages     = {369--374}
}
@article{udrescu2020ai,
  title     = {AI Feynman: A physics-inspired method for symbolic regression},
  author    = {Udrescu, Silviu-Marian and Tegmark, Max},
  year      = 2020,
  journal   = {Science Advances},
  publisher = {American Association for the Advancement of Science},
  volume    = 6,
  number    = 16,
  pages     = {eaay2631}
}
@article{cranmer2020discovering,
  title   = {Discovering symbolic models from deep learning with inductive biases},
  author  = {Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
  year    = 2020,
  journal = {Advances in Neural Information Processing Systems},
  volume  = 33,
  pages   = {17429--17442}
}
@inproceedings{kim2018interpretability,
  title        = {Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author       = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  year         = 2018,
  booktitle    = {International conference on machine learning},
  pages        = {2668--2677},
  organization = {PMLR}
}
@article{lipton2018mythos,
  title     = {The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author    = {Lipton, Zachary C},
  year      = 2018,
  journal   = {Queue},
  publisher = {ACM New York, NY, USA},
  volume    = 16,
  number    = 3,
  pages     = {31--57}
}
@article{pirtskhalava2021dbaasp,
  title     = {DBAASP v3: database of antimicrobial/cytotoxic activity and structure of peptides as a resource for development of new therapeutics},
  author    = {Pirtskhalava, Malak and Amstrong, Anthony A and Grigolava, Maia and Chubinidze, Mindia and Alimbarashvili, Evgenia and Vishnepolsky, Boris and Gabrielian, Andrei and Rosenthal, Alex and Hurt, Darrell E and Tartakovsky, Michael},
  year      = 2021,
  journal   = {Nucleic acids research},
  publisher = {Oxford University Press},
  volume    = 49,
  number    = {D1},
  pages     = {D288--D297}
}
@article{liaw2018tune,
  title   = {Tune: A Research Platform for Distributed Model Selection and Training},
  author  = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
  year    = 2018,
  journal = {arXiv preprint arXiv:1807.05118}
}
@inproceedings{optuna_2019,
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year      = 2019,
  booktitle = {Proceedings of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}
@inproceedings{bergstra2013making,
  title        = {Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author       = {Bergstra, James and Yamins, Daniel and Cox, David},
  year         = 2013,
  booktitle    = {International conference on machine learning},
  pages        = {115--123},
  organization = {PMLR}
}
@inproceedings{louppe2017bayesian,
  title     = {Bayesian optimisation with scikit-optimize},
  author    = {Louppe, Gilles},
  booktitle = {PyData Amsterdam},
  year      = {2017}
}

@article{frostig2018compiling,
  title   = {Compiling machine learning programs via high-level tracing},
  author  = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
  year    = 2018,
  journal = {Systems for Machine Learning},
  pages   = {23--24}
}
@article{ball2011beyond,
  title     = {Beyond the bond},
  author    = {Ball, Philip},
  year      = 2011,
  journal   = {Nature},
  publisher = {Nature Publishing Group},
  volume    = 469,
  number    = 7328,
  pages     = {26--28}
}
@article{baydin2018automatic,
  title     = {Automatic differentiation in machine learning: a survey},
  author    = {Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year      = 2018,
  journal   = {Journal of Marchine Learning Research},
  publisher = {Microtome Publishing},
  volume    = 18,
  pages     = {1--43}
}
@article{chuang2018comment,
  title     = {Comment on ``Predicting reaction performance in C--N cross-coupling using machine learning''},
  author    = {Chuang, Kangway V and Keiser, Michael J},
  year      = 2018,
  journal   = {Science},
  publisher = {American Association for the Advancement of Science},
  volume    = 362,
  number    = 6416,
  pages     = {eaat8603}
}
@article{rogers2010extended,
  title     = {Extended-connectivity fingerprints},
  author    = {Rogers, David and Hahn, Mathew},
  year      = 2010,
  journal   = {Journal of chemical information and modeling},
  publisher = {ACS Publications},
  volume    = 50,
  number    = 5,
  pages     = {742--754}
}
@article{shorten2019survey,
  title     = {A survey on image data augmentation for deep learning},
  author    = {Shorten, Connor and Khoshgoftaar, Taghi M},
  year      = 2019,
  journal   = {Journal of big data},
  publisher = {Springer},
  volume    = 6,
  number    = 1,
  pages     = {1--48}
}
@misc{straub2020mathematical,
  title     = {Mathematical Methods for Molecular Science},
  author    = {Straub, J},
  year      = 2020,
  publisher = {Unit Circle Press, Cambridge, MA}
}

@article{schutt2018schnet,
  title     = {Schnet--a deep learning architecture for molecules and materials},
  author    = {Sch{\"u}tt, Kristof T and Sauceda, Huziel E and Kindermans, P-J and Tkatchenko, Alexandre and M{\"u}ller, K-R},
  journal   = {The Journal of Chemical Physics},
  volume    = {148},
  number    = {24},
  pages     = {241722},
  year      = {2018},
  publisher = {AIP Publishing LLC}
}

@article{cox2022symmetric,
  title   = {Symmetric Molecular Dynamics},
  author  = {Cox, Sam and White, Andrew D},
  journal = {arXiv preprint arXiv:2204.01114},
  year    = {2022}
}


@article{velivckovic2022message,
  title   = {Message passing all the way up},
  author  = {Veli{\v{c}}kovi{\'c}, Petar},
  journal = {arXiv preprint arXiv:2202.11097},
  year    = {2022}
}

@article{thiede2021autobahn,
  title   = {Autobahn: Automorphism-based graph neural nets},
  author  = {Thiede, Erik and Zhou, Wenda and Kondor, Risi},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@article{bodnar2021weisfeiler,
  title   = {Weisfeiler and Lehman go cellular: CW networks},
  author  = {Bodnar, Cristian and Frasca, Fabrizio and Otter, Nina and Wang, Yuguang and Lio, Pietro and Montufar, Guido F and Bronstein, Michael},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {2625--2640},
  year    = {2021}
}

@article{timmons2020happenn,
  title     = {HAPPENN is a novel tool for hemolytic activity prediction for therapeutic peptides which employs neural networks},
  author    = {Timmons, P. Brendan and Hewage, Chandralal M.},
  journal   = {Scientific reports},
  volume    = {10},
  number    = {1},
  pages     = {1--18},
  year      = {2020},
  publisher = {Nature Publishing Group}
}
@inproceedings{khodamoradi2021aslr,
  title        = {ASLR: An Adaptive Scheduler for Learning Rate},
  author       = {Khodamoradi, Alireza and Denolf, Kristof and Vissers, Kees and Kastner, Ryan C},
  booktitle    = {2021 International Joint Conference on Neural Networks (IJCNN)},
  pages        = {1--8},
  year         = {2021},
  organization = {IEEE}
}
@article{yang2020pacl,
  title     = {PACL: piecewise arc cotangent decay learning rate for deep neural network training},
  author    = {Yang, Haixu and Liu, Jihong and Sun, Hongwei and Zhang, Henggui},
  journal   = {IEEE Access},
  volume    = {8},
  pages     = {112805--112813},
  year      = {2020},
  publisher = {IEEE}
}
@article{vidyabharathi2021achieving,
  title     = {Achieving generalization of deep learning models in a quick way by adapting T-HTR learning rate scheduler},
  author    = {Vidyabharathi, D and Mohanraj, V and Kumar, J Senthil and Suresh, Y},
  journal   = {Personal and Ubiquitous Computing},
  pages     = {1--19},
  year      = {2021},
  publisher = {Springer}
}
@article{goodfellow2017deep,
  title   = {Deep learning (adaptive computation and machine learning series)},
  author  = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  journal = {Cambridge Massachusetts},
  pages   = {321--359},
  year    = {2017}
}
@article{hoffer2017train,
  title   = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author  = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@article{keskar2016large,
  title   = {On large-batch training for deep learning: Generalization gap and sharp minima},
  author  = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal = {arXiv preprint arXiv:1609.04836},
  year    = {2016}
}
@inproceedings{lin2020extrapolation,
  title        = {Extrapolation for large-batch training in deep learning},
  author       = {Lin, Tao and Kong, Lingjing and Stich, Sebastian and Jaggi, Martin},
  booktitle    = {International Conference on Machine Learning},
  pages        = {6094--6104},
  year         = {2020},
  organization = {PMLR}
}

@article{eger2019time,
  title   = {Is it time to swish? Comparing deep learning activation functions across NLP tasks},
  author  = {Eger, Steffen and Youssef, Paul and Gurevych, Iryna},
  journal = {arXiv preprint arXiv:1901.02671},
  year    = {2019}
}

@article{hendrycks2016gaussian,
  title   = {Gaussian error linear units (gelus)},
  author  = {Hendrycks, Dan and Gimpel, Kevin},
  journal = {arXiv preprint arXiv:1606.08415},
  year    = {2016}
}

@article{zhang2021motif,
  title   = {Motif-based graph self-supervised learning for molecular property prediction},
  author  = {Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Chee-Kong},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {15870--15882},
  year    = {2021}
}

@article{you2020graph,
  title   = {Graph contrastive learning with augmentations},
  author  = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {5812--5823},
  year    = {2020}
}

@inproceedings{sun2021mocl,
  title     = {MoCL: data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph},
  author    = {Sun, Mengying and Xing, Jing and Wang, Huijun and Chen, Bin and Zhou, Jiayu},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages     = {3585--3594},
  year      = {2021}
}

@article{liu2021pre,
  title   = {Pre-training molecular graph representation with 3d geometry},
  author  = {Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  journal = {arXiv preprint arXiv:2110.07728},
  year    = {2021}
}

@inproceedings{erhan2010does,
  title        = {Why does unsupervised pre-training help deep learning?},
  author       = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle    = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages        = {201--208},
  year         = {2010},
  organization = {JMLR Workshop and Conference Proceedings}
}

@article{xie2022self,
  title     = {Self-supervised learning of graph neural networks: A unified review},
  author    = {Xie, Yaochen and Xu, Zhao and Zhang, Jingtun and Wang, Zhengyang and Ji, Shuiwang},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2022},
  publisher = {IEEE}
}

@article{mao2020survey,
  title   = {A survey on self-supervised pre-training for sequential transfer learning in neural networks},
  author  = {Mao, Huanru Henry},
  journal = {arXiv preprint arXiv:2007.00800},
  year    = {2020}
}

@inproceedings{finn2017model,
  title        = {Model-agnostic meta-learning for fast adaptation of deep networks},
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {International conference on machine learning},
  pages        = {1126--1135},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{wang2019smiles,
  title     = {SMILES-BERT: large scale unsupervised pre-training for molecular property prediction},
  author    = {Wang, Sheng and Guo, Yuzhi and Wang, Yuhong and Sun, Hongmao and Huang, Junzhou},
  booktitle = {Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics},
  pages     = {429--436},
  year      = {2019}
}
@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}
@article{omalley2019kerastuner,
  title   = {Keras tuner},
  author  = {O’Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran{\c{c}}ois and Jin, Haifeng and Invernizzi, Luca and others},
  journal = {Retrieved May},
  volume  = {21},
  pages   = {2020},
  year    = {2019}
}

@article{anderson2019cormorant,
  title   = {Cormorant: Covariant molecular neural networks},
  author  = {Anderson, Brandon and Hy, Truong Son and Kondor, Risi},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}


@article{spellings2021geometric,
  title   = {Geometric Algebra Attention Networks for Small Point Clouds},
  author  = {Spellings, Matthew},
  journal = {arXiv preprint arXiv:2110.02393},
  year    = {2021}
}


@article{weisfeiler1968reduction,
  title   = {The reduction of a graph to canonical form and the algebra which appears therein},
  author  = {Weisfeiler, Boris and Leman, Andrei},
  journal = {NTI, Series},
  volume  = {2},
  number  = {9},
  pages   = {12--16},
  year    = {1968}
}

@article{batatia2022design,
  title   = {The Design Space of E (3)-Equivariant Atom-Centered Interatomic Potentials},
  author  = {Batatia, Ilyes and Batzner, Simon and Kov{\'a}cs, D{\'a}vid P{\'e}ter and Musaelian, Albert and Simm, Gregor NC and Drautz, Ralf and Ortner, Christoph and Kozinsky, Boris and Cs{\'a}nyi, G{\'a}bor},
  journal = {arXiv preprint arXiv:2205.06643},
  year    = {2022}
}

@article{pozdnyakov2022incompleteness,
  title   = {Incompleteness of graph convolutional neural networks for points clouds in three dimensions},
  author  = {Pozdnyakov, Sergey N and Ceriotti, Michele},
  journal = {arXiv preprint arXiv:2201.07136},
  year    = {2022}
}


@inproceedings{ioffe2015batch,
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  pages        = {448--456},
  year         = {2015},
  organization = {PMLR}
}

@article{kim2022pure,
  title   = {Pure transformers are powerful graph learners},
  author  = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
  journal = {arXiv preprint arXiv:2207.02505},
  year    = {2022}
}

@article{blumenthal2022ai,
  title     = {An AI Bill of Rights: Implications for Health Care AI and Machine Learning—A Bioethics Lens},
  author    = {Blumenthal-Barby, Jennifer},
  journal   = {The American Journal of Bioethics},
  pages     = {1--3},
  year      = {2022},
  publisher = {Taylor \& Francis}
}


 @article{rankovic_griffiths_moss_schwaller_2022,
  place     = {Cambridge},
  title     = {Bayesian optimisation for additive screening and yield
               improvements in chemical reactions -- beyond one-hot encodings},
  doi       = {10.26434/chemrxiv-2022-nll2j},
  journal   = {ChemRxiv},
  publisher = {Cambridge Open Engage},
  author    = {Ranković, Bojana and Griffiths, Ryan-Rhys and Moss, Henry B. and Schwaller, Philippe},
  year      = {2022}
}

@article{yang2022classifying,
  title     = {Classifying the toxicity of pesticides to honey bees via support vector machines with random walk graph kernels},
  author    = {Yang, Ping and Henle, E Adrian and Fern, Xiaoli Z and Simon, Cory M},
  journal   = {The Journal of Chemical Physics},
  year      = {2022},
  publisher = {AIP Publishing LLC}
}
